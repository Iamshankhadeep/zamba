{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to zamba's documentation! \u00b6 Zamba means \"forest\" in the Lingala language. Zamba is a tool built in Python to automatically identify the species seen in camera trap videos from sites in Africa and Europe. Using the combined input of various deep learning models, the tool makes predictions for 42 common species in these videos (as well as blank, or, \"no species present\"). Zamba can be accessed as both a command-line tool and a Python package. Zamba ships with three model options. time_distributed and slowfast are trained on 32 common species from central and west Africa. european is trained on 11 common species from western Europe. time_distributed and european are image-based models while slowfast is a video-based model. Getting Started \u00b6 Installing Zamba Quickstart User Tutorials \u00b6 Classifying Unlabeled Videos Training a Model on Labeled Videos Debugging Available Models \u00b6 Advanced Options \u00b6 All Optional Arguments Using YAML Configuration Files Contributing \u00b6 Changelog \u00b6 Version 2","title":"Home"},{"location":"#welcome-to-zambas-documentation","text":"Zamba means \"forest\" in the Lingala language. Zamba is a tool built in Python to automatically identify the species seen in camera trap videos from sites in Africa and Europe. Using the combined input of various deep learning models, the tool makes predictions for 42 common species in these videos (as well as blank, or, \"no species present\"). Zamba can be accessed as both a command-line tool and a Python package. Zamba ships with three model options. time_distributed and slowfast are trained on 32 common species from central and west Africa. european is trained on 11 common species from western Europe. time_distributed and european are image-based models while slowfast is a video-based model.","title":"Welcome to zamba's documentation!"},{"location":"#getting-started","text":"Installing Zamba Quickstart","title":"Getting Started"},{"location":"#user-tutorials","text":"Classifying Unlabeled Videos Training a Model on Labeled Videos Debugging","title":"User Tutorials"},{"location":"#available-models","text":"","title":"Available Models"},{"location":"#advanced-options","text":"All Optional Arguments Using YAML Configuration Files","title":"Advanced Options"},{"location":"#contributing","text":"","title":"Contributing"},{"location":"#changelog","text":"Version 2","title":"Changelog"},{"location":"configurations/","text":"All Optional Arguments \u00b6 Three main configuration classes are specific in zamba : VideoLoaderConfig : Defines all possible parameters for how videos are loaded PredictConfig : Defines all possible parameters for model inference TrainConfig : Defines all possible parameters for model training Here's a helpful diagram which shows how everything is related. Video loading arguments \u00b6 The VideoLoaderConfig class <!-- TODO: add link to source code><!--> defines all of the optional parameters that can be specified for how videos are loaded before either inference or training. This includes selecting which frames to use from each video. >> from zamba.data.video import VideoLoaderConfig >> help ( VideoLoaderConfig ) class VideoLoaderConfig ( pydantic . main . BaseModel ) | VideoLoaderConfig ( * , crop_bottom_pixels : int = None , i_frames : bool = False , scene_threshold : float = None , megadetector_lite_config : zamba . models . megadetector_lite_yolox . MegadetectorLiteYoloXConfig = None , video_height : int = None , video_width : int = None , total_frames : int = None , ensure_total_frames : bool = True , fps : float = None , early_bias : bool = False , frame_indices : List [ int ] = None , evenly_sample_total_frames : bool = False , pix_fmt : str = 'rgb24' , resize_after_frame_selection : bool = False ) -> None Let's go through each of those arguments. crop_bottom_pixels (int, optional) \u00b6 Number of pixels to crop from the bottom of the video (prior to resizing to video_height ). Defaults to None i_frames (bool, optional) \u00b6 Only load the I-Frames . Defaults to False scene_threshold (float, optional) \u00b6 Only load frames that correspond to scene changes . Defaults to None megadetector_lite_config (MegadetectorLiteYoloXConfig, optional) \u00b6 The megadetector_lite_config is used to specify any parameters that should be passed to the MegadetectorLiteYoloX model for frame selection. For all possible options, see the MegadetectorLiteYoloXConfig<!-- TODO: add github link><!-->. If megadetector_lite_config is None (the default), the MegadetectorLiteYoloX model will not be used to select frames. video_height (int, optional), video_width (int, optional) \u00b6 Resize the video to this height and width in pixels. Defaults to None total_frames (int, optional) \u00b6 Number of frames that should ultimately be returned. Defaults to None ensure_total_frames (bool) \u00b6 Selecting the number of frames by resampling may result in one more or fewer frames due to rounding. If True, ensure the requested number of frames is returned by either clipping or duplicating the final frame. Raises an error if no frames have been selected. Otherwise, return the array unchanged. Defaults to True fps (int, optional) \u00b6 Resample the video evenly from the entire duration to a specific number of frames per second. Defaults to None early_bias (bool, optional) \u00b6 Resamples to 24 fps and selects 16 frames biased toward the front. This strategy was used by the Pri-matrix Factorization machine learning competition winner. Defaults to False frame_indices (list(int), optional) \u00b6 Select specific frame numbers. Note: frame selection is done after any resampling. Defaults to None evenly_sample_total_frames (bool, optional) \u00b6 Reach the total number of frames specified by evenly sampling from the duration of the video. Defaults to False pix_fmt (str, optional) \u00b6 FFmpeg pixel format, defaults to rgb24 for RGB channels; can be changed to bgr24 for BGR. Prediction arguments \u00b6 All possible model inference parameters are defined by the PredictConfig class<!-- TODO: add link to class definition on github><!-->. Let's see the class documentation in Python: >> from zamba.models.config import PredictConfig >> help ( PredictConfig ) class PredictConfig ( ZambaBaseModel ) | PredictConfig ( * , data_directory : pydantic . types . DirectoryPath = PosixPath ( '/home/ubuntu/zamba-algorithms' ), filepaths : pydantic . types . FilePath = None , checkpoint : pydantic . types . FilePath = None , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , species : List [ str ] = None , gpus : Union [ List [ int ], str , int ] = 1 , num_workers : int = 7 , batch_size : int = 8 , save : Union [ bool , pathlib . Path ] = True , dry_run : bool = False , proba_threshold : float = None , output_class_names : bool = False , weight_download_region : zamba . models . utils . RegionEnum = 'us' , cache_dir : pathlib . Path = None , skip_load_validation : bool = False ) -> None ... Either data_directory or filepaths must be specified to instantiate PredictConfig . Otherwise the current working directory will be used as the default data_directory . data_directory (DirectoryPath, optional) \u00b6 Path to the directory containing training videos. Defaults to the current working directory. filepaths (FilePath, optional) \u00b6 Path to a list of files for classification. Defaults to the files in the current working directory checkpoint (Path or str, optional) \u00b6 Path to a model checkpoint to load and use for inference. The default is None , which automatically loads the pretrained checkpoint for the model specified by model_name . Since the default model_name is time_distributed the default checkpoint is zamba_time_distributed.ckpt `model_name (time_distributed|slowfast|european, optional) \u00b6 Name of the model to use for inference. The three model options that ship with zamba are time_distributed , slowfast , and european . See the Available Models page for details. Defaults to time_distributed species (list(str), optional) \u00b6 List of possible species class labels for the data. The default is None , which automatically loads the classes associated with the model specified by model_name . Since the default model_name is time_distributed , the default is the 32 species (plus blank) from central and west Africa. gpus (int, optional) \u00b6 The number of GPUs to use during inference. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available on the machine. num_workers (int, optional) \u00b6 The number of CPUs to use during training. By default, it will be set to either one less than the number of CPUs in the system, or one if there is only one CPU in the system. batch_size (int, optional) \u00b6 The batch size to use for inference. Defaults to 8 save (bool, optional) \u00b6 Whether to save out the predictions to a CSV file. y default, predictions will be saved at zamba_predictions.csv in the current working directory. Defaults to True dry_run (bool, optional) \u00b6 Specifying True is useful for trying out model implementations more quickly by running only a single batch of inference. Defaults to False proba_threshold (float between 0 and 1, optional) \u00b6 For advanced uses, you may want the algorithm to be more or less sensitive to if a species is present. This parameter is a FLOAT number, e.g., 0.64 corresponding to the probability threshold beyond which an animal is considered to be present in the video being analyzed. By default no threshold is passed, proba_threshold=None . This will return a probability from 0-1 for each species that could occur in each video. If a threshold is passed, then the final prediction value returned for each class is probability >= proba_threshold , so that all class values become 0 ( False , the species does not appear) or 1 ( True , the species does appear). output_class_names (bool, optional) \u00b6 Setting this option to True yields the most concise output zamba is capable of. The highest species probability in a video is taken to be the only species in that video, and the output returned is simply the video name and the name of the species with the highest class probability, or blank if the most likely classification is no animal. Defaults to False weight_download_region [us|eu|asia] \u00b6 Because zamba needs to download pretrained weights for the neural network architecture, we make these weights available in different regions. us is the default, but if you are not in the US you should use either eu for the European Union or asia for Asia Pacific to make sure that these download as quickly as possible for you. cache_dir (FilePath, optional) \u00b6 The directory where the model weights will be saved. If it is None (the default), the model will be cached to an automatic temp directory at ~/.cache/zamba skip_load_validation (bool, optional) \u00b6 By default, before kicking off inference zamba will iterate through all of the videos in the data and verify that each can be loaded. Setting skip_load_verification to True skips this step. Validation can be very time intensive depending on the number of videos. It is recommended to run validation once, but not on future inference runs if the videos have not changed. Defaults to False Training arguments \u00b6 All possible model training parameters are defined by the TrainConfig class<!-- TODO: add link to class definition><!-->. Let's see the class documentation in Python: >> from zamba.models.config import TrainConfig >> help ( TrainConfig ) class TrainConfig ( ZambaBaseModel ) | TrainConfig ( * , labels : Union [ pydantic . types . FilePath , pandas . core . frame . DataFrame ], data_directory : pydantic . types . DirectoryPath = PosixPath ( '/home/ubuntu/zamba-algorithms' ), checkpoint : pydantic . types . FilePath = None , scheduler_config : Union [ str , zamba . models . config . SchedulerConfig , NoneType ] = 'default' , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , dry_run : Union [ bool , int ] = False , batch_size : int = 8 , auto_lr_find : bool = True , backbone_finetune : bool = False , backbone_finetune_params : zamba . models . config . BackboneFinetuneConfig = BackboneFinetuneConfig ( unfreeze_backbone_at_epoch = 15 , backbone_initial_ratio_lr = 0.01 , multiplier = 1 , pre_train_bn = False , train_bn = False , verbose = True ), gpus : Union [ List [ int ], str , int ] = 1 , num_workers : int = 7 , max_epochs : int = None , early_stopping : bool = True , early_stopping_params : zamba . models . config . EarlyStoppingConfig = EarlyStoppingConfig ( monitor = 'val_macro_f1' , patience = 3 , verbose = True , mode = 'max' ), tensorboard_log_dir : str = 'tensorboard_logs' , weight_download_region : zamba . models . utils . RegionEnum = 'us' , cache_dir : pathlib . Path = None , split_proportions : Dict [ str , int ] = { 'train' : 3 , 'val' : 1 , 'holdout' : 1 }, save_directory : pathlib . Path = None , skip_load_validation : bool = False , from_scratch : bool = False , predict_all_zamba_species : bool = True ) -> None ... data_directory and labels must be specified to instantiate TrainConfig . If no data_directory is provided, it will default the current working directory. labels (FilePath or pd.DataFrame, required) \u00b6 Either the path to a CSV file with labels for training, or a dataframe of the training labels. There must be columns for filename and label . data_directory (DirectoryPath, optional) \u00b6 Path to the directory containing training videos. Defaults to the current working directory. checkpoint (Path or str, optional) \u00b6 Path to a model checkpoint to load and resume training from. The default is None , which automatically loads the pretrained checkpoint for the model specified by model_name . Since the default model_name is time_distributed the default checkpoint is zamba_time_distributed.ckpt scheduler_config (zamba.models.config.SchedulerConfig, optional) \u00b6 A PyTorch learning rate schedule to adjust the learning rate based on the number of epochs. Scheduler can either be default (the default), None , or a torch.optim.lr_scheduler . If default , model_name (time_distributed|slowfast|european, optional) \u00b6 Name of the model to use for inference. The three model options that ship with zamba are time_distributed , slowfast , and european . See the Available Models page for details. Defaults to time_distributed dry_run (bool, optional) \u00b6 Specifying True is useful for trying out model implementations more quickly by running only a single batch of train and validation. Defaults to False batch_size (int, optional) \u00b6 The batch size to use for training. Defaults to 8 auto_lr_find (bool, optional) \u00b6 Whether to run a learning rate finder algorithm when calling pytorch_lightning.trainer.tune() to find the optimal initial learning rate. See the PyTorch Lightning docs for more details. Defaults to True backbone_finetune (bool, optional) \u00b6 Finetune a backbone model based on a learning rate user-defined scheduling. Derined from Pytorch Lightning's built-in BackboneFinetuning , but with the ability to freeze batch norm layers during the freeze phase. See zamba.pytorch.finetuning for details.<!-- TODO: add github link><!--> Defaults to False backbone_finetune_params (zamba.models.config.BackboneFinetuneConfig, optional) \u00b6 Parameters to pass to the BackboneFinetuning <!-- TODO: add link to github source code><!-->class if backbone_finetune is True . The default values are specified in the BackboneFinetuneConfig <!-- TODO: add link to github source code><!--> class: BackboneFinetuneConfig(unfreeze_backbone_at_epoch=15, backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False, train_bn=False, verbose=True) gpus (int, optional) \u00b6 The number of GPUs to use during training. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available on the machine. num_workers (int, optional) \u00b6 The number of CPUs to use during training. By default, it will be set to either one less than the number of CPUs in the system, or one if there is only one CPU in the system. max_epochs (int, optional) \u00b6 The maximum number of epochs to run during training. Defaults to None early_stopping (bool, optional) \u00b6 Whether to monitor a metric during model training and stop training when the metric stops improving. Uses pytorch_lightning.callbacks.early_stopping . Defaults to True early_stopping_params (zamba.models.config.EarlyStoppingConfig, optional) \u00b6 Parameters to pass to Pytorch lightning's EarlyStopping if early_stopping is True . The default values are specified in the EarlyStoppingConfig <!-- TODO: add link to github source code><!--> class: EarlyStoppingConfig(monitor='val_macro_f1', patience=3, verbose=True, mode='max') tensorboard_log_dir (str, optional) \u00b6 Pytorch Lightning can log to a local file system in TensorBoard format with TensorBoardLogger . The directory in which to save these logs is set to zamba/models/<tensorboard_log_dir>/ . Defaults to tensorboard_logs weight_download_region [us|eu|asia] \u00b6 Because zamba needs to download pretrained weights for the neural network architecture, we make these weights available in different regions. us is the default, but if you are not in the US you should use either eu for the European Union or asia for Asia Pacific to make sure that these download as quickly as possible for you. cache_dir (FilePath, optional) \u00b6 The directory where the trained model will be saved. If it is None (the default), the model will be cached to an automatic temp directory at ~/.cache/zamba split_proportions (dict(str, int), optional) \u00b6 The proportion of data to use during training, validation, and as a holdout set. Defaults to {\"train\": 3, \"val\": 1, \"holdout\": 1} save_directory (Path, optional) \u00b6 Directory in which to save model checkpoint and configuration file. If not specified, will save to a folder called 'zamba_{model_name}' in your working directory. skip_load_validation (bool, optional) \u00b6 By default, before kicking off training zamba will iterate through all of the videos in the training data and verify that each can be loaded. Setting skip_load_verification to True skips this step. Validation can be very time intensive depending on the number of videos. It is recommended to run validation once, but not on future training runs if the videos have not changed. Defaults to False from_scratch (bool, optional) \u00b6 Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Only used if labels is not None. Defaults to False predict_all_zamba_species (bool, optional) \u00b6 Whether the species outputted by the model should be all zamba species. If you want the model classes to only be the species in your labels file, set to False . Only used if labels is not None . If either predict_all_zamba_species is False or the labels contain species that are not in the model, the model head will be replaced. Defaults to True .","title":"All Optional Arguments"},{"location":"configurations/#all-optional-arguments","text":"Three main configuration classes are specific in zamba : VideoLoaderConfig : Defines all possible parameters for how videos are loaded PredictConfig : Defines all possible parameters for model inference TrainConfig : Defines all possible parameters for model training Here's a helpful diagram which shows how everything is related.","title":"All Optional Arguments"},{"location":"configurations/#video-loading-arguments","text":"The VideoLoaderConfig class <!-- TODO: add link to source code><!--> defines all of the optional parameters that can be specified for how videos are loaded before either inference or training. This includes selecting which frames to use from each video. >> from zamba.data.video import VideoLoaderConfig >> help ( VideoLoaderConfig ) class VideoLoaderConfig ( pydantic . main . BaseModel ) | VideoLoaderConfig ( * , crop_bottom_pixels : int = None , i_frames : bool = False , scene_threshold : float = None , megadetector_lite_config : zamba . models . megadetector_lite_yolox . MegadetectorLiteYoloXConfig = None , video_height : int = None , video_width : int = None , total_frames : int = None , ensure_total_frames : bool = True , fps : float = None , early_bias : bool = False , frame_indices : List [ int ] = None , evenly_sample_total_frames : bool = False , pix_fmt : str = 'rgb24' , resize_after_frame_selection : bool = False ) -> None Let's go through each of those arguments.","title":"Video loading arguments"},{"location":"configurations/#prediction-arguments","text":"All possible model inference parameters are defined by the PredictConfig class<!-- TODO: add link to class definition on github><!-->. Let's see the class documentation in Python: >> from zamba.models.config import PredictConfig >> help ( PredictConfig ) class PredictConfig ( ZambaBaseModel ) | PredictConfig ( * , data_directory : pydantic . types . DirectoryPath = PosixPath ( '/home/ubuntu/zamba-algorithms' ), filepaths : pydantic . types . FilePath = None , checkpoint : pydantic . types . FilePath = None , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , species : List [ str ] = None , gpus : Union [ List [ int ], str , int ] = 1 , num_workers : int = 7 , batch_size : int = 8 , save : Union [ bool , pathlib . Path ] = True , dry_run : bool = False , proba_threshold : float = None , output_class_names : bool = False , weight_download_region : zamba . models . utils . RegionEnum = 'us' , cache_dir : pathlib . Path = None , skip_load_validation : bool = False ) -> None ... Either data_directory or filepaths must be specified to instantiate PredictConfig . Otherwise the current working directory will be used as the default data_directory .","title":"Prediction arguments"},{"location":"configurations/#training-arguments","text":"All possible model training parameters are defined by the TrainConfig class<!-- TODO: add link to class definition><!-->. Let's see the class documentation in Python: >> from zamba.models.config import TrainConfig >> help ( TrainConfig ) class TrainConfig ( ZambaBaseModel ) | TrainConfig ( * , labels : Union [ pydantic . types . FilePath , pandas . core . frame . DataFrame ], data_directory : pydantic . types . DirectoryPath = PosixPath ( '/home/ubuntu/zamba-algorithms' ), checkpoint : pydantic . types . FilePath = None , scheduler_config : Union [ str , zamba . models . config . SchedulerConfig , NoneType ] = 'default' , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , dry_run : Union [ bool , int ] = False , batch_size : int = 8 , auto_lr_find : bool = True , backbone_finetune : bool = False , backbone_finetune_params : zamba . models . config . BackboneFinetuneConfig = BackboneFinetuneConfig ( unfreeze_backbone_at_epoch = 15 , backbone_initial_ratio_lr = 0.01 , multiplier = 1 , pre_train_bn = False , train_bn = False , verbose = True ), gpus : Union [ List [ int ], str , int ] = 1 , num_workers : int = 7 , max_epochs : int = None , early_stopping : bool = True , early_stopping_params : zamba . models . config . EarlyStoppingConfig = EarlyStoppingConfig ( monitor = 'val_macro_f1' , patience = 3 , verbose = True , mode = 'max' ), tensorboard_log_dir : str = 'tensorboard_logs' , weight_download_region : zamba . models . utils . RegionEnum = 'us' , cache_dir : pathlib . Path = None , split_proportions : Dict [ str , int ] = { 'train' : 3 , 'val' : 1 , 'holdout' : 1 }, save_directory : pathlib . Path = None , skip_load_validation : bool = False , from_scratch : bool = False , predict_all_zamba_species : bool = True ) -> None ... data_directory and labels must be specified to instantiate TrainConfig . If no data_directory is provided, it will default the current working directory.","title":"Training arguments"},{"location":"contribute/","text":"Help Make zamba Better \u00b6 zamba is an open source project, which means you can help make it better! Develop the Github Repository \u00b6 To get involved, check out the Github code repository . There you can find open issues , project goals , and plenty of comments and links to help you along. zamba uses continuous integration and test-driven-development to ensure that we always have a working project. So what are you waiting for? git going! Installation for development \u00b6 To install zamba for development, you need to clone the git repository and then install the cloned version of the library for local development. To install for development: $ git clone https://github.com/drivendataorg/zamba.git $ cd zamba $ pip install --editable . Running the zamba test suite \u00b6 The included Makefile contains code that uses pytest to run all tests in zamba/tests . The command is (from the project root), $ make tests Submit additional training videos \u00b6 If you have additional labeled videos that may be useful for improving the basic models that ship with zamba , we'd love to hear from you! You can get in touch at info@drivendata.org","title":"Contribute to zamba"},{"location":"contribute/#help-make-zamba-better","text":"zamba is an open source project, which means you can help make it better!","title":"Help Make zamba Better"},{"location":"contribute/#develop-the-github-repository","text":"To get involved, check out the Github code repository . There you can find open issues , project goals , and plenty of comments and links to help you along. zamba uses continuous integration and test-driven-development to ensure that we always have a working project. So what are you waiting for? git going!","title":"Develop the Github Repository"},{"location":"contribute/#installation-for-development","text":"To install zamba for development, you need to clone the git repository and then install the cloned version of the library for local development. To install for development: $ git clone https://github.com/drivendataorg/zamba.git $ cd zamba $ pip install --editable .","title":"Installation for development"},{"location":"contribute/#running-the-zamba-test-suite","text":"The included Makefile contains code that uses pytest to run all tests in zamba/tests . The command is (from the project root), $ make tests","title":"Running the zamba test suite"},{"location":"contribute/#submit-additional-training-videos","text":"If you have additional labeled videos that may be useful for improving the basic models that ship with zamba , we'd love to hear from you! You can get in touch at info@drivendata.org","title":"Submit additional training videos"},{"location":"debugging/","text":"Debugging \u00b6 Before kicking off a full run of inference or model training, we recommend testing your code with a \"dry run\". If you are generating predictions, this will run one batch of inference to quickly detect any bugs. If you are trainig a model, this will run one training and validation batch for one epoch. If the dry run completes successfully, predict and train away! CLI $ zamba predict --data-dir example_vids/ --dry-run $ zamba train --data-dir example_vids/ --labels example_labels.csv --dry-run Python In Python, add dry_run=True to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_directory = \"example_vids/\" , dry_run = True ) GPU memory errors \u00b6 The dry run will also catch any GPU memory errors. If you hit a GPU memory error, there are a couple fixes. Reducing the batch size \u00b6 CLI zamba train --data-dir example_vids/ --labels example_labels.csv --batch-size 1 Python In Python, add batch_size to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_directory = \"example_vids/\" , batch_size = 1 ) Decreasing video size \u00b6 Resize video frames to be smaller before they are passed to the model. The default for all three models is 224x224 pixels. video_height and video_width cannot be passed directly to the command line, so if you are using the CLI these must be specified in a YAML file . YAML file video_loader_config : video_height : 100 video_width : 100 total_frames : 16 # total_frames is always required Python video_loader_config = VideoLoaderConfig ( video_height = 100 , video_width = 100 , total_frames = 16 ) # total_frames is always required Reducing num_workers \u00b6 Reduce the number of workers (subprocesses) used for data loading. By default, num_workers will be set to either one less than the number of CPUs in the system, or one if there is only one CPU in the system. num_workers cannot be passed directly to the command line, so if you are using the CLI it must be specified in a YAML file . YAML file In a YAML file, add num_workers to predict_config or train_config : train_config : data_directory : \"example_vids/\" # required labels : \"example_labels.csv\" # required num_workers : 1 Python In Python, add num_workers to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_directory = \"example_vids/\" , num_workers = 1 )","title":"Debugging"},{"location":"debugging/#debugging","text":"Before kicking off a full run of inference or model training, we recommend testing your code with a \"dry run\". If you are generating predictions, this will run one batch of inference to quickly detect any bugs. If you are trainig a model, this will run one training and validation batch for one epoch. If the dry run completes successfully, predict and train away! CLI $ zamba predict --data-dir example_vids/ --dry-run $ zamba train --data-dir example_vids/ --labels example_labels.csv --dry-run Python In Python, add dry_run=True to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_directory = \"example_vids/\" , dry_run = True )","title":"Debugging"},{"location":"debugging/#gpu-memory-errors","text":"The dry run will also catch any GPU memory errors. If you hit a GPU memory error, there are a couple fixes.","title":"GPU memory errors"},{"location":"extra-options/","text":"Guide to Common Optional Parameters \u00b6 There are a LOT of ways to customize model training or inference. Here, we take that elephant-sized list of options and condense it to a manageable monkey-sized list of common considerations. To read about all possible customizations, see the All Optional Arguments page. Many of the options below cannot be passed directly to the command line. Instead, some must be passed as part of a YAML configuration file. For example: $ zamba train --config path_to_your_config_file.yaml For using a YAML file with the Python package and other details, see the YAML Configuration File page. Downloading model weights \u00b6 zamba needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Model weights are stored on servers in three locations, and downloading weights from the server closest to you will run the fastest. By default, weights will be downloaded from the US. To specify a different region: CLI zamba predict --data-dir example_vids/ --weight_download_region asia Python In Python this can be specified in PredictConfig or TrainConfig : predict_config = PredictConfig ( data_directory = \"example_vids/\" , weight_download_region = 'asia' , ) The options for weight_download_region are us , eu , and asia . Once a model's weights are downloaded, the tool will use the local version and will not need to perform this download again. Video size \u00b6 When zamba loads videos prior to either inference or training, it resizes all of the video frames before feeding them into a model. Higher resolution videos will lead to more detailed accuracy in prediction, but will use more memory and take longer to either predict on or train from. The default video loading configuration for all three pretrained models resizes images to 224x224 pixels. Say that you have a large number of videos, and you are more concerned with detecting blank v. non-blank videos than with identifying different species. In this case, you may not need a very high resolution and iterating through all of your videos with a high resolution would take a very long time. To resize all images to 50x50 pixels instead of the default 224x224: YAML file video_loader_config : video_height : 50 video_width : 50 total_frames : 16 # total_frames must always be specified Python In Python, video resizing can be specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( video_height = 50 , video_width = 50 , total_frames = 16 ) # total_frames must always be specified Frame selection \u00b6 Each video is simply a series of frames, or images. Most of the videos on which zamba was trained had 30 frames per second. That means even just a 15-second video would contain 450 frames. The model only trains or generates prediction based on a subset of the frames in a video, because using every frame would be far too computationally intensive. There are a number of different ways to select frames. For a full list of options, see the section on Video loading arguments . A few common approaches are explained below. Early bias \u00b6 Some camera traps begin recording a video when movement is detected. If this is the case, you may be more likely to see an animal towards when the video starts. Setting early_bias to True selects 16 frames towards the beginning of a video. YAML File video_loader_config : early_bias : True # ... other parameters Python In Python, early_bias is specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( early_bias = True , ... ) This method was used by the winning solution of the Pri-matrix Factorization machine learning competition, which was the basis for zamba v1. Evenly distributed frames \u00b6 A simple option is to sample frames that are evenly distributed throughout a video. For example, to select 32 evenly distributed frames, add the following to a YAML configuration file : YAML file video_loader_config : total_frames : 32 evenly_sample_total_frames : True ensure_total_frames : True # ... other parameters Python In Python, these arguments can be specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( total_frames = 32 , evenly_sample_total_frames = True , ensure_total_frames = True , ... ) MegadetectorLiteYoloX \u00b6 You can use a pretrained object detection model called MegadetectorLiteYoloX to select only the frames that are mostly likely to contain an animal. This is the default strategy for all three pretrained models. The parameter megadetector_lite_config is used to specify any arguments that should be passed to the megadetector model. If megadetector_lite_config is None, the MegadetectorLiteYoloX model will not be used. For example, to take the 16 frames with the highest probability of detection: YAML file video_loader_config : megadetector_lite_config : n_frames : 16 fill_mode : \"score_sorted\" # ... other parameters Python In Python, these can be specified in the megadetector_lite_config argument passed to VideoLoaderConfig : video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 16 , }, total_frames = 16 , ) train_config = TrainConfig ( data_directory = \"example_vids/\" , labels = \"example_labels.csv\" ,) train_model ( video_loader_config = video_loader_config , train_config = train_config ) To see all of the options that can be passed to MegadetectorLiteYoloX , see the MegadetectorLiteYoloXConfig class. <!-- TODO: add link to github code><!--> And that's just the tip of the iceberg! See the All Optional Arguments page for more possibilities.","title":"Guide to Common Optional Parameters"},{"location":"extra-options/#guide-to-common-optional-parameters","text":"There are a LOT of ways to customize model training or inference. Here, we take that elephant-sized list of options and condense it to a manageable monkey-sized list of common considerations. To read about all possible customizations, see the All Optional Arguments page. Many of the options below cannot be passed directly to the command line. Instead, some must be passed as part of a YAML configuration file. For example: $ zamba train --config path_to_your_config_file.yaml For using a YAML file with the Python package and other details, see the YAML Configuration File page.","title":"Guide to Common Optional Parameters"},{"location":"extra-options/#downloading-model-weights","text":"zamba needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Model weights are stored on servers in three locations, and downloading weights from the server closest to you will run the fastest. By default, weights will be downloaded from the US. To specify a different region: CLI zamba predict --data-dir example_vids/ --weight_download_region asia Python In Python this can be specified in PredictConfig or TrainConfig : predict_config = PredictConfig ( data_directory = \"example_vids/\" , weight_download_region = 'asia' , ) The options for weight_download_region are us , eu , and asia . Once a model's weights are downloaded, the tool will use the local version and will not need to perform this download again.","title":"Downloading model weights"},{"location":"extra-options/#video-size","text":"When zamba loads videos prior to either inference or training, it resizes all of the video frames before feeding them into a model. Higher resolution videos will lead to more detailed accuracy in prediction, but will use more memory and take longer to either predict on or train from. The default video loading configuration for all three pretrained models resizes images to 224x224 pixels. Say that you have a large number of videos, and you are more concerned with detecting blank v. non-blank videos than with identifying different species. In this case, you may not need a very high resolution and iterating through all of your videos with a high resolution would take a very long time. To resize all images to 50x50 pixels instead of the default 224x224: YAML file video_loader_config : video_height : 50 video_width : 50 total_frames : 16 # total_frames must always be specified Python In Python, video resizing can be specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( video_height = 50 , video_width = 50 , total_frames = 16 ) # total_frames must always be specified","title":"Video size"},{"location":"extra-options/#frame-selection","text":"Each video is simply a series of frames, or images. Most of the videos on which zamba was trained had 30 frames per second. That means even just a 15-second video would contain 450 frames. The model only trains or generates prediction based on a subset of the frames in a video, because using every frame would be far too computationally intensive. There are a number of different ways to select frames. For a full list of options, see the section on Video loading arguments . A few common approaches are explained below.","title":"Frame selection"},{"location":"extra-options/#early-bias","text":"Some camera traps begin recording a video when movement is detected. If this is the case, you may be more likely to see an animal towards when the video starts. Setting early_bias to True selects 16 frames towards the beginning of a video. YAML File video_loader_config : early_bias : True # ... other parameters Python In Python, early_bias is specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( early_bias = True , ... ) This method was used by the winning solution of the Pri-matrix Factorization machine learning competition, which was the basis for zamba v1.","title":"Early bias"},{"location":"extra-options/#evenly-distributed-frames","text":"A simple option is to sample frames that are evenly distributed throughout a video. For example, to select 32 evenly distributed frames, add the following to a YAML configuration file : YAML file video_loader_config : total_frames : 32 evenly_sample_total_frames : True ensure_total_frames : True # ... other parameters Python In Python, these arguments can be specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( total_frames = 32 , evenly_sample_total_frames = True , ensure_total_frames = True , ... )","title":"Evenly distributed frames"},{"location":"extra-options/#megadetectorliteyolox","text":"You can use a pretrained object detection model called MegadetectorLiteYoloX to select only the frames that are mostly likely to contain an animal. This is the default strategy for all three pretrained models. The parameter megadetector_lite_config is used to specify any arguments that should be passed to the megadetector model. If megadetector_lite_config is None, the MegadetectorLiteYoloX model will not be used. For example, to take the 16 frames with the highest probability of detection: YAML file video_loader_config : megadetector_lite_config : n_frames : 16 fill_mode : \"score_sorted\" # ... other parameters Python In Python, these can be specified in the megadetector_lite_config argument passed to VideoLoaderConfig : video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 16 , }, total_frames = 16 , ) train_config = TrainConfig ( data_directory = \"example_vids/\" , labels = \"example_labels.csv\" ,) train_model ( video_loader_config = video_loader_config , train_config = train_config ) To see all of the options that can be passed to MegadetectorLiteYoloX , see the MegadetectorLiteYoloXConfig class. <!-- TODO: add link to github code><!--> And that's just the tip of the iceberg! See the All Optional Arguments page for more possibilities.","title":"MegadetectorLiteYoloX"},{"location":"install/","text":"Installing zamba \u00b6 Zamba has been developed and tested on macOS and Ubuntu Linux for both CPU and GPU configurations. To install zamba \u00b6 1. Install prerequisites \u00b6 Prerequisites: Python 3.7 or 3.8 FFmpeg Python 3.7 or 3.8 \u00b6 We recommend Python installation using Anaconda for all platforms. For more information about how to install Anaconda, here are some useful YouTube videos of installation: Anaconda download link Windows install video macOS installation video FFmpeg version 4.3 \u00b6 FFmpeg is an open source library for loading videos of different codecs. Using FFmpeg means that zamba can be flexible in terms of the video formats we support. FFmpeg can be installed on all different platforms, but requires some additional configuration depending on the platform. Here are some videos and instructions walking through FFmpeg installation: FFmpeg download link Install on Ubuntu or Linux . In the command line, enter sudo apt update and then sudo apt install ffmpeg . MacOS install video First, install Homebrew . Then run brew install ffmpeg To check that FFmpeg is installed, run ffmpeg : $ ffmpeg ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers built with Apple clang version 12.0.0 (clang-1200.0.32.29) ... To check your installed version, run ffmpeg -version . 2. Install zamba \u00b6 On macOS, run these commands in the terminal (\u2318+space, \"Terminal\"). On Windows, run them in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt). To install for development: $ pip install zamba To check what version of zamba you have installed: $ pip show zamba To update zamba to the most recent version if needed: $ pip install -U zamba Operating Systems that have been tested \u00b6 macOS \u00b6 zamba has been tested on macOS High Sierra. Linux \u00b6 zamba has been tested on Ubuntu versions 16 and 17. Windows \u00b6 zamba has been tested on Windows 10. Using GPU \u00b6 zamba is much faster on a machine with a graphics processing unit (GPU), but has also been developed and tested for machines without GPU(s). To use a GPU, you must be using an NVIDIA GPU , have installed and configured CUDA , and have installed and configured CuDNN per their specifications.","title":"Installing Zamba"},{"location":"install/#installing-zamba","text":"Zamba has been developed and tested on macOS and Ubuntu Linux for both CPU and GPU configurations.","title":"Installing zamba"},{"location":"install/#to-install-zamba","text":"","title":"To install zamba"},{"location":"install/#1-install-prerequisites","text":"Prerequisites: Python 3.7 or 3.8 FFmpeg","title":"1. Install prerequisites"},{"location":"install/#2-install-zamba","text":"On macOS, run these commands in the terminal (\u2318+space, \"Terminal\"). On Windows, run them in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt). To install for development: $ pip install zamba To check what version of zamba you have installed: $ pip show zamba To update zamba to the most recent version if needed: $ pip install -U zamba","title":"2. Install zamba"},{"location":"install/#operating-systems-that-have-been-tested","text":"","title":"Operating Systems that have been tested"},{"location":"install/#macos","text":"zamba has been tested on macOS High Sierra.","title":"macOS"},{"location":"install/#linux","text":"zamba has been tested on Ubuntu versions 16 and 17.","title":"Linux"},{"location":"install/#windows","text":"zamba has been tested on Windows 10.","title":"Windows"},{"location":"install/#using-gpu","text":"zamba is much faster on a machine with a graphics processing unit (GPU), but has also been developed and tested for machines without GPU(s). To use a GPU, you must be using an NVIDIA GPU , have installed and configured CUDA , and have installed and configured CuDNN per their specifications.","title":"Using GPU"},{"location":"models/","text":"Available Models \u00b6 The algorithms in zamba are designed to identify species of animals that appear in camera trap videos. There are three models that ship with the zamba package: time_distributed , slowfast , and european . For more details of each, read on! Basic usage \u00b6 Model Use cases Strengths Geography time_distributed Model training or fine tuning Classifying species Running more quickly Central and west Africa european Western Europe slowfast Detailed prediction of blank vs. non-blank Identifying blank vs. non-blank videos Central and west Africa time_distributed and european use the same basic algorithm. The main difference is that they predict different species based on their intended geography. For training or fine tuning, either the time_distributed and european model is recommended. These run much more quickly thatn the slowfast model. For inference, slowfast is recommended if the highest priority is differentiating between blank and non-blank videos. If the priority is species classification, either time_distributed or european is recommended based on the given geography. What species can zamba detect? \u00b6 time_distributed and slowfast are both trained to identify 32 common species from central and west Africa. The possible class labels in these models are: aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal european is trained to identify 11 common species in western Europe. The possible class labels are: bird blank domestic_cat european_badger european_beaver european_hare european_roe_deer north_american_raccoon red_fox unidentified weasel wild_boar time_distributed model \u00b6 Algorithm \u00b6 The time_distributed model was built by re-training a well-known image classification architecture called EfficientNetV2 to identify the species in our camera trap videos (Tan, M., & Le, Q., 2019). EfficientNetV2 models are convolutional neural networks designed to jointly optimize model size and training speed. EfficientNetV2 is image native, meaning it classifies each frame separately when generating predictions. It does take into account the relationship between frames in the video. Training data \u00b6 time_distributed was trained using data collected and annotated by partners at The Max Planck Institute for Evolutionary Anthropology and Chimp & See . The data included camera trap videos from: Dzanga-Sangha Protected Area, Central African Republic Gorongosa National Park, Mozambique Grumeti Game Reserve, Tanzania Lop\u00e9 National Park, Gabon Moyen-Bafing National Park, Guinea Nouabale-Ndoki National Park, Republic of the Congo Salonga National Park, Democratic Republic of the Congo Ta\u00ef National Park, C\u00f4te d'Ivoire Default configuration \u00b6 By default, an efficient object detection model called MegadetectorLiteYoloX is run on all frames to determine which are the most likely to contain an animal. Then time_distributed is run on only the 16 frames with the highest predicted probability of detection. By default, videos are resized to 224x224 pixels. The full default video loading configuration is: video_loader_config : video_height : 224 video_width : 224 crop_bottom_pixels : 50 ensure_total_frames : True megadetector_lite_config : confidence : 0.25 fill_model : \"score_sorted\" n_frames : 16 total_frames : 16 Requirements \u00b6 The above is pulled in by default if time_distributed is used in the command line. If you are passing in a custom YAML configuration file or using zamba as a Python package, at a minimum you must specify: YAML file video_loader_config : video_height : # any integer video_width : # any integer total_frames : 16 Python video_loader_config = VideoLoaderConfig ( video_height =... , # any integer video_width =... , # any integer total_frames = 16 ) slowfast model \u00b6 Algorithm \u00b6 The slowfast model was built by re-training a video classification backbone called SlowFast (Feichtenhofer, C., Fan, H., Malik, J., & He, K., 2019). SlowFast refers to the two model pathways involved: one that operates at a low frame rate to capture spatial semantics, and one that operates at a high frame rate to capture motion over time. The basic architectures are deep neural networks using pytorch . Source: Feichtenhofer, C., Fan, H., Malik, J., & He, K. (2019). Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 6202-6211). Unlike time_distributed , slowfast is video native. This means it takes into account the relationship between frames in a video, rather than running independently on each frame. Training data \u00b6 The slowfast model was trained using the same data as the time_distributed model . Default configuration \u00b6 By default, an efficient object detection model called MegadetectorLiteYoloX is run on all frames to determine which are the most likely to contain an animal. Then slowfast is run on only the 32 frames with the highest predicted probability of detection. By default, videos are resized to 224x224 pixels. The full default video loading configuration is: video_loader_config : video_height : 224 video_width : 224 crop_bottom_pixels : 50 ensure_total_frames : True megadetector_lite_config : confidence : 0.25 fill_model : \"score_sorted\" n_frames : 32 total_frames : 32 Requirements \u00b6 The above is pulled in by default if slowfast is used in the command line. If you are passing in a custom YAML configuration file or using zamba as a Python package, at a minimum you must specify: YAML file video_loader_config : video_height : # any integer >= 200 video_width : # any integer >= 200 total_frames : 32 Python video_loader_config = VideoLoaderConfig ( video_height =... , # any integer >= 200 video_width =... , # any integer >= 200 total_frames = 32 ) european model \u00b6 Algorithm \u00b6 The european model has the same backbone as the time_distributed model, but is trained on data from camera traps in western Europe instead of central and west Africa. The european model was built by re-training a well-known image classification architecture called EfficientNetV2 to identify the species in our camera trap videos (Tan, M., & Le, Q., 2019). EfficientNetV2 models are convolutional neural networks designed to jointly optimize model size and training speed. EfficientNetV2 is image native, meaning it classifies each frame separately when generating predictions. It does take into account the relationship between frames in the video. european combines the EfficientNetV2 architecture with an open-source image object detection model to implement frame selection. The YOLOX detection model is run on all frames in a video. Only the frames with the highest probability of detection are then passed to the more computationally intensive EfficientNetV2 for detailed detection and classification. Training data \u00b6 The european model is built by starting with the fully trained time_distributed model. The network is then finetuned with data collected and annotated by partners at The Max Planck Institute for Evolutionary Anthropology . The finetuning data included camera trap videos from Hintenteiche bei Biesenbrow, Germany. Default configuration \u00b6 By default, an efficient object detection model called MegadetectorLiteYoloX is run on all frames to determine which are the most likely to contain an animal. Then european is run on only the 16 frames with the highest predicted probability of detection. By default, videos are resized to 224x224 pixels. The full default video loading configuration is: video_loader_config : video_height : 224 video_width : 224 crop_bottom_pixels : 50 ensure_total_frames : True megadetector_lite_config : confidence : 0.25 fill_model : \"score_sorted\" n_frames : 16 total_frames : 16 Requirements \u00b6 The above is pulled in by default if european is used in the command line. If you are passing in a custom YAML configuration file or using zamba as a Python package, at a minimum you must specify: YAML file video_loader_config : video_height : # any integer video_width : # any integer total_frames : 16 Python video_loader_config = VideoLoaderConfig ( video_height =... , # any integer video_width =... , # any integer total_frames = 16 ) MegadetectorLiteYoloX \u00b6 Running any of the three models that ship with zamba on all frames of a video would be incredibly time consuming and computationally intensive. Instead, zamba uses a more efficient object detection model called MegadetectorLiteYoloX to determine the likelihood that each frame contains an animal. Then, only the frames with the highest probability of detection can be passed to the model. MegadetectorLiteYoloX combines two open-source models: Megadetector is a pretrained image model designed to detect animals, people, and vehicles in camera trap videos. YOLOX is a high-performance, lightweight object detection model that is much less computationally intensive than Megadetector. Megadetector is much better at identifying frames with animals than YOLOX, but too computationally intensive to run on every frame. MegadetectorLiteYoloX was created by training the YOLOX model using the predictions of the Megadetector as ground truth - this method is called student-teacher training .","title":"Available Models"},{"location":"models/#available-models","text":"The algorithms in zamba are designed to identify species of animals that appear in camera trap videos. There are three models that ship with the zamba package: time_distributed , slowfast , and european . For more details of each, read on!","title":"Available Models"},{"location":"models/#basic-usage","text":"Model Use cases Strengths Geography time_distributed Model training or fine tuning Classifying species Running more quickly Central and west Africa european Western Europe slowfast Detailed prediction of blank vs. non-blank Identifying blank vs. non-blank videos Central and west Africa time_distributed and european use the same basic algorithm. The main difference is that they predict different species based on their intended geography. For training or fine tuning, either the time_distributed and european model is recommended. These run much more quickly thatn the slowfast model. For inference, slowfast is recommended if the highest priority is differentiating between blank and non-blank videos. If the priority is species classification, either time_distributed or european is recommended based on the given geography.","title":"Basic usage"},{"location":"models/#what-species-can-zamba-detect","text":"time_distributed and slowfast are both trained to identify 32 common species from central and west Africa. The possible class labels in these models are: aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal european is trained to identify 11 common species in western Europe. The possible class labels are: bird blank domestic_cat european_badger european_beaver european_hare european_roe_deer north_american_raccoon red_fox unidentified weasel wild_boar","title":"What species can zamba detect?"},{"location":"models/#time_distributed-model","text":"","title":"time_distributed model"},{"location":"models/#algorithm","text":"The time_distributed model was built by re-training a well-known image classification architecture called EfficientNetV2 to identify the species in our camera trap videos (Tan, M., & Le, Q., 2019). EfficientNetV2 models are convolutional neural networks designed to jointly optimize model size and training speed. EfficientNetV2 is image native, meaning it classifies each frame separately when generating predictions. It does take into account the relationship between frames in the video.","title":"Algorithm"},{"location":"models/#training-data","text":"time_distributed was trained using data collected and annotated by partners at The Max Planck Institute for Evolutionary Anthropology and Chimp & See . The data included camera trap videos from: Dzanga-Sangha Protected Area, Central African Republic Gorongosa National Park, Mozambique Grumeti Game Reserve, Tanzania Lop\u00e9 National Park, Gabon Moyen-Bafing National Park, Guinea Nouabale-Ndoki National Park, Republic of the Congo Salonga National Park, Democratic Republic of the Congo Ta\u00ef National Park, C\u00f4te d'Ivoire","title":"Training data"},{"location":"models/#default-configuration","text":"By default, an efficient object detection model called MegadetectorLiteYoloX is run on all frames to determine which are the most likely to contain an animal. Then time_distributed is run on only the 16 frames with the highest predicted probability of detection. By default, videos are resized to 224x224 pixels. The full default video loading configuration is: video_loader_config : video_height : 224 video_width : 224 crop_bottom_pixels : 50 ensure_total_frames : True megadetector_lite_config : confidence : 0.25 fill_model : \"score_sorted\" n_frames : 16 total_frames : 16","title":"Default configuration"},{"location":"models/#requirements","text":"The above is pulled in by default if time_distributed is used in the command line. If you are passing in a custom YAML configuration file or using zamba as a Python package, at a minimum you must specify: YAML file video_loader_config : video_height : # any integer video_width : # any integer total_frames : 16 Python video_loader_config = VideoLoaderConfig ( video_height =... , # any integer video_width =... , # any integer total_frames = 16 )","title":"Requirements"},{"location":"models/#slowfast-model","text":"","title":"slowfast model"},{"location":"models/#algorithm_1","text":"The slowfast model was built by re-training a video classification backbone called SlowFast (Feichtenhofer, C., Fan, H., Malik, J., & He, K., 2019). SlowFast refers to the two model pathways involved: one that operates at a low frame rate to capture spatial semantics, and one that operates at a high frame rate to capture motion over time. The basic architectures are deep neural networks using pytorch . Source: Feichtenhofer, C., Fan, H., Malik, J., & He, K. (2019). Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 6202-6211). Unlike time_distributed , slowfast is video native. This means it takes into account the relationship between frames in a video, rather than running independently on each frame.","title":"Algorithm"},{"location":"models/#training-data_1","text":"The slowfast model was trained using the same data as the time_distributed model .","title":"Training data"},{"location":"models/#default-configuration_1","text":"By default, an efficient object detection model called MegadetectorLiteYoloX is run on all frames to determine which are the most likely to contain an animal. Then slowfast is run on only the 32 frames with the highest predicted probability of detection. By default, videos are resized to 224x224 pixels. The full default video loading configuration is: video_loader_config : video_height : 224 video_width : 224 crop_bottom_pixels : 50 ensure_total_frames : True megadetector_lite_config : confidence : 0.25 fill_model : \"score_sorted\" n_frames : 32 total_frames : 32","title":"Default configuration"},{"location":"models/#requirements_1","text":"The above is pulled in by default if slowfast is used in the command line. If you are passing in a custom YAML configuration file or using zamba as a Python package, at a minimum you must specify: YAML file video_loader_config : video_height : # any integer >= 200 video_width : # any integer >= 200 total_frames : 32 Python video_loader_config = VideoLoaderConfig ( video_height =... , # any integer >= 200 video_width =... , # any integer >= 200 total_frames = 32 )","title":"Requirements"},{"location":"models/#european-model","text":"","title":"european model"},{"location":"models/#algorithm_2","text":"The european model has the same backbone as the time_distributed model, but is trained on data from camera traps in western Europe instead of central and west Africa. The european model was built by re-training a well-known image classification architecture called EfficientNetV2 to identify the species in our camera trap videos (Tan, M., & Le, Q., 2019). EfficientNetV2 models are convolutional neural networks designed to jointly optimize model size and training speed. EfficientNetV2 is image native, meaning it classifies each frame separately when generating predictions. It does take into account the relationship between frames in the video. european combines the EfficientNetV2 architecture with an open-source image object detection model to implement frame selection. The YOLOX detection model is run on all frames in a video. Only the frames with the highest probability of detection are then passed to the more computationally intensive EfficientNetV2 for detailed detection and classification.","title":"Algorithm"},{"location":"models/#training-data_2","text":"The european model is built by starting with the fully trained time_distributed model. The network is then finetuned with data collected and annotated by partners at The Max Planck Institute for Evolutionary Anthropology . The finetuning data included camera trap videos from Hintenteiche bei Biesenbrow, Germany.","title":"Training data"},{"location":"models/#default-configuration_2","text":"By default, an efficient object detection model called MegadetectorLiteYoloX is run on all frames to determine which are the most likely to contain an animal. Then european is run on only the 16 frames with the highest predicted probability of detection. By default, videos are resized to 224x224 pixels. The full default video loading configuration is: video_loader_config : video_height : 224 video_width : 224 crop_bottom_pixels : 50 ensure_total_frames : True megadetector_lite_config : confidence : 0.25 fill_model : \"score_sorted\" n_frames : 16 total_frames : 16","title":"Default configuration"},{"location":"models/#requirements_2","text":"The above is pulled in by default if european is used in the command line. If you are passing in a custom YAML configuration file or using zamba as a Python package, at a minimum you must specify: YAML file video_loader_config : video_height : # any integer video_width : # any integer total_frames : 16 Python video_loader_config = VideoLoaderConfig ( video_height =... , # any integer video_width =... , # any integer total_frames = 16 )","title":"Requirements"},{"location":"models/#megadetectorliteyolox","text":"Running any of the three models that ship with zamba on all frames of a video would be incredibly time consuming and computationally intensive. Instead, zamba uses a more efficient object detection model called MegadetectorLiteYoloX to determine the likelihood that each frame contains an animal. Then, only the frames with the highest probability of detection can be passed to the model. MegadetectorLiteYoloX combines two open-source models: Megadetector is a pretrained image model designed to detect animals, people, and vehicles in camera trap videos. YOLOX is a high-performance, lightweight object detection model that is much less computationally intensive than Megadetector. Megadetector is much better at identifying frames with animals than YOLOX, but too computationally intensive to run on every frame. MegadetectorLiteYoloX was created by training the YOLOX model using the predictions of the Megadetector as ground truth - this method is called student-teacher training .","title":"MegadetectorLiteYoloX"},{"location":"predict-tutorial/","text":"User Tutorial: Classifying Unlabeled Videos \u00b6 This section walks through how to classify videos using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have unlabeled videos that you want to generate labels for The possible class species labels for your videos are included in the list of possible zamba labels . If your species are not included in this list, you can retrain a model using your own labeled data and then run inference. Basic usage: command line interface \u00b6 Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction in the command line: $ zamba predict --data-dir example_vids/ Required arguments \u00b6 To run zamba predict in the command line, you must specify either --data-dir or --filepaths . --data-dir PATH : Path to the folder containing your videos. --filepaths PATH : Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for filepath . All other flags are optional. To choose a model, either --model or --checkpoint must be specified. Use --model to specify one of the three pretrained models that ship with zamba . Use --checkpoint to run inference with a locally saved model. --model defaults to time_distributed . Basic usage: Python package \u00b6 Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction using the Python package: from zamba.models.model_manager import predict_model from zamba.models.config import PredictConfig from zamba.data.video import VideoLoaderConfig predict_config = PredictConfig ( data_directory = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , total_frames = 16 ) predict_model ( predict_config = predict_config , video_loader_config = video_loader_config ) To specify various parameters when running predict_model , the first step is to instantiate PredictConfig and VideoLoaderConfig with any specifications for prediction and video loading respectively. The only two arguments that can be specified in predict_model are predict_config and video_loader_config . Required arguments \u00b6 To run predict_model in Python, you must specify either data_directory or filepaths when PredictConfig is instantiated. data_directory (DirectoryPath) : Path to the folder containing your videos. filepaths (FilePath) : Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for filepath . In the command line, video loading configurations are loaded by default based on the model being used. This is not the case in Python. There are additional requirements for VideoLoaderConfig based on the model you are using. video_height (int) , video_width (int) : Dimensions for resizing videos as they are loaded. time_distributed or european : The suggested dimensions are 224x224, but any integers are acceptable slowfast : Both must be greater than or equal to 200 total_frames (int) : The number of frames to select from each video and use during inference. time_distributed or european : Must be 16 slowfast : Must be 32 The full recommended VideoLoaderConfig for the time_distributed or european model is: from zamba.data.video import VideoLoaderConfig video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 16 , }, total_frames = 16 , ) The full recommended VideoLoaderConfig for the slowfast model is: video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 32 , }, total_frames = 32 , ) You can see the full default configuration for each model in models/config <!-- TODO: add link to source and update if needed><!-->. For detailed explanations of all possible configuration arguments, see All Optional Arguments . Default behavior \u00b6 In each case, zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) can be interpreted as the probability that animal j is present in video i. By default, predictions will be saved to zamba_predictions.csv . You can save predictions to a custom path using the --save-path argument. $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal example_vids/eleph.MP4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 example_vids/leopard.MP4,0.0,0.0,0.0,0.0,2e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 example_vids/blank.MP4,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 example_vids/chimp.MP4,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,4e-05,0.00162,0.0,0.0,0.0,0.0,0.0,2e-05,2e-05,0.0,1e-05,0.0,0.0038,4e-05,0.0 Step-by-step tutorial \u00b6 1. Specify the path to your videos \u00b6 Save all of your videos in one folder. Your videos should all be saved in formats that are suppored by FFmpeg, which are listed here . Your video folder must contain only valid video files, since zamba will try to load all of the files in the directory. Your videos must all be in the top level of the video folder - zamba does not extract videos from nested directories. Add the path to your video folder with --data-dir . For example, if your videos are in a folder called example_vids : CLI $ zamba predict --data-dir example_vids/ Python predict_config = PredictConfig ( data_directory = 'example_vids/' ) 2. Choose a model for prediction \u00b6 If your camera videos contain species common to central or west Africa: Use the time_distributed model if your priority is species classification Use the slowfast model if your priority is blank vs. non-blank video detection If your videos contain species common to Europe, use the european model . Add the model name to your command with --model . The time_distributed model will be used if no model is specified. For example, if you want to use the slowfast model to classify the videos in example_vids : CLI $ zamba predict --data-dir example_vids/ --model slowfast Python predict_config = PredictConfig ( data_directory = 'example_vids/' , model_name = 'slowfast' ) 3. Choose the output format \u00b6 There are three options for how to format predictions, listed from most information to least: Store all probabilities (default): Return predictions with a row for each filename and a column for each class label, with probabilities between 0 and 1. Cell (i,j) is the probability that animal j is present in video i. Presence/absence: Return predictions with a row for each filename and a column for each class label, with cells indicating either presence or absense based on a user-specified probability threshold. Cell (i, j) indicates whether animal j is present ( 1 ) or not present ( 0 ) in video i. The probability threshold cutoff is specified with --proba-threshold in the CLI. Most likely class: Return predictions with a row for each filename and one column for the most likely class in each video. The most likely class can also be blank. To get the most likely class, add --output-class-names to your command. In Python, it can be specified by adding output_class_names=True when PredictConfig is instantiated. This is not recommended if you'd like to detect more than one species in each video. Say we want to generate predictions for the videos in example_vids indicating which animals are present in each video based on a probability threshold of 50%: CLI $ zamba predict --data-dir example_vids/ --proba-threshold 0 .5 $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal example_vids/eleph.MP4,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 example_vids/leopard.MP4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0 example_vids/blank.MP4,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 example_vids/chimp.MP4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 Python predict_config = PredictConfig ( data_directory = \"example_vids/\" , proba_threshold = 0.5 ) predictions = pd . read_csv ( \"zamba_predictions.csv\" ) predictions filepath aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal example_vids/blank.MP4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 example_vids/chimp.MP4 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 example_vids/eleph.MP4 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 example_vids/leopard.MP4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 4. Specify any additional parameters \u00b6 And there's so much more! You can also do things like specify your region for faster model download ( --weight-download-region ), use a saved model checkpoint ( --checkpoint ), or specify a different path where your predictions should be saved ( --save ). To read about a few common considerations, see the Guide to Common Optional Parameters page. 5. Test your configuration with a dry run \u00b6 Before kicking off a full run of inference, we recommend testing your code with a \"dry run\". This will run one batch of inference to quickly detect any bugs. See the Debugging page for details.","title":"Classifying Unlabeled Videos"},{"location":"predict-tutorial/#user-tutorial-classifying-unlabeled-videos","text":"This section walks through how to classify videos using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have unlabeled videos that you want to generate labels for The possible class species labels for your videos are included in the list of possible zamba labels . If your species are not included in this list, you can retrain a model using your own labeled data and then run inference.","title":"User Tutorial: Classifying Unlabeled Videos"},{"location":"predict-tutorial/#basic-usage-command-line-interface","text":"Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction in the command line: $ zamba predict --data-dir example_vids/","title":"Basic usage: command line interface"},{"location":"predict-tutorial/#required-arguments","text":"To run zamba predict in the command line, you must specify either --data-dir or --filepaths . --data-dir PATH : Path to the folder containing your videos. --filepaths PATH : Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for filepath . All other flags are optional. To choose a model, either --model or --checkpoint must be specified. Use --model to specify one of the three pretrained models that ship with zamba . Use --checkpoint to run inference with a locally saved model. --model defaults to time_distributed .","title":"Required arguments"},{"location":"predict-tutorial/#basic-usage-python-package","text":"Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction using the Python package: from zamba.models.model_manager import predict_model from zamba.models.config import PredictConfig from zamba.data.video import VideoLoaderConfig predict_config = PredictConfig ( data_directory = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , total_frames = 16 ) predict_model ( predict_config = predict_config , video_loader_config = video_loader_config ) To specify various parameters when running predict_model , the first step is to instantiate PredictConfig and VideoLoaderConfig with any specifications for prediction and video loading respectively. The only two arguments that can be specified in predict_model are predict_config and video_loader_config .","title":"Basic usage: Python package"},{"location":"predict-tutorial/#required-arguments_1","text":"To run predict_model in Python, you must specify either data_directory or filepaths when PredictConfig is instantiated. data_directory (DirectoryPath) : Path to the folder containing your videos. filepaths (FilePath) : Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for filepath . In the command line, video loading configurations are loaded by default based on the model being used. This is not the case in Python. There are additional requirements for VideoLoaderConfig based on the model you are using. video_height (int) , video_width (int) : Dimensions for resizing videos as they are loaded. time_distributed or european : The suggested dimensions are 224x224, but any integers are acceptable slowfast : Both must be greater than or equal to 200 total_frames (int) : The number of frames to select from each video and use during inference. time_distributed or european : Must be 16 slowfast : Must be 32 The full recommended VideoLoaderConfig for the time_distributed or european model is: from zamba.data.video import VideoLoaderConfig video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 16 , }, total_frames = 16 , ) The full recommended VideoLoaderConfig for the slowfast model is: video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 32 , }, total_frames = 32 , ) You can see the full default configuration for each model in models/config <!-- TODO: add link to source and update if needed><!-->. For detailed explanations of all possible configuration arguments, see All Optional Arguments .","title":"Required arguments"},{"location":"predict-tutorial/#default-behavior","text":"In each case, zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) can be interpreted as the probability that animal j is present in video i. By default, predictions will be saved to zamba_predictions.csv . You can save predictions to a custom path using the --save-path argument. $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal example_vids/eleph.MP4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 example_vids/leopard.MP4,0.0,0.0,0.0,0.0,2e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 example_vids/blank.MP4,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 example_vids/chimp.MP4,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,4e-05,0.00162,0.0,0.0,0.0,0.0,0.0,2e-05,2e-05,0.0,1e-05,0.0,0.0038,4e-05,0.0","title":"Default behavior"},{"location":"predict-tutorial/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"predict-tutorial/#1-specify-the-path-to-your-videos","text":"Save all of your videos in one folder. Your videos should all be saved in formats that are suppored by FFmpeg, which are listed here . Your video folder must contain only valid video files, since zamba will try to load all of the files in the directory. Your videos must all be in the top level of the video folder - zamba does not extract videos from nested directories. Add the path to your video folder with --data-dir . For example, if your videos are in a folder called example_vids : CLI $ zamba predict --data-dir example_vids/ Python predict_config = PredictConfig ( data_directory = 'example_vids/' )","title":"1. Specify the path to your videos"},{"location":"predict-tutorial/#2-choose-a-model-for-prediction","text":"If your camera videos contain species common to central or west Africa: Use the time_distributed model if your priority is species classification Use the slowfast model if your priority is blank vs. non-blank video detection If your videos contain species common to Europe, use the european model . Add the model name to your command with --model . The time_distributed model will be used if no model is specified. For example, if you want to use the slowfast model to classify the videos in example_vids : CLI $ zamba predict --data-dir example_vids/ --model slowfast Python predict_config = PredictConfig ( data_directory = 'example_vids/' , model_name = 'slowfast' )","title":"2. Choose a model for prediction"},{"location":"predict-tutorial/#3-choose-the-output-format","text":"There are three options for how to format predictions, listed from most information to least: Store all probabilities (default): Return predictions with a row for each filename and a column for each class label, with probabilities between 0 and 1. Cell (i,j) is the probability that animal j is present in video i. Presence/absence: Return predictions with a row for each filename and a column for each class label, with cells indicating either presence or absense based on a user-specified probability threshold. Cell (i, j) indicates whether animal j is present ( 1 ) or not present ( 0 ) in video i. The probability threshold cutoff is specified with --proba-threshold in the CLI. Most likely class: Return predictions with a row for each filename and one column for the most likely class in each video. The most likely class can also be blank. To get the most likely class, add --output-class-names to your command. In Python, it can be specified by adding output_class_names=True when PredictConfig is instantiated. This is not recommended if you'd like to detect more than one species in each video. Say we want to generate predictions for the videos in example_vids indicating which animals are present in each video based on a probability threshold of 50%: CLI $ zamba predict --data-dir example_vids/ --proba-threshold 0 .5 $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal example_vids/eleph.MP4,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 example_vids/leopard.MP4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0 example_vids/blank.MP4,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 example_vids/chimp.MP4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 Python predict_config = PredictConfig ( data_directory = \"example_vids/\" , proba_threshold = 0.5 ) predictions = pd . read_csv ( \"zamba_predictions.csv\" ) predictions filepath aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal example_vids/blank.MP4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 example_vids/chimp.MP4 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 example_vids/eleph.MP4 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 example_vids/leopard.MP4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0","title":"3. Choose the output format"},{"location":"predict-tutorial/#4-specify-any-additional-parameters","text":"And there's so much more! You can also do things like specify your region for faster model download ( --weight-download-region ), use a saved model checkpoint ( --checkpoint ), or specify a different path where your predictions should be saved ( --save ). To read about a few common considerations, see the Guide to Common Optional Parameters page.","title":"4. Specify any additional parameters"},{"location":"predict-tutorial/#5-test-your-configuration-with-a-dry-run","text":"Before kicking off a full run of inference, we recommend testing your code with a \"dry run\". This will run one batch of inference to quickly detect any bugs. See the Debugging page for details.","title":"5. Test your configuration with a dry run"},{"location":"quickstart/","text":"Quickstart \u00b6 This section assumes you have successfully installed zamba and are ready to train a model or identify species in your videos! zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. To perform inference, you simply need to run zamba predict followed by a set of arguments that let zamba know where your videos are located, which model you want to use, and where to save your output. To train a model, you can similarly run zamba train and specify your labels. The following sections provide details about these separate modules. There are two ways to interact with the zamba package: Use zamba as a command line interface tool. This page provides an overview of how to use the CLI. Import zamba in Python and use it as a Python package. For instructions on using the Python package, see the user tutorial that corresponds to your use case. Installation is the same for both the command line interface tool and the Python package. All of the commands on this page should be run at the command line. On macOS, this can be done in the terminal (\u2318+space, \"Terminal\"). On Windows, this can be done in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt). How do I organize my videos for zamba ? \u00b6 You can input the path to a directory of videos to classify. The folder must contain only valid video files , since zamba will try to load all of the files in the directory. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. zamba will only generate predictions for the videos in the top level of a directory ( zamba does not currently extract videos from nested directories). For example, say we have a directory of videos called example_vids that we want to generate predictions for using zamba . Let's list the videos: $ ls example_vids/ blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 Here are some screenshots from those videos: blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 In this example, the videos have meaningful names so that we can easily compare the predictions made by zamba . In practice, your videos will probably be named something much less useful! Generating predictions \u00b6 To generate and save predictions for your videos using the default settings, run: $ zamba predict --data-dir example_vids/ zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) is the probability that animal j is present in video i. Comperehensive predictions are helpful when a single video contains multiple species. Predictions will be saved to zamba_predictions.csv in the current working directory by default. You can save out predictions under a different name or in a different folder using the --save-path argument. Adding the argument --output-class-names will simplify the predictions to return only the most likely animal in each video: $ zamba predict --data-dir example_vids/ --output-class-names $ cat zamba_predictions.csv vids/blank.mp4,blank vids/chimp.mp4,chimpanzee_bonobo vids/eleph.mp4,elephant vids/leopard.mp4,leopard There are three pretrained models that ship with zamba : time_distributed , slowfast , and european . Which model you should use depends on your priorities and geography (see the Available Models page for more details). By default zamba will use the time_distributed model. Add the --model argument to specify one of other options: $ zamba predict --data-dir example_vids/ --model slowfast Training a model \u00b6 You can continue training one of the models that ships with zamba by either: Fine-tuning with additional labeled videos where the species are included in the list of zamba class labels Fine-tuning with labeled videos that include new species In either case, the commands for training are the same. Say that we have labels for the videos in the example_vids folder saved in example_labels.csv . To train a model, run: $ zamba train --data-dir example_vids/ --labels example_labels.csv The labels file must have columns for both filepath and label. Optionally, there can also be columns for split ( train , val , or holdout ) and site . Let's print the example labels: $ cat example_labels.csv filepath,label example_vids/eleph.MP4,elephant example_vids/leopard.MP4,leopard example_vids/blank.MP4,blank example_vids/chimp.MP4,chimpanzee_bonobo By default, the trained model and additional training output will be saved to a folder in the current working directory called zamba_{model_name} . For example, a model finetuned from the provided time_distributed model will be saved in zamba_time_distributed . $ zamba train --data-dir example_vids/ --labels example_labels.csv $ ls zamba_time_distributed time_distributed.ckpt configuration.yaml events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 hparams.yaml Downloading model weights \u00b6 zamba needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Once a model's weights are downloaded, the tool will use the local version and will not need to perform this download again. If you are not in the US, we recommend running the above command with the additional flag either --weight_download_region eu or --weight_download_region asia depending on your location. The closer you are to the server the faster the downloads will be. Getting help \u00b6 Once zamba is installed, you can see more details of each function with --help . To get help with zamba predict : $ zamba predict --help Usage: zamba predict [OPTIONS] Identify species in a video. This is a command line interface for prediction on camera trap footage. Given a path to camera trap footage, the predict function use a deep learning model to predict the presence or absense of a variety of species of common interest to wildlife researchers working with camera trap data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --filepaths PATH Path to csv containing `filepath` column with videos. --model [time_distributed|slowfast|european] Model to use for inference. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for inference. If provided, model is not required. --gpus INTEGER Number of GPUs to use for inference. If not specifiied, will use all GPUs found on machine. --batch-size INTEGER Batch size to use for training. --save / --no-save Whether to save out predictions to a csv file. If you want to specify the location of the csv, use save_path instead. --save-path PATH Full path for prediction CSV file. Any needed parent directories will be created. --dry-run / --no-dry-run Runs one batch of inference to check for bugs. --config PATH Specify options using yaml configuration file instead of through command line options. --proba-threshold FLOAT Probability threshold for classification between 0 and 1. If specified binary predictions are returned with 1 being greater than the threshold, 0 being less than or equal to. If not specified, probabilities between 0 and 1 are returned. --output-class-names / --no-output-class-names If True, we just return a video and the name of the most likely class. If False, we return a probability or indicator (depending on --proba_threshold) for every possible class. --weight-download-region [us|eu|asia] Server region for downloading weights. --cache-dir PATH Path to directory for model weights. Alternatively, specify with environment variable `ZAMBA_CACHE_DIR`. If not specified, user's cache directory is used. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to inference. Only use if you're very confident all your videos can be loaded. -y, --yes Skip confirmation of configuration and proceed right to prediction. [default: False] --help Show this message and exit. To get help with zamba train : $ zamba train --help Usage: zamba train [OPTIONS] Train a model on your labeled data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --labels PATH Path to csv containing video labels. --model [time_distributed|slowfast|european] Model to train. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for training. If provided, model is not required. --config PATH Specify options using yaml configuration file instead of through command line options. --batch-size INTEGER Batch size to use for training. --gpus INTEGER Number of GPUs to use for training. If not specifiied, will use all GPUs found on machine. --dry-run / --no-dry-run Runs one batch of train and validation to check for bugs. --save-dir PATH Directory in which to save model checkpoint and configuration file. If not specified, will save to a folder called 'zamba_{model_name}' in your working directory. --weight-download-region [us|eu|asia] Server region for downloading weights. --cache-dir PATH Path to directory for model weights. Alternatively, specify with environment variable `ZAMBA_CACHE_DIR`. If not specified, user's cache directory is used. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to training. Only use if you're very confident all your videos can be loaded. -y, --yes Skip confirmation of configuration and proceed right to training. [default: False] --help Show this message and exit.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This section assumes you have successfully installed zamba and are ready to train a model or identify species in your videos! zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. To perform inference, you simply need to run zamba predict followed by a set of arguments that let zamba know where your videos are located, which model you want to use, and where to save your output. To train a model, you can similarly run zamba train and specify your labels. The following sections provide details about these separate modules. There are two ways to interact with the zamba package: Use zamba as a command line interface tool. This page provides an overview of how to use the CLI. Import zamba in Python and use it as a Python package. For instructions on using the Python package, see the user tutorial that corresponds to your use case. Installation is the same for both the command line interface tool and the Python package. All of the commands on this page should be run at the command line. On macOS, this can be done in the terminal (\u2318+space, \"Terminal\"). On Windows, this can be done in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt).","title":"Quickstart"},{"location":"quickstart/#how-do-i-organize-my-videos-for-zamba","text":"You can input the path to a directory of videos to classify. The folder must contain only valid video files , since zamba will try to load all of the files in the directory. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. zamba will only generate predictions for the videos in the top level of a directory ( zamba does not currently extract videos from nested directories). For example, say we have a directory of videos called example_vids that we want to generate predictions for using zamba . Let's list the videos: $ ls example_vids/ blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 Here are some screenshots from those videos: blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 In this example, the videos have meaningful names so that we can easily compare the predictions made by zamba . In practice, your videos will probably be named something much less useful!","title":"How do I organize my videos for zamba?"},{"location":"quickstart/#generating-predictions","text":"To generate and save predictions for your videos using the default settings, run: $ zamba predict --data-dir example_vids/ zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) is the probability that animal j is present in video i. Comperehensive predictions are helpful when a single video contains multiple species. Predictions will be saved to zamba_predictions.csv in the current working directory by default. You can save out predictions under a different name or in a different folder using the --save-path argument. Adding the argument --output-class-names will simplify the predictions to return only the most likely animal in each video: $ zamba predict --data-dir example_vids/ --output-class-names $ cat zamba_predictions.csv vids/blank.mp4,blank vids/chimp.mp4,chimpanzee_bonobo vids/eleph.mp4,elephant vids/leopard.mp4,leopard There are three pretrained models that ship with zamba : time_distributed , slowfast , and european . Which model you should use depends on your priorities and geography (see the Available Models page for more details). By default zamba will use the time_distributed model. Add the --model argument to specify one of other options: $ zamba predict --data-dir example_vids/ --model slowfast","title":"Generating predictions"},{"location":"quickstart/#training-a-model","text":"You can continue training one of the models that ships with zamba by either: Fine-tuning with additional labeled videos where the species are included in the list of zamba class labels Fine-tuning with labeled videos that include new species In either case, the commands for training are the same. Say that we have labels for the videos in the example_vids folder saved in example_labels.csv . To train a model, run: $ zamba train --data-dir example_vids/ --labels example_labels.csv The labels file must have columns for both filepath and label. Optionally, there can also be columns for split ( train , val , or holdout ) and site . Let's print the example labels: $ cat example_labels.csv filepath,label example_vids/eleph.MP4,elephant example_vids/leopard.MP4,leopard example_vids/blank.MP4,blank example_vids/chimp.MP4,chimpanzee_bonobo By default, the trained model and additional training output will be saved to a folder in the current working directory called zamba_{model_name} . For example, a model finetuned from the provided time_distributed model will be saved in zamba_time_distributed . $ zamba train --data-dir example_vids/ --labels example_labels.csv $ ls zamba_time_distributed time_distributed.ckpt configuration.yaml events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 hparams.yaml","title":"Training a model"},{"location":"quickstart/#downloading-model-weights","text":"zamba needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Once a model's weights are downloaded, the tool will use the local version and will not need to perform this download again. If you are not in the US, we recommend running the above command with the additional flag either --weight_download_region eu or --weight_download_region asia depending on your location. The closer you are to the server the faster the downloads will be.","title":"Downloading model weights"},{"location":"quickstart/#getting-help","text":"Once zamba is installed, you can see more details of each function with --help . To get help with zamba predict : $ zamba predict --help Usage: zamba predict [OPTIONS] Identify species in a video. This is a command line interface for prediction on camera trap footage. Given a path to camera trap footage, the predict function use a deep learning model to predict the presence or absense of a variety of species of common interest to wildlife researchers working with camera trap data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --filepaths PATH Path to csv containing `filepath` column with videos. --model [time_distributed|slowfast|european] Model to use for inference. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for inference. If provided, model is not required. --gpus INTEGER Number of GPUs to use for inference. If not specifiied, will use all GPUs found on machine. --batch-size INTEGER Batch size to use for training. --save / --no-save Whether to save out predictions to a csv file. If you want to specify the location of the csv, use save_path instead. --save-path PATH Full path for prediction CSV file. Any needed parent directories will be created. --dry-run / --no-dry-run Runs one batch of inference to check for bugs. --config PATH Specify options using yaml configuration file instead of through command line options. --proba-threshold FLOAT Probability threshold for classification between 0 and 1. If specified binary predictions are returned with 1 being greater than the threshold, 0 being less than or equal to. If not specified, probabilities between 0 and 1 are returned. --output-class-names / --no-output-class-names If True, we just return a video and the name of the most likely class. If False, we return a probability or indicator (depending on --proba_threshold) for every possible class. --weight-download-region [us|eu|asia] Server region for downloading weights. --cache-dir PATH Path to directory for model weights. Alternatively, specify with environment variable `ZAMBA_CACHE_DIR`. If not specified, user's cache directory is used. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to inference. Only use if you're very confident all your videos can be loaded. -y, --yes Skip confirmation of configuration and proceed right to prediction. [default: False] --help Show this message and exit. To get help with zamba train : $ zamba train --help Usage: zamba train [OPTIONS] Train a model on your labeled data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --labels PATH Path to csv containing video labels. --model [time_distributed|slowfast|european] Model to train. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for training. If provided, model is not required. --config PATH Specify options using yaml configuration file instead of through command line options. --batch-size INTEGER Batch size to use for training. --gpus INTEGER Number of GPUs to use for training. If not specifiied, will use all GPUs found on machine. --dry-run / --no-dry-run Runs one batch of train and validation to check for bugs. --save-dir PATH Directory in which to save model checkpoint and configuration file. If not specified, will save to a folder called 'zamba_{model_name}' in your working directory. --weight-download-region [us|eu|asia] Server region for downloading weights. --cache-dir PATH Path to directory for model weights. Alternatively, specify with environment variable `ZAMBA_CACHE_DIR`. If not specified, user's cache directory is used. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to training. Only use if you're very confident all your videos can be loaded. -y, --yes Skip confirmation of configuration and proceed right to training. [default: False] --help Show this message and exit.","title":"Getting help"},{"location":"train-tutorial/","text":"User Tutorial: Training a Model on Labaled Videos \u00b6 This section walks through how to train a model using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have labeled videos that you want to use to train or finetune a model zamba can run two types of model training: Fine-tuning a model with labels that are a subset of the possible zamba labels Fine-tuning a model to predict an entirely new set of labels The process is the same for both cases. Basic usage: command line interface \u00b6 Say that we want to finetune the time_distributed model based on the videos in example_vids and the labels in example_labels.csv . Minimum example for training in the command line: $ zamba train --data-dir example_vids/ --labels example_labels.csv Required arguments \u00b6 To run zamba train in the command line, you must specify both --data-directory and --labels . --data-dir PATH : Path to the folder containing your labeled videos. --labels PATH : Path to a CSV containing the video labels to use as ground truth during training. There must be columns for both filepath and label. Optionally, there can also be columns for split ( train , val , or holdout ) and site . If your labels file does not have a column for split , you can alternately use the split_proportions argument. Basic usage: Python package \u00b6 Say that we want to finetune the time_distributed model based on the videos in example_vids and the labels in example_labels.csv . Minimum example for training using the Python package: from zamba.models.model_manager import train_model from zamba.models.config import TrainConfig from zamba.data.video import VideoLoaderConfig train_config = TrainConfig ( data_directory = \"example_vids/\" , labels = \"example_labels.csv\" ) video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , total_frames = 16 ) train_model ( train_config = train_config , video_loader_config = video_loader_config ) To specify various parameters when running train_model , the first step is to instantiate TrainConfig and VideoLoaderConfig with any specifications for model training and video loading respectively. The only two arguments that can be specified in train_model are train_config and video_loader_config . Required arguments \u00b6 To run train_model in Python, you must specify both data_directory and labels when TrainConfig is instantiated. data_directory (DirectoryPath) : Path to the folder containing your videos. labels (FilePath or pd.DataFrame) : Either the path to a CSV file with labels for training, or a dataframe of the training labels. There must be columns for filename and label . In the command line, video loading configurations are loaded by default based on the model being used. This is not the case in Python. There are additional requirements for VideoLoaderConfig based on the model you are using. video_height (int) , video_width (int) : Dimensions for resizing videos as they are loaded. time_distributed or european : The suggested dimensions are 224x224, but any integers are acceptable slowfast : Both must be greater than or equal to 200 total_frames (int) : The number of frames to select from each video and use during training. time_distributed or european : Must be 16 slowfast : Must be 32 The full recommended VideoLoaderConfig for the time_distributed or european model is: from zamba.data.video import VideoLoaderConfig from zamba.models.megadetector_lite_yolox import MegadetectorLiteYoloXConfig megadetector_config = MegadetectorLiteYoloXConfig ( confidence = 0.25 , fill_mode = \"score_sorted\" , n_frames = 16 ) video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_list_config = megadetector_config , total_frames = 16 , ) You can see the full default configuration for each model in models/config <!-- TODO: add link to source and update if needed><!-->. For detailed explanations of all possible configuration arguments, see All Optional Arguments . Default behavior \u00b6 By default, the model will be saved to a folder in the current working directory called zamba_{model_name} . For example, a model finetuned from the provided time_distributed model (the default) will be saved in zamba_time_distributed . $ zamba train --data-dir example_vids/ --labels example_labels.csv $ ls zamba_time_distributed configuration.yaml hparams.yaml time_distributed.ckpt events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 test_metrics.json val_metrics.json zamba_time_distributed contains three files: configuration.yaml : The full model configuration used to generate the given model, including video_loader_config and train_config . To continue training using the same configuration, or to train another model using the same configuration, you can pass in configurations.yaml (see Specifying Model Configurations with a YAML File ). hparams.yaml : Model hyperparameters. For example, the YAML file below tells us that the model was trained with a learning rate ( lr ) of 0.001: $ cat zamba_time_distributed/hparams.yaml lr : 0.001 model_class : TimeDistributedEfficientNetMultiLayerHead num_frames : 16 scheduler : MultiStepLR scheduler_params : gamma : 0.5 milestones : - 3 verbose : true species : - species_blank - species_chimpanzee_bonobo - species_elephant - species_leopard time_distributed.ckpt : Model checkpoint. The model checkpoint also includes both the model configuration in configuration.yaml and the model hyperparameters in hparams.yaml . You can continue training from this checkpoint by passing it to zamba train with the --checkpoint flag: $ zamba train --checkpoint time_distributed.ckpt --data-dir example_vids/ --labels example_labels.csv events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 : TensorBoard logs test_metrics.json : The model's performance on the test subset val_metrics.json : The model's performance on the validation subset Step-by-step tutorial \u00b6 1. Specify the path to your videos \u00b6 Save all of your videos in one folder. Your videos should all be saved in formats that are suppored by FFmpeg, which are listed here . Your video folder must contain only valid video files, since zamba will try to load all of the files in the directory. Your videos must all be in the top level of the video folder - zamba does not extract videos from nested directories. Add the path to your video folder with --data-dir . For example, if your videos are in a folder called example_vids , add --data-dir example_vids/ to your command. CLI $ zamba train --data-dir example_vids Python from zamba.models.config import TrainConfig train_config = TrainConfig ( data_directory = 'example_vids/' ) Note that the above will not run yet because labels are not specified. 2. Specify your labels \u00b6 Your labels should be saved in a .csv file with columns for filepath and label. For example: $ cat example_labels.csv filepath,label example_vids/eleph.MP4,elephant example_vids/leopard.MP4,leopard example_vids/blank.MP4,blank example_vids/chimp.MP4,chimpanzee_bonobo Add the path to your labels with --labels . For example, if your videos are in a folder called example_vids and your labels are saved in example_labels.csv : CLI $ zamba train --data-dir example_vids/ --labels example_labels.csv Python In Python, the labels are passed in when TrainConfig is instantiated. The Python package allows you to pass in labels as either a file path or a pandas dataframe: labels_dataframe = pd . read_csv ( 'example_labels.csv' , index_col = 'filepath' ) train_config = TrainConfig ( data_directory = 'example_vids/' , labels = labels_dataframe ) Labels zamba has seen before \u00b6 Your labels may be included in the list of zamba class labels that the provided models are trained to predict. If so, the relevant model that ships with zamba will essentially be used as a checkpoint, and model training will resume from that checkpoint. Completely new labels \u00b6 You can also train a model to predict completely new labels - the world is your oyster! (We'd love to see a model trained to predict oysters.) If this is the case, the model architecture will replace the final neural network layer with a new head that predicts your labels instead of those that ship with zamba . Backpropogation will continue from that point with the new head. This process is called transfer learning . 3. Choose a model for training \u00b6 If your videos contain species common to central or west Africa, use the time_distributed model . If they contain species common to western Europe, use the european model . We do not recommend using the slowfast model for training because it is much more computationally intensive and slower to run. Add the model name to your command with --model . The time_distributed model will be used if no model is specified. For example, if you want to continue training the european model based on the videos in example_euro_vids and the labels in example_euro_labels.csv : CLI $ zamba train --data-dir example_euro_vids/ --labels example_euro_labels.csv --model european Python train_config = TrainConfig ( data_directory = \"example_euro_vids/\" , labels = \"example_euro_labels.csv\" , model_name = \"european\" , ) 4. Specify any additional parameters \u00b6 And there's so much more! You can also do things like specify your region for faster model download ( --weight-download-region ), start training from a saved model checkpoint ( --checkpoint ), or specify a different path where your model should be saved ( --save-directory ). To read about a few common considerations, see the Guide to Common Optional Parameters page. 5. Test your configuration with a dry run \u00b6 Before kicking off the full model training, we recommend testing your code with a \"dry run\". This will run one training and validation batch for one epoch to quickly detect any bugs. See the Debugging page for details.","title":"Training a Model on Labeled Videos"},{"location":"train-tutorial/#user-tutorial-training-a-model-on-labaled-videos","text":"This section walks through how to train a model using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have labeled videos that you want to use to train or finetune a model zamba can run two types of model training: Fine-tuning a model with labels that are a subset of the possible zamba labels Fine-tuning a model to predict an entirely new set of labels The process is the same for both cases.","title":"User Tutorial: Training a Model on Labaled Videos"},{"location":"train-tutorial/#basic-usage-command-line-interface","text":"Say that we want to finetune the time_distributed model based on the videos in example_vids and the labels in example_labels.csv . Minimum example for training in the command line: $ zamba train --data-dir example_vids/ --labels example_labels.csv","title":"Basic usage: command line interface"},{"location":"train-tutorial/#required-arguments","text":"To run zamba train in the command line, you must specify both --data-directory and --labels . --data-dir PATH : Path to the folder containing your labeled videos. --labels PATH : Path to a CSV containing the video labels to use as ground truth during training. There must be columns for both filepath and label. Optionally, there can also be columns for split ( train , val , or holdout ) and site . If your labels file does not have a column for split , you can alternately use the split_proportions argument.","title":"Required arguments"},{"location":"train-tutorial/#basic-usage-python-package","text":"Say that we want to finetune the time_distributed model based on the videos in example_vids and the labels in example_labels.csv . Minimum example for training using the Python package: from zamba.models.model_manager import train_model from zamba.models.config import TrainConfig from zamba.data.video import VideoLoaderConfig train_config = TrainConfig ( data_directory = \"example_vids/\" , labels = \"example_labels.csv\" ) video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , total_frames = 16 ) train_model ( train_config = train_config , video_loader_config = video_loader_config ) To specify various parameters when running train_model , the first step is to instantiate TrainConfig and VideoLoaderConfig with any specifications for model training and video loading respectively. The only two arguments that can be specified in train_model are train_config and video_loader_config .","title":"Basic usage: Python package"},{"location":"train-tutorial/#required-arguments_1","text":"To run train_model in Python, you must specify both data_directory and labels when TrainConfig is instantiated. data_directory (DirectoryPath) : Path to the folder containing your videos. labels (FilePath or pd.DataFrame) : Either the path to a CSV file with labels for training, or a dataframe of the training labels. There must be columns for filename and label . In the command line, video loading configurations are loaded by default based on the model being used. This is not the case in Python. There are additional requirements for VideoLoaderConfig based on the model you are using. video_height (int) , video_width (int) : Dimensions for resizing videos as they are loaded. time_distributed or european : The suggested dimensions are 224x224, but any integers are acceptable slowfast : Both must be greater than or equal to 200 total_frames (int) : The number of frames to select from each video and use during training. time_distributed or european : Must be 16 slowfast : Must be 32 The full recommended VideoLoaderConfig for the time_distributed or european model is: from zamba.data.video import VideoLoaderConfig from zamba.models.megadetector_lite_yolox import MegadetectorLiteYoloXConfig megadetector_config = MegadetectorLiteYoloXConfig ( confidence = 0.25 , fill_mode = \"score_sorted\" , n_frames = 16 ) video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_list_config = megadetector_config , total_frames = 16 , ) You can see the full default configuration for each model in models/config <!-- TODO: add link to source and update if needed><!-->. For detailed explanations of all possible configuration arguments, see All Optional Arguments .","title":"Required arguments"},{"location":"train-tutorial/#default-behavior","text":"By default, the model will be saved to a folder in the current working directory called zamba_{model_name} . For example, a model finetuned from the provided time_distributed model (the default) will be saved in zamba_time_distributed . $ zamba train --data-dir example_vids/ --labels example_labels.csv $ ls zamba_time_distributed configuration.yaml hparams.yaml time_distributed.ckpt events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 test_metrics.json val_metrics.json zamba_time_distributed contains three files: configuration.yaml : The full model configuration used to generate the given model, including video_loader_config and train_config . To continue training using the same configuration, or to train another model using the same configuration, you can pass in configurations.yaml (see Specifying Model Configurations with a YAML File ). hparams.yaml : Model hyperparameters. For example, the YAML file below tells us that the model was trained with a learning rate ( lr ) of 0.001: $ cat zamba_time_distributed/hparams.yaml lr : 0.001 model_class : TimeDistributedEfficientNetMultiLayerHead num_frames : 16 scheduler : MultiStepLR scheduler_params : gamma : 0.5 milestones : - 3 verbose : true species : - species_blank - species_chimpanzee_bonobo - species_elephant - species_leopard time_distributed.ckpt : Model checkpoint. The model checkpoint also includes both the model configuration in configuration.yaml and the model hyperparameters in hparams.yaml . You can continue training from this checkpoint by passing it to zamba train with the --checkpoint flag: $ zamba train --checkpoint time_distributed.ckpt --data-dir example_vids/ --labels example_labels.csv events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 : TensorBoard logs test_metrics.json : The model's performance on the test subset val_metrics.json : The model's performance on the validation subset","title":"Default behavior"},{"location":"train-tutorial/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"train-tutorial/#1-specify-the-path-to-your-videos","text":"Save all of your videos in one folder. Your videos should all be saved in formats that are suppored by FFmpeg, which are listed here . Your video folder must contain only valid video files, since zamba will try to load all of the files in the directory. Your videos must all be in the top level of the video folder - zamba does not extract videos from nested directories. Add the path to your video folder with --data-dir . For example, if your videos are in a folder called example_vids , add --data-dir example_vids/ to your command. CLI $ zamba train --data-dir example_vids Python from zamba.models.config import TrainConfig train_config = TrainConfig ( data_directory = 'example_vids/' ) Note that the above will not run yet because labels are not specified.","title":"1. Specify the path to your videos"},{"location":"train-tutorial/#2-specify-your-labels","text":"Your labels should be saved in a .csv file with columns for filepath and label. For example: $ cat example_labels.csv filepath,label example_vids/eleph.MP4,elephant example_vids/leopard.MP4,leopard example_vids/blank.MP4,blank example_vids/chimp.MP4,chimpanzee_bonobo Add the path to your labels with --labels . For example, if your videos are in a folder called example_vids and your labels are saved in example_labels.csv : CLI $ zamba train --data-dir example_vids/ --labels example_labels.csv Python In Python, the labels are passed in when TrainConfig is instantiated. The Python package allows you to pass in labels as either a file path or a pandas dataframe: labels_dataframe = pd . read_csv ( 'example_labels.csv' , index_col = 'filepath' ) train_config = TrainConfig ( data_directory = 'example_vids/' , labels = labels_dataframe )","title":"2. Specify your labels"},{"location":"train-tutorial/#3-choose-a-model-for-training","text":"If your videos contain species common to central or west Africa, use the time_distributed model . If they contain species common to western Europe, use the european model . We do not recommend using the slowfast model for training because it is much more computationally intensive and slower to run. Add the model name to your command with --model . The time_distributed model will be used if no model is specified. For example, if you want to continue training the european model based on the videos in example_euro_vids and the labels in example_euro_labels.csv : CLI $ zamba train --data-dir example_euro_vids/ --labels example_euro_labels.csv --model european Python train_config = TrainConfig ( data_directory = \"example_euro_vids/\" , labels = \"example_euro_labels.csv\" , model_name = \"european\" , )","title":"3. Choose a model for training"},{"location":"train-tutorial/#4-specify-any-additional-parameters","text":"And there's so much more! You can also do things like specify your region for faster model download ( --weight-download-region ), start training from a saved model checkpoint ( --checkpoint ), or specify a different path where your model should be saved ( --save-directory ). To read about a few common considerations, see the Guide to Common Optional Parameters page.","title":"4. Specify any additional parameters"},{"location":"train-tutorial/#5-test-your-configuration-with-a-dry-run","text":"Before kicking off the full model training, we recommend testing your code with a \"dry run\". This will run one training and validation batch for one epoch to quickly detect any bugs. See the Debugging page for details.","title":"5. Test your configuration with a dry run"},{"location":"v2_updates/","text":"Key Version 2 Updates \u00b6 Released: <!-- TODO: add release date><!--> Previous Model - Machine Learning Competition \u00b6 The algorithms used by zamba v1 were based on the winning solution from the Pri-matrix Factorization machine learning competition, hosted by DrivenData . Data for the competition was provided by the Chimp&See project and manually labeled by volunteers. The competition had over 300 participants and over 450 submissions throughout the three month challenge. The v1 algorithm was adapted from the winning competition submission, with some aspects changed during development to improve performance. The core algorithm in zamba v1 was a stacked ensemble which consisted of a first layer of models that were then combined into a final prediction in a second layer. The first level of the stack consisted of 5 keras deep learning models, whose individual predictions were combined in the second level of the stack to form the final prediction. In v2, the stacked ensemble algorithm from v1 is replaced with three more powerful single-model options: time_distributed , slowfast , and european . The new models utilize state-of-the-art image and video classification architectures, and are able to outperform the much more computationally intensive stacked ensemble model. New geographies and species \u00b6 zamba v2 incorporates data from western Europe (Germany) in additional to locations in central and west Africa. The new data is packaged in the pretrained european model, which can predict 11 common European species not present in zamba v1. zamba v2 also incorporates new training data for central and west Africa. zamba v1 was primarily focused on species commonly found on savannas. v2 incorporates data from camera traps in jungle ecosystems, adding 13 additional species to the pretrained models for central and west Africa. Retraining flexibility \u00b6 Model training is easier to reproduce in zamba v2, so users can finetune a pretrained model using their own data. zamba v2 also allows users to retrain a model on completely new labels.","title":"Version 2"},{"location":"v2_updates/#key-version-2-updates","text":"Released: <!-- TODO: add release date><!-->","title":"Key Version 2 Updates"},{"location":"v2_updates/#previous-model-machine-learning-competition","text":"The algorithms used by zamba v1 were based on the winning solution from the Pri-matrix Factorization machine learning competition, hosted by DrivenData . Data for the competition was provided by the Chimp&See project and manually labeled by volunteers. The competition had over 300 participants and over 450 submissions throughout the three month challenge. The v1 algorithm was adapted from the winning competition submission, with some aspects changed during development to improve performance. The core algorithm in zamba v1 was a stacked ensemble which consisted of a first layer of models that were then combined into a final prediction in a second layer. The first level of the stack consisted of 5 keras deep learning models, whose individual predictions were combined in the second level of the stack to form the final prediction. In v2, the stacked ensemble algorithm from v1 is replaced with three more powerful single-model options: time_distributed , slowfast , and european . The new models utilize state-of-the-art image and video classification architectures, and are able to outperform the much more computationally intensive stacked ensemble model.","title":"Previous Model - Machine Learning Competition"},{"location":"v2_updates/#new-geographies-and-species","text":"zamba v2 incorporates data from western Europe (Germany) in additional to locations in central and west Africa. The new data is packaged in the pretrained european model, which can predict 11 common European species not present in zamba v1. zamba v2 also incorporates new training data for central and west Africa. zamba v1 was primarily focused on species commonly found on savannas. v2 incorporates data from camera traps in jungle ecosystems, adding 13 additional species to the pretrained models for central and west Africa.","title":"New geographies and species"},{"location":"v2_updates/#retraining-flexibility","text":"Model training is easier to reproduce in zamba v2, so users can finetune a pretrained model using their own data. zamba v2 also allows users to retrain a model on completely new labels.","title":"Retraining flexibility"},{"location":"yaml-config/","text":"Using YAML Configuration Files \u00b6 In both the command line and the Python module, options for video loading, training, and prediction can be set by passing a YAML file instead of passing arguments directly. YAML files ( .yml or .yaml ) are commonly used to serialize data in an easily readable way. The basic structure of a YAML model configuration is: $ cat basic_config.yaml video_loader_config : video_height : 224 video_width : 224 total_frames : 16 # other video loading parameters predict_config : model_name : time_distributed data_directoty : example_vids/ # other training parameters, eg. batch_size, video_height, video_width train_config : model_name : time_distributed data_directory : example_vids/ labels : example_labels.csv # other training parameters, eg. batch_size, video_height, video_width For example, the configuration below will predict labels for the videos in example_vids using the time_distributed model. When videos are loaded, each will be resized to 224x224 pixels and 16 frames will be selected: video_loader_config : video_height : 224 video_width : 224 total_frames : 16 predict_config : model_name : time_distributed data_directoty : example_vids/ Required arguments \u00b6 Either predict_config or train_config is required, based on whether you will be running inference or training a model. See All Optional Arguments for a full list of what can be specified under each class. To run inference, either data_directory or filepaths must be specified. To train a model, both data_directory and labels must be specified. In video_loader_config , you must specify at least video_height , video_width , and total_frames . For time_distributed or european , total_frames must be 16 For slowfast , total_frames must be 32 See the Available Models page for more details on each model's requirements. Command line interface \u00b6 A YAML configuration file can be passed to the command line interface with the --config argument. For example, say the example configuration above is saved as example_config.yaml . To run prediction: $ zamba predict --config example_config.yaml Only some of the possible parameters can be passed directly as arguments to the command line. Those not listed in zamba predict --help or zamba train --help must be passed in a YAML file (see the Quickstart guide for details). Python package \u00b6 The main API for zamba is the ModelManager class that can be accessed with: from zamba.models.manager import ModelManager The ModelManager class is used by zamba \u2019s command line interface to handle preprocessing the filenames, loading the videos, serving them to the model, and saving predictions. Therefore any functionality available to the command line interface is accessible via the ModelManager class. To instantiate the ModelManager based on a configuration file saved at test_config.yaml : >>> manager = ModelManager . from_yaml ( 'test_config.yaml' ) >>> manager . config ModelConfig ( video_loader_config = VideoLoaderConfig ( crop_bottom_pixels = None , i_frames = False , scene_threshold = None , megadetector_lite_config = None , video_height = 224 , video_width = 224 , total_frames = 16 , ensure_total_frames = True , fps = None , early_bias = False , frame_indices = None , evenly_sample_total_frames = False , pix_fmt = 'rgb24' ), train_config = None , predict_config = PredictConfig ( data_directory = PosixPath ( 'vids' ), filepaths = filepath 0 / home / ubuntu / vids / eleph . MP4 1 / home / ubuntu / vids / leopard . MP4 2 / home / ubuntu / vids / blank . MP4 3 / home / ubuntu / vids / chimp . MP4 , checkpoint = 'zamba_time_distributed.ckpt' , model_params = ModelParams ( scheduler = None , scheduler_params = None ), model_name = 'time_distributed' , species = None , gpus = 1 , num_workers = 7 , batch_size = 8 , save = True , dry_run = False , proba_threshold = None , output_class_names = False , weight_download_region = 'us' , cache_dir = None , skip_load_validation = False ) ) We can now run inference or model training without specifying any additional parameters, because they are already associated with our instance of the ModelManager class. To run inference or training: manager . predict () # inference manager . train () # training In our user tutorials, we refer to train_model and predict_model functions. The ModelManager class calls these same functions behind the scenes when .predict() or .train() is run. Default configurations \u00b6 In the command line, the default configuration for each model is passed in using a specified YAML file that ships with zamba <!-- TODO: add link to github><!-->. For example, the default configuration for the time_distributed model is: video_loader_config : video_height : 224 video_width : 224 crop_bottom_pixels : 50 ensure_total_frames : True megadetector_lite_config : confidence : 0.25 fill_model : \"score_sorted\" n_frames : 16 total_frames : 16 train_config : # data_directory: YOUR_DATA_DIRECTORY_HERE # labels: YOUR_LABELS_CSV_HERE model_name : time_distributed # or # checkpoint: YOUR_CKPT_HERE batch_size : 8 num_workers : 3 scheduler_config : scheduler : MultiStepLR scheduler_params : milestones : [ 3 ] gamma : 0.5 verbose : True auto_lr_find : True backbone_finetune : True backbone_finetune_params : unfreeze_backbone_at_epoch : 3 verbose : True pre_train_bn : True multiplier : 1 early_stopping : True early_stopping_params : patience : 5 predict_config : # data_directory: YOUR_DATA_DIRECTORY_HERE # or # filepaths: YOUR_FILEPATH_HERE model_name : time_distributed # or # checkpoint: YOUR_CKPT_HERE For reference, the below shows how to specify the same video loading and training parameters using only the Python package: from zamba.data.video import VideoLoaderConfig from zamba.models.config import TrainConfig from zamba.models.model_manager import train_model video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 16 , }, total_frames = 16 , ) train_config = TrainConfig ( # data_directory=YOUR_DATA_DIRECTORY_HERE, # labels=YOUR_LABELS_CSV_HERE, model_name = \"time_distributed\" , # or # checkpoint=YOUR_CKPT_HERE, batch_size = 8 , backbone_finetune = True , backbone_finetune_params = { \"unfreeze_backbone_at_epoch\" : 3 , \"verbose\" : True , \"pre_train_bn\" : True , \"multiplier\" : 1 , }, num_workers = 3 , auto_lr_find = True , early_stopping = True , early_stopping_params = { \"patience\" : 5 ,}, ) train_model ( train_config = train_config , video_loader_config = video_loader_config )","title":"Using YAML configuration files"},{"location":"yaml-config/#using-yaml-configuration-files","text":"In both the command line and the Python module, options for video loading, training, and prediction can be set by passing a YAML file instead of passing arguments directly. YAML files ( .yml or .yaml ) are commonly used to serialize data in an easily readable way. The basic structure of a YAML model configuration is: $ cat basic_config.yaml video_loader_config : video_height : 224 video_width : 224 total_frames : 16 # other video loading parameters predict_config : model_name : time_distributed data_directoty : example_vids/ # other training parameters, eg. batch_size, video_height, video_width train_config : model_name : time_distributed data_directory : example_vids/ labels : example_labels.csv # other training parameters, eg. batch_size, video_height, video_width For example, the configuration below will predict labels for the videos in example_vids using the time_distributed model. When videos are loaded, each will be resized to 224x224 pixels and 16 frames will be selected: video_loader_config : video_height : 224 video_width : 224 total_frames : 16 predict_config : model_name : time_distributed data_directoty : example_vids/","title":"Using YAML Configuration Files"},{"location":"yaml-config/#required-arguments","text":"Either predict_config or train_config is required, based on whether you will be running inference or training a model. See All Optional Arguments for a full list of what can be specified under each class. To run inference, either data_directory or filepaths must be specified. To train a model, both data_directory and labels must be specified. In video_loader_config , you must specify at least video_height , video_width , and total_frames . For time_distributed or european , total_frames must be 16 For slowfast , total_frames must be 32 See the Available Models page for more details on each model's requirements.","title":"Required arguments"},{"location":"yaml-config/#command-line-interface","text":"A YAML configuration file can be passed to the command line interface with the --config argument. For example, say the example configuration above is saved as example_config.yaml . To run prediction: $ zamba predict --config example_config.yaml Only some of the possible parameters can be passed directly as arguments to the command line. Those not listed in zamba predict --help or zamba train --help must be passed in a YAML file (see the Quickstart guide for details).","title":"Command line interface"},{"location":"yaml-config/#python-package","text":"The main API for zamba is the ModelManager class that can be accessed with: from zamba.models.manager import ModelManager The ModelManager class is used by zamba \u2019s command line interface to handle preprocessing the filenames, loading the videos, serving them to the model, and saving predictions. Therefore any functionality available to the command line interface is accessible via the ModelManager class. To instantiate the ModelManager based on a configuration file saved at test_config.yaml : >>> manager = ModelManager . from_yaml ( 'test_config.yaml' ) >>> manager . config ModelConfig ( video_loader_config = VideoLoaderConfig ( crop_bottom_pixels = None , i_frames = False , scene_threshold = None , megadetector_lite_config = None , video_height = 224 , video_width = 224 , total_frames = 16 , ensure_total_frames = True , fps = None , early_bias = False , frame_indices = None , evenly_sample_total_frames = False , pix_fmt = 'rgb24' ), train_config = None , predict_config = PredictConfig ( data_directory = PosixPath ( 'vids' ), filepaths = filepath 0 / home / ubuntu / vids / eleph . MP4 1 / home / ubuntu / vids / leopard . MP4 2 / home / ubuntu / vids / blank . MP4 3 / home / ubuntu / vids / chimp . MP4 , checkpoint = 'zamba_time_distributed.ckpt' , model_params = ModelParams ( scheduler = None , scheduler_params = None ), model_name = 'time_distributed' , species = None , gpus = 1 , num_workers = 7 , batch_size = 8 , save = True , dry_run = False , proba_threshold = None , output_class_names = False , weight_download_region = 'us' , cache_dir = None , skip_load_validation = False ) ) We can now run inference or model training without specifying any additional parameters, because they are already associated with our instance of the ModelManager class. To run inference or training: manager . predict () # inference manager . train () # training In our user tutorials, we refer to train_model and predict_model functions. The ModelManager class calls these same functions behind the scenes when .predict() or .train() is run.","title":"Python package"},{"location":"yaml-config/#default-configurations","text":"In the command line, the default configuration for each model is passed in using a specified YAML file that ships with zamba <!-- TODO: add link to github><!-->. For example, the default configuration for the time_distributed model is: video_loader_config : video_height : 224 video_width : 224 crop_bottom_pixels : 50 ensure_total_frames : True megadetector_lite_config : confidence : 0.25 fill_model : \"score_sorted\" n_frames : 16 total_frames : 16 train_config : # data_directory: YOUR_DATA_DIRECTORY_HERE # labels: YOUR_LABELS_CSV_HERE model_name : time_distributed # or # checkpoint: YOUR_CKPT_HERE batch_size : 8 num_workers : 3 scheduler_config : scheduler : MultiStepLR scheduler_params : milestones : [ 3 ] gamma : 0.5 verbose : True auto_lr_find : True backbone_finetune : True backbone_finetune_params : unfreeze_backbone_at_epoch : 3 verbose : True pre_train_bn : True multiplier : 1 early_stopping : True early_stopping_params : patience : 5 predict_config : # data_directory: YOUR_DATA_DIRECTORY_HERE # or # filepaths: YOUR_FILEPATH_HERE model_name : time_distributed # or # checkpoint: YOUR_CKPT_HERE For reference, the below shows how to specify the same video loading and training parameters using only the Python package: from zamba.data.video import VideoLoaderConfig from zamba.models.config import TrainConfig from zamba.models.model_manager import train_model video_loader_config = VideoLoaderConfig ( video_height = 224 , video_width = 224 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 16 , }, total_frames = 16 , ) train_config = TrainConfig ( # data_directory=YOUR_DATA_DIRECTORY_HERE, # labels=YOUR_LABELS_CSV_HERE, model_name = \"time_distributed\" , # or # checkpoint=YOUR_CKPT_HERE, batch_size = 8 , backbone_finetune = True , backbone_finetune_params = { \"unfreeze_backbone_at_epoch\" : 3 , \"verbose\" : True , \"pre_train_bn\" : True , \"multiplier\" : 1 , }, num_workers = 3 , auto_lr_find = True , early_stopping = True , early_stopping_params = { \"patience\" : 5 ,}, ) train_model ( train_config = train_config , video_loader_config = video_loader_config )","title":"Default configurations"}]}