{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"zamba \u00b6 zamba means \"forest\" in Lingala, a Bantu language spoken throughout the Democratic Republic of the Congo and the Republic of the Congo. zamba is a tool built in Python that uses machine learning and computer vision to automatically detect and classify animals in camera trap videos. You can use zamba to: Identify which species appear in each video Filter out blank videos The models in zamba can identify blank videos (where no animal is present) along with 32 species common to Africa and 11 species commmon to Europe. Users can also finetune models using their own labeled videos to then make predictions for new species and/or new ecologies. zamba can be used both as a command-line tool and as a Python package. It is also available as a user-friendly website application, Zamba Cloud . Check out the Wiki for community-submitted models. Installing zamba \u00b6 First, make sure you have the prerequisites installed: Python 3.7 or 3.8 FFmpeg Then run: pip install zamba See the Installation page of the documentation for details. Getting started \u00b6 Once you have zamba installed, some good starting points are: The Quickstart page for basic examples of usage The user tutorial for either classifying videos or training a model depending on what you want to do with zamba Example usage \u00b6 Once zamba is installed, you can see the basic command options with: $ zamba --help Usage: zamba [OPTIONS] COMMAND [ARGS]... Zamba is a tool built in Python to automatically identify the species seen in camera trap videos from sites in Africa and Europe. Visit https://zamba.drivendata.org/docs for more in-depth documentation. Options: --version Show zamba version and exit. --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: densepose Run densepose algorithm on videos. predict Identify species in a video. train Train a model on your labeled data. zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. Classifying unlabeled videos \u00b6 $ zamba predict --data-dir path/to/videos By default, predictions will be saved to zamba_predictions.csv . Run zamba predict --help to list all possible options to pass to predict . See the Quickstart page or the user tutorial on classifying videos for more details. Training a model \u00b6 $ zamba train --data-dir path/to/videos --labels path_to_labels.csv --save_dir my_trained_model The newly trained model will be saved to the specified save directory. The folder will contain a model checkpoint as well as training configuration, model hyperparameters, and validation and test metrics. Run zamba train --help to list all possible options to pass to train . See the Quickstart page or the user tutorial on training a model for more details. Running the zamba test suite \u00b6 The included Makefile contains code that uses pytest to run all tests in zamba/tests . The command is (from the project root): $ make tests See the docs page on contributing to zamba for details.","title":"Home"},{"location":"#zamba","text":"zamba means \"forest\" in Lingala, a Bantu language spoken throughout the Democratic Republic of the Congo and the Republic of the Congo. zamba is a tool built in Python that uses machine learning and computer vision to automatically detect and classify animals in camera trap videos. You can use zamba to: Identify which species appear in each video Filter out blank videos The models in zamba can identify blank videos (where no animal is present) along with 32 species common to Africa and 11 species commmon to Europe. Users can also finetune models using their own labeled videos to then make predictions for new species and/or new ecologies. zamba can be used both as a command-line tool and as a Python package. It is also available as a user-friendly website application, Zamba Cloud . Check out the Wiki for community-submitted models.","title":"zamba"},{"location":"#installing-zamba","text":"First, make sure you have the prerequisites installed: Python 3.7 or 3.8 FFmpeg Then run: pip install zamba See the Installation page of the documentation for details.","title":"Installing zamba"},{"location":"#getting-started","text":"Once you have zamba installed, some good starting points are: The Quickstart page for basic examples of usage The user tutorial for either classifying videos or training a model depending on what you want to do with zamba","title":"Getting started"},{"location":"#example-usage","text":"Once zamba is installed, you can see the basic command options with: $ zamba --help Usage: zamba [OPTIONS] COMMAND [ARGS]... Zamba is a tool built in Python to automatically identify the species seen in camera trap videos from sites in Africa and Europe. Visit https://zamba.drivendata.org/docs for more in-depth documentation. Options: --version Show zamba version and exit. --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: densepose Run densepose algorithm on videos. predict Identify species in a video. train Train a model on your labeled data. zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training.","title":"Example usage"},{"location":"#running-the-zamba-test-suite","text":"The included Makefile contains code that uses pytest to run all tests in zamba/tests . The command is (from the project root): $ make tests See the docs page on contributing to zamba for details.","title":"Running the zamba test suite"},{"location":"changelog/","text":"zamba changelog \u00b6 v2 (2021-10-22) \u00b6 Previous model: Machine learning competition \u00b6 The algorithms used by zamba v1 were based on the winning solution from the Pri-matrix Factorization machine learning competition, hosted by DrivenData . Data for the competition was provided by the Chimp&See project and manually labeled by volunteers. The competition had over 300 participants and over 450 submissions throughout the three month challenge. The v1 algorithm was adapted from the winning competition submission, with some aspects changed during development to improve performance. The core algorithm in zamba v1 was a stacked ensemble which consisted of a first layer of models that were then combined into a final prediction in a second layer. The first level of the stack consisted of 5 keras deep learning models, whose individual predictions were combined in the second level of the stack to form the final prediction. In v2, the stacked ensemble algorithm from v1 is replaced with three more powerful single-model options : time_distributed , slowfast , and european . The new models utilize state-of-the-art image and video classification architectures, and are able to outperform the much more computationally intensive stacked ensemble model. New geographies and species \u00b6 zamba v2 incorporates data from western Europe (Germany). The new data is packaged in the pretrained european model, which can predict 11 common European species not present in zamba v1. zamba v2 also incorporates new training data from 15 countries in central and west Africa, and adds 12 additional species to the pretrained African models. Retraining flexibility \u00b6 Model training is made available zamba v2, so users can finetune a pretrained model using their own data to improve performance for a specific ecology or set of sites. zamba v2 also allows users to retrain a model on completely new species labels.","title":"`zamba` changelog"},{"location":"changelog/#zamba-changelog","text":"","title":"zamba changelog"},{"location":"changelog/#v2-2021-10-22","text":"","title":"v2 (2021-10-22)"},{"location":"configurations/","text":"All configuration options \u00b6 Three main configuration classes are specific in zamba : VideoLoaderConfig : Defines all possible parameters for how videos are loaded PredictConfig : Defines all possible parameters for model inference TrainConfig : Defines all possible parameters for model training Here's a helpful diagram which shows how everything is related. Video loading arguments \u00b6 The VideoLoaderConfig class defines all of the optional parameters that can be specified for how videos are loaded before either inference or training. This includes selecting which frames to use from each video. All video loading arguments can be specified either in a YAML file or when instantiating the VideoLoaderConfig class in Python. Some can also be specified directly in the command line. YAML file video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 # ... other parameters Python from zamba.data.video import VideoLoaderConfig from zamba.models.config import PredictConfig from zamba.models.model_manager import predict_model predict_config = PredictConfig ( data_dir = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( model_input_height = 240 , model_input_width = 426 , total_frames = 16 # ... other parameters ) predict_model ( predict_config = predict_config , video_loader_config = video_loader_config ) Let's look at the class documentation in Python. >> from zamba.data.video import VideoLoaderConfig >> help ( VideoLoaderConfig ) class VideoLoaderConfig ( pydantic . main . BaseModel ) | VideoLoaderConfig ( * , crop_bottom_pixels : int = None , i_frames : bool = False , scene_threshold : float = None , megadetector_lite_config : zamba . models . megadetector_lite_yolox . MegadetectorLiteYoloXConfig = None , frame_selection_height : int = None , frame_selection_width : int = None , total_frames : int = None , ensure_total_frames : bool = True , fps : float = None , early_bias : bool = False , frame_indices : List [ int ] = None , evenly_sample_total_frames : bool = False , pix_fmt : str = 'rgb24' , model_input_height : int = None , model_input_width : int = None , cache_dir : pathlib . Path = None , cleanup_cache : bool = False ) -> None ... crop_bottom_pixels (int, optional) \u00b6 Number of pixels to crop from the bottom of the video (prior to resizing to frame_selection_height ). Defaults to None i_frames (bool, optional) \u00b6 Only load the I-Frames . Defaults to False scene_threshold (float, optional) \u00b6 Only load frames that correspond to scene changes . Defaults to None megadetector_lite_config (MegadetectorLiteYoloXConfig, optional) \u00b6 The megadetector_lite_config is used to specify any parameters that should be passed to the MegadetectorLite model for frame selection. For all possible options, see the MegadetectorLiteYoloXConfig class . If megadetector_lite_config is None (the default), the MegadetectorLite model will not be used to select frames. frame_selection_height (int, optional), frame_selection_width (int, optional) \u00b6 Resize the video to this height and width in pixels, prior to frame selection. If None, the full size video will be used for frame selection. Using full size videos (setting to None ) is recommended for MegadetectorLite, especially if your species of interest are smaller. Defaults to None total_frames (int, optional) \u00b6 Number of frames that should ultimately be returned. Defaults to None ensure_total_frames (bool) \u00b6 Some frame selection methods may yield varying numbers of frames. If True , ensure the requested number of frames is returned by either clipping or duplicating the final frame. If no frames are selected, returns an array of the desired shape with all zeros. Otherwise, return the array unchanged. Defaults to True fps (int, optional) \u00b6 Resample the video evenly from the entire duration to a specific number of frames per second. Defaults to None early_bias (bool, optional) \u00b6 Resamples to 24 fps and selects 16 frames biased toward the beginning of the video. This strategy was used by the Pri-matrix Factorization machine learning competition winner. Defaults to False frame_indices (list(int), optional) \u00b6 Select specific frame numbers. Note: frame selection is done after any resampling. Defaults to None evenly_sample_total_frames (bool, optional) \u00b6 Reach the total number of frames specified by evenly sampling from the duration of the video. Defaults to False pix_fmt (str, optional) \u00b6 FFmpeg pixel format, defaults to rgb24 for RGB channels; can be changed to bgr24 for BGR. model_input_height (int, optional), model_input_width (int, optional) \u00b6 After frame selection, resize the video to this height and width in pixels. Defaults to None cache_dir (Path, optional) \u00b6 Cache directory where preprocessed videos will be saved upon first load. Alternatively, can be set with VIDEO_CACHE_DIR environment variable. Provided there is enough space on your machine, it is highly encouraged to cache videos for training as this will speed up all subsequent epochs. If you are predicting on the same videos with the same video loader configuration, this will save time on future runs. Defaults to None , which means videos will not be cached. cleanup_cache (bool, optional) \u00b6 Whether to delete the cache directory after training or predicting ends. Defaults to False Prediction arguments \u00b6 All possible model inference parameters are defined by the PredictConfig class . Let's see the class documentation in Python: >> from zamba.models.config import PredictConfig >> help ( PredictConfig ) class PredictConfig ( ZambaBaseModel ) | PredictConfig ( * , data_dir : DirectoryPath = Path . cwd (), filepaths : FilePath = None , checkpoint : FilePath = None , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , gpus : int = 0 , num_workers : int = 3 , batch_size : int = 2 , save : bool = True , save_dir : Optional [ Path ] = None , overwrite : bool = False , dry_run : bool = False , proba_threshold : float = None , output_class_names : bool = False , weight_download_region : zamba . models . utils . RegionEnum = 'us' , skip_load_validation : bool = False , model_cache_dir : pathlib . Path = None ) -> None ... Either data_dir or filepaths must be specified to instantiate PredictConfig . If neither is specified, the current working directory will be used as the default data_dir . data_dir (DirectoryPath, optional) \u00b6 Path to the directory containing videos for inference. Defaults to the current working directory. filepaths (FilePath, optional) \u00b6 Path to a csv containing a filepath column with videos for classification. checkpoint (Path or str, optional) \u00b6 Path to a model checkpoint to load and use for inference. The default is None , which will load the pretrained checkpoint if the model specified by model_name . model_name (time_distributed|slowfast|european, optional) \u00b6 Name of the model to use for inference. The three model options that ship with zamba are time_distributed , slowfast , and european . See the Available Models page for details. Defaults to time_distributed gpus (int, optional) \u00b6 The number of GPUs to use during inference. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available on the machine. num_workers (int, optional) \u00b6 The number of CPUs to use during training. The maximum value for num_workers is the number of CPUs available on the machine. If you are using MegadetectorLite for frame selection, it is not recommended to use the total number of CPUs available. Defaults to 3 batch_size (int, optional) \u00b6 The batch size to use for inference. Defaults to 2 save (bool) \u00b6 Whether to save out predictions. If False , predictions are not saved. Defaults to True . save_dir (Path, optional) \u00b6 An optional directory in which to save the model predictions and configuration yaml. If no save_dir is specified and save is True, outputs will be written to the current working directory. Defaults to None overwrite (bool) \u00b6 If True, will overwrite zamba_predictions.csv and predict_configuration.yaml in save_dir if they exist. Defaults to False. dry_run (bool, optional) \u00b6 Specifying True is useful for trying out model implementations more quickly by running only a single batch of inference. Defaults to False proba_threshold (float between 0 and 1, optional) \u00b6 For advanced uses, you may want the algorithm to be more or less sensitive to if a species is present. This parameter is a float, e.g., 0.6 corresponding to the probability threshold beyond which an animal is considered to be present in the video being analyzed. By default no threshold is passed, proba_threshold=None . This will return a probability from 0-1 for each species that could occur in each video. If a threshold is passed, then the final prediction value returned for each class is probability >= proba_threshold , so that all class values become 0 ( False , the species does not appear) or 1 ( True , the species does appear). output_class_names (bool, optional) \u00b6 Setting this option to True yields the most concise output zamba is capable of. The highest species probability in a video is taken to be the only species in that video, and the output returned is simply the video name and the name of the species with the highest class probability, or blank if the most likely classification is no animal. Defaults to False weight_download_region [us|eu|asia] \u00b6 Because zamba needs to download pretrained weights for the neural network architecture, we make these weights available in different regions. us is the default, but if you are not in the US you should use either eu for the European Union or asia for Asia Pacific to make sure that these download as quickly as possible for you. skip_load_validation (bool, optional) \u00b6 By default, before kicking off inference zamba will iterate through all of the videos in the data and verify that each can be loaded. Setting skip_load_verification to True skips this step. Validation can be very time intensive depending on the number of videos. It is recommended to run validation once, but not on future inference runs if the videos have not changed. Defaults to False model_cache_dir (Path, optional) \u00b6 Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, will use your default cache directory (e.g. ~/.cache ). Defaults to None Training arguments \u00b6 All possible model training parameters are defined by the TrainConfig class . Let's see the class documentation in Python: >> from zamba.models.config import TrainConfig >> help ( TrainConfig ) class TrainConfig ( ZambaBaseModel ) | TrainConfig ( * , labels : Union [ FilePath , pandas . DataFrame ], data_dir : DirectoryPath = # your current working directory , checkpoint : FilePath = None , scheduler_config : Union [ str , zamba . models . config . SchedulerConfig , NoneType ] = 'default' , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , dry_run : Union [ bool , int ] = False , batch_size : int = 2 , auto_lr_find : bool = False , backbone_finetune_config : zamba . models . config . BackboneFinetuneConfig = BackboneFinetuneConfig ( unfreeze_backbone_at_epoch = 5 , backbone_initial_ratio_lr = 0.01 , multiplier = 1 , pre_train_bn = False , train_bn = False , verbose = True ), gpus : int = 0 , num_workers : int = 3 , max_epochs : int = None , early_stopping_config : zamba . models . config . EarlyStoppingConfig = EarlyStoppingConfig ( monitor = 'val_macro_f1' , patience = 5 , verbose = True , mode = 'max' ), weight_download_region : zamba . models . utils . RegionEnum = 'us' , split_proportions : Dict [ str , int ] = { 'train' : 3 , 'val' : 1 , 'holdout' : 1 }, save_dir : pathlib . Path = # your current working directory , overwrite : bool = False , skip_load_validation : bool = False , from_scratch : bool = False , predict_all_zamba_species : bool = True , model_cache_dir : pathlib . Path = None ) -> None ... labels (FilePath or pd.DataFrame, required) \u00b6 Either the path to a CSV file with labels for training, or a dataframe of the training labels. There must be columns for filename and label . labels must be specified to instantiate TrainConfig . data_dir (DirectoryPath, optional) \u00b6 Path to the directory containing training videos. Defaults to the current working directory. checkpoint (Path or str, optional) \u00b6 Path to a model checkpoint to load and resume training from. The default is None , which automatically loads the pretrained checkpoint for the model specified by model_name . Since the default model_name is time_distributed the default checkpoint is zamba_time_distributed.ckpt scheduler_config (zamba.models.config.SchedulerConfig, optional) \u00b6 A PyTorch learning rate schedule to adjust the learning rate based on the number of epochs. Scheduler can either be default (the default), None , or a torch.optim.lr_scheduler . If default , model_name (time_distributed|slowfast|european, optional) \u00b6 Name of the model to use for inference. The three model options that ship with zamba are time_distributed , slowfast , and european . See the Available Models page for details. Defaults to time_distributed dry_run (bool, optional) \u00b6 Specifying True is useful for trying out model implementations more quickly by running only a single batch of train and validation. Defaults to False batch_size (int, optional) \u00b6 The batch size to use for training. Defaults to 2 auto_lr_find (bool, optional) \u00b6 Whether to run a learning rate finder algorithm when calling pytorch_lightning.trainer.tune() to try to find an optimal initial learning rate. The learning rate finder is not guaranteed to find a good learning rate; depending on the dataset, it can select a learning rate that leads to poor model training. Use with caution. See the PyTorch Lightning docs for more details. Defaults to False . backbone_finetune_config (zamba.models.config.BackboneFinetuneConfig, optional) \u00b6 Set parameters to finetune a backbone model to align with the current learning rate. Derived from Pytorch Lightning's built-in BackboneFinetuning . The default values are specified in the BackboneFinetuneConfig class : BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5, backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False, train_bn=False, verbose=True) gpus (int, optional) \u00b6 The number of GPUs to use during training. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available on the machine. num_workers (int, optional) \u00b6 The number of CPUs to use during training. The maximum value for num_workers is the number of CPUs available in the system. If you are using the Megadetector, it is not recommended to use the total number of CPUs available. Defaults to 3 max_epochs (int, optional) \u00b6 The maximum number of epochs to run during training. Defaults to None early_stopping_config (zamba.models.config.EarlyStoppingConfig, optional) \u00b6 Parameters to pass to Pytorch lightning's EarlyStopping to monitor a metric during model training and stop training when the metric stops improving. The default values are specified in the EarlyStoppingConfig class : EarlyStoppingConfig(monitor='val_macro_f1', patience=5, verbose=True, mode='max') weight_download_region [us|eu|asia] \u00b6 Because zamba needs to download pretrained weights for the neural network architecture, we make these weights available in different regions. us is the default, but if you are not in the US you should use either eu for the European Union or asia for Asia Pacific to make sure that these download as quickly as possible for you. split_proportions (dict(str, int), optional) \u00b6 The proportion of data to use during training, validation, and as a holdout set. Defaults to {\"train\": 3, \"val\": 1, \"holdout\": 1} save_dir (Path, optional) \u00b6 Directory in which to save model checkpoint and configuration file. If not specified, will save to a version_n folder in your current working directory. overwrite (bool, optional) \u00b6 If True , will save outputs in save_dir and overwrite the directory if it exists. If False, will create an auto-incremented version_n folder within save_dir with model outputs. Defaults to False . skip_load_validation (bool, optional) \u00b6 By default, before kicking off training zamba will iterate through all of the videos in the training data and verify that each can be loaded. Setting skip_load_verification to True skips this step. Validation can be very time intensive depending on the number of videos. It is recommended to run validation once, but not on future training runs if the videos have not changed. Defaults to False from_scratch (bool, optional) \u00b6 Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Only used if labels is not None. Defaults to False predict_all_zamba_species (bool, optional) \u00b6 Whether the species outputted by the model should be all zamba species. If you want the model classes to only be the species in your labels file, set to False . Only used if labels is not None . If either predict_all_zamba_species is False or the labels contain species that are not in the model, the model head will be replaced. Defaults to True model_cache_dir (Path, optional) \u00b6 Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, will use your default cache directory, which is often an automatic temp directory at ~/.cache/zamba . Defaults to None","title":"All configuration options"},{"location":"configurations/#all-configuration-options","text":"Three main configuration classes are specific in zamba : VideoLoaderConfig : Defines all possible parameters for how videos are loaded PredictConfig : Defines all possible parameters for model inference TrainConfig : Defines all possible parameters for model training Here's a helpful diagram which shows how everything is related.","title":"All configuration options"},{"location":"configurations/#video-loading-arguments","text":"The VideoLoaderConfig class defines all of the optional parameters that can be specified for how videos are loaded before either inference or training. This includes selecting which frames to use from each video. All video loading arguments can be specified either in a YAML file or when instantiating the VideoLoaderConfig class in Python. Some can also be specified directly in the command line. YAML file video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 # ... other parameters Python from zamba.data.video import VideoLoaderConfig from zamba.models.config import PredictConfig from zamba.models.model_manager import predict_model predict_config = PredictConfig ( data_dir = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( model_input_height = 240 , model_input_width = 426 , total_frames = 16 # ... other parameters ) predict_model ( predict_config = predict_config , video_loader_config = video_loader_config ) Let's look at the class documentation in Python. >> from zamba.data.video import VideoLoaderConfig >> help ( VideoLoaderConfig ) class VideoLoaderConfig ( pydantic . main . BaseModel ) | VideoLoaderConfig ( * , crop_bottom_pixels : int = None , i_frames : bool = False , scene_threshold : float = None , megadetector_lite_config : zamba . models . megadetector_lite_yolox . MegadetectorLiteYoloXConfig = None , frame_selection_height : int = None , frame_selection_width : int = None , total_frames : int = None , ensure_total_frames : bool = True , fps : float = None , early_bias : bool = False , frame_indices : List [ int ] = None , evenly_sample_total_frames : bool = False , pix_fmt : str = 'rgb24' , model_input_height : int = None , model_input_width : int = None , cache_dir : pathlib . Path = None , cleanup_cache : bool = False ) -> None ...","title":"Video loading arguments"},{"location":"configurations/#prediction-arguments","text":"All possible model inference parameters are defined by the PredictConfig class . Let's see the class documentation in Python: >> from zamba.models.config import PredictConfig >> help ( PredictConfig ) class PredictConfig ( ZambaBaseModel ) | PredictConfig ( * , data_dir : DirectoryPath = Path . cwd (), filepaths : FilePath = None , checkpoint : FilePath = None , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , gpus : int = 0 , num_workers : int = 3 , batch_size : int = 2 , save : bool = True , save_dir : Optional [ Path ] = None , overwrite : bool = False , dry_run : bool = False , proba_threshold : float = None , output_class_names : bool = False , weight_download_region : zamba . models . utils . RegionEnum = 'us' , skip_load_validation : bool = False , model_cache_dir : pathlib . Path = None ) -> None ... Either data_dir or filepaths must be specified to instantiate PredictConfig . If neither is specified, the current working directory will be used as the default data_dir .","title":"Prediction arguments"},{"location":"configurations/#training-arguments","text":"All possible model training parameters are defined by the TrainConfig class . Let's see the class documentation in Python: >> from zamba.models.config import TrainConfig >> help ( TrainConfig ) class TrainConfig ( ZambaBaseModel ) | TrainConfig ( * , labels : Union [ FilePath , pandas . DataFrame ], data_dir : DirectoryPath = # your current working directory , checkpoint : FilePath = None , scheduler_config : Union [ str , zamba . models . config . SchedulerConfig , NoneType ] = 'default' , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , dry_run : Union [ bool , int ] = False , batch_size : int = 2 , auto_lr_find : bool = False , backbone_finetune_config : zamba . models . config . BackboneFinetuneConfig = BackboneFinetuneConfig ( unfreeze_backbone_at_epoch = 5 , backbone_initial_ratio_lr = 0.01 , multiplier = 1 , pre_train_bn = False , train_bn = False , verbose = True ), gpus : int = 0 , num_workers : int = 3 , max_epochs : int = None , early_stopping_config : zamba . models . config . EarlyStoppingConfig = EarlyStoppingConfig ( monitor = 'val_macro_f1' , patience = 5 , verbose = True , mode = 'max' ), weight_download_region : zamba . models . utils . RegionEnum = 'us' , split_proportions : Dict [ str , int ] = { 'train' : 3 , 'val' : 1 , 'holdout' : 1 }, save_dir : pathlib . Path = # your current working directory , overwrite : bool = False , skip_load_validation : bool = False , from_scratch : bool = False , predict_all_zamba_species : bool = True , model_cache_dir : pathlib . Path = None ) -> None ...","title":"Training arguments"},{"location":"debugging/","text":"Debugging \u00b6 Before kicking off a full run of inference or model training, we recommend testing your code with a \"dry run\". If you are generating predictions, this will run one batch of inference to quickly detect any bugs. If you are trainig a model, this will run one training and validation batch for one epoch. If the dry run completes successfully, predict and train away! CLI $ zamba predict --data-dir example_vids/ --dry-run $ zamba train --data-dir example_vids/ --labels example_labels.csv --dry-run Python In Python, add dry_run=True to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , dry_run = True ) GPU memory errors \u00b6 The dry run will also catch any GPU memory errors. If you hit a GPU memory error, there are a couple fixes. Reducing the batch size \u00b6 CLI zamba train --data-dir example_vids/ --labels example_labels.csv --batch-size 1 Python In Python, add batch_size to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , batch_size = 1 ) Decreasing video size \u00b6 Resize video frames to be smaller before they are passed to the model. The default for all three models is 240x426 pixels. model_input_height and model_input_width cannot be passed directly to the command line, so if you are using the CLI these must be specified in a YAML file . YAML file video_loader_config : model_input_height : 100 model_input_width : 100 total_frames : 16 # total_frames is always required Python video_loader_config = VideoLoaderConfig ( model_input_height = 100 , model_input_width = 100 , total_frames = 16 ) # total_frames is always required Reducing num_workers \u00b6 Reduce the number of workers (subprocesses) used for data loading. By default num_workers will be set to 3. The minimum value is 0, which means that the data will be loaded in the main process, and the maximum is one less than the number of CPUs in the system. CLI $ zamba predict --data-dir example_vids/ --num-workers 1 $ zamba train --data-dir example_vids/ --labels example_labels.csv --num-workers 1 Python In Python, add num_workers to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , num_workers = 1 ) Logging \u00b6 To check that videos are getting loaded and cached as expected, set your environment variabe LOG_LEVEL to debug . The default log level is info .","title":"Debugging"},{"location":"debugging/#debugging","text":"Before kicking off a full run of inference or model training, we recommend testing your code with a \"dry run\". If you are generating predictions, this will run one batch of inference to quickly detect any bugs. If you are trainig a model, this will run one training and validation batch for one epoch. If the dry run completes successfully, predict and train away! CLI $ zamba predict --data-dir example_vids/ --dry-run $ zamba train --data-dir example_vids/ --labels example_labels.csv --dry-run Python In Python, add dry_run=True to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , dry_run = True )","title":"Debugging"},{"location":"debugging/#gpu-memory-errors","text":"The dry run will also catch any GPU memory errors. If you hit a GPU memory error, there are a couple fixes.","title":"GPU memory errors"},{"location":"debugging/#logging","text":"To check that videos are getting loaded and cached as expected, set your environment variabe LOG_LEVEL to debug . The default log level is info .","title":"Logging"},{"location":"extra-options/","text":"Guide to common optional parameters \u00b6 There are a LOT of ways to customize model training or inference. Here, we take that elephant-sized list of options and condense it to a manageable monkey-sized list of common considerations. To read about all possible customizations, see All Configuration Options . Many of the options below cannot be passed directly to the command line. Instead, some must be passed as part of a YAML configuration file. For example: $ zamba train --config path_to_your_config_file.yaml For using a YAML file with the Python package and other details, see the YAML Configuration File page. Downloading model weights \u00b6 zamba needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Model weights are stored on servers in three locations, and downloading weights from the server closest to you will run the fastest. By default, weights will be downloaded from the US. To specify a different region: CLI zamba predict --data-dir example_vids/ --weight_download_region asia Python In Python this can be specified in PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , weight_download_region = 'asia' , ) The options for weight_download_region are us , eu , and asia . Once a model's weights are downloaded, zamba will use the local version and will not need to perform this download again. Video size \u00b6 When zamba loads videos prior to either inference or training, it resizes all of the video frames before feeding them into a model. Higher resolution videos will lead to superior accuracy in prediction, but will use more memory and take longer to train and/or predict. The default video loading configuration for all three pretrained models resizes images to 240x426 pixels. Say that you have a large number of videos, and you are more concerned with detecting blank v. non-blank videos than with identifying different species. In this case, you may not need a very high resolution and iterating through all of your videos with a high resolution would take a very long time. To resize all images to 50x50 pixels instead of the default 240x426: YAML file video_loader_config : model_input_height : 50 model_input_width : 50 total_frames : 16 # total_frames must always be specified Python In Python, video resizing can be specified when VideoLoaderConfig is instantiated: from zamba.data.video import VideoLoaderConfig from zamba.models.config import PredictConfig from zamba.models.model_manager import predict_model predict_config = PredictConfig ( data_dir = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( model_input_height = 50 , model_input_width = 50 , total_frames = 16 ) # total_frames must always be specified predict_model ( predict_config = predict_config , video_loader_config = video_loader_config ) Frame selection \u00b6 Each video is simply a series of frames, or images. Most of the videos on which zamba was trained had 30 frames per second. That means even just a 15-second video would contain 450 frames. All models only use a subset of the frames in a video, because using every frame would be far too computationally intensive. There are a number of different ways to select frames. For a full list of options, see the section on Video loading arguments . A few common approaches are explained below. Early bias \u00b6 Some camera traps begin recording a video when movement is detected. If this is the case, you may be more likely to see an animal towards when the video starts. Setting early_bias to True selects 16 frames towards the beginning of a video. YAML File video_loader_config : early_bias : True # ... other parameters Python In Python, early_bias is specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( early_bias = True , ... ) This method was used by the winning solution of the Pri-matrix Factorization machine learning competition, which was the basis for zamba v1. Evenly distributed frames \u00b6 A simple option is to sample frames that are evenly distributed throughout a video. For example, to select 32 evenly distributed frames: YAML file video_loader_config : total_frames : 32 evenly_sample_total_frames : True ensure_total_frames : True # ... other parameters Python In Python, these arguments can be specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( total_frames = 32 , evenly_sample_total_frames = True , ensure_total_frames = True , ... ) MegadetectorLite \u00b6 You can use a pretrained object detection model called MegadetectorLite to select only the frames that are mostly likely to contain an animal. This is the default strategy for all three pretrained models. The parameter megadetector_lite_config is used to specify any arguments that should be passed to the MegadetectorLite model. If megadetector_lite_config is None, the MegadetectorLite model will not be used. For example, to take the 16 frames with the highest probability of detection: YAML file video_loader_config : megadetector_lite_config : n_frames : 16 fill_mode : \"score_sorted\" # ... other parameters Python In Python, these can be specified in the megadetector_lite_config argument passed to VideoLoaderConfig : video_loader_config = VideoLoaderConfig ( model_input_height = 240 , model_input_width = 426 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 16 , }, total_frames = 16 , ) train_config = TrainConfig ( data_dir = \"example_vids/\" , labels = \"example_labels.csv\" ,) train_model ( video_loader_config = video_loader_config , train_config = train_config ) If you are using the MegadetectorLite for frame selection, there are two ways that you can specify frame resizing: frame_selection_width and frame_selection_height resize images before they are input to the frame selection method. If both are None , the full size images will be used during frame selection. Using full size images for selection is recommended for better detection of smaller species, but will slow down training and inference. model_input_height and model_input_width resize images after frame selection. These specify the image size that is passed to the actual model. You can specify both of the above at once, just one, or neither. The example code feeds full-size images to MegadetectorLite, and then resizes images before running them through the neural network. To see all of the options that can be passed to the MegadetectorLite, see the MegadetectorLiteYoloXConfig class . Speed up training \u00b6 Training will run faster if you increase num_workers and/or increase batch_size . num_workers is the number of subprocesses to use for data loading. The minimum is 0, meaning the data will be loaded in the main process, and the maximum is one less than the number of CPUs in your system. By default num_workers is set to 3 and batch_size is set to 2. Increasing either of these will use more GPU memory, and could raise an error if the memory required is more than your machine has available. Both can be specified in either predict_config or train_config . For example, to increase num_workers to 5 and batch_size to 4 for inference: YAML file predict_config : data_dir : example_vids/ num_workers : 5 batch_size : 4 # ... other parameters Python predict_config = PredictConfig ( data_dir = \"example_vids/\" , num_workers = 5 , batch_size = 4 , # ... other parameters ) And that's just the tip of the iceberg! See All Configuration Options page for more possibilities.","title":"Guide to common optional parameters"},{"location":"extra-options/#guide-to-common-optional-parameters","text":"There are a LOT of ways to customize model training or inference. Here, we take that elephant-sized list of options and condense it to a manageable monkey-sized list of common considerations. To read about all possible customizations, see All Configuration Options . Many of the options below cannot be passed directly to the command line. Instead, some must be passed as part of a YAML configuration file. For example: $ zamba train --config path_to_your_config_file.yaml For using a YAML file with the Python package and other details, see the YAML Configuration File page.","title":"Guide to common optional parameters"},{"location":"extra-options/#downloading-model-weights","text":"zamba needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Model weights are stored on servers in three locations, and downloading weights from the server closest to you will run the fastest. By default, weights will be downloaded from the US. To specify a different region: CLI zamba predict --data-dir example_vids/ --weight_download_region asia Python In Python this can be specified in PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , weight_download_region = 'asia' , ) The options for weight_download_region are us , eu , and asia . Once a model's weights are downloaded, zamba will use the local version and will not need to perform this download again.","title":"Downloading model weights"},{"location":"extra-options/#video-size","text":"When zamba loads videos prior to either inference or training, it resizes all of the video frames before feeding them into a model. Higher resolution videos will lead to superior accuracy in prediction, but will use more memory and take longer to train and/or predict. The default video loading configuration for all three pretrained models resizes images to 240x426 pixels. Say that you have a large number of videos, and you are more concerned with detecting blank v. non-blank videos than with identifying different species. In this case, you may not need a very high resolution and iterating through all of your videos with a high resolution would take a very long time. To resize all images to 50x50 pixels instead of the default 240x426: YAML file video_loader_config : model_input_height : 50 model_input_width : 50 total_frames : 16 # total_frames must always be specified Python In Python, video resizing can be specified when VideoLoaderConfig is instantiated: from zamba.data.video import VideoLoaderConfig from zamba.models.config import PredictConfig from zamba.models.model_manager import predict_model predict_config = PredictConfig ( data_dir = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( model_input_height = 50 , model_input_width = 50 , total_frames = 16 ) # total_frames must always be specified predict_model ( predict_config = predict_config , video_loader_config = video_loader_config )","title":"Video size"},{"location":"extra-options/#frame-selection","text":"Each video is simply a series of frames, or images. Most of the videos on which zamba was trained had 30 frames per second. That means even just a 15-second video would contain 450 frames. All models only use a subset of the frames in a video, because using every frame would be far too computationally intensive. There are a number of different ways to select frames. For a full list of options, see the section on Video loading arguments . A few common approaches are explained below.","title":"Frame selection"},{"location":"extra-options/#speed-up-training","text":"Training will run faster if you increase num_workers and/or increase batch_size . num_workers is the number of subprocesses to use for data loading. The minimum is 0, meaning the data will be loaded in the main process, and the maximum is one less than the number of CPUs in your system. By default num_workers is set to 3 and batch_size is set to 2. Increasing either of these will use more GPU memory, and could raise an error if the memory required is more than your machine has available. Both can be specified in either predict_config or train_config . For example, to increase num_workers to 5 and batch_size to 4 for inference: YAML file predict_config : data_dir : example_vids/ num_workers : 5 batch_size : 4 # ... other parameters Python predict_config = PredictConfig ( data_dir = \"example_vids/\" , num_workers = 5 , batch_size = 4 , # ... other parameters ) And that's just the tip of the iceberg! See All Configuration Options page for more possibilities.","title":"Speed up training"},{"location":"install/","text":"Installing zamba \u00b6 Zamba has been developed and tested on macOS and Ubuntu Linux for both CPU and GPU configurations. To install zamba \u00b6 1. Install prerequisites \u00b6 Prerequisites: Python 3.7 or 3.8 FFmpeg Python 3.7 or 3.8 \u00b6 We recommend Python installation using Anaconda for all platforms. For more information about how to install Anaconda, here are some useful YouTube videos of installation: Anaconda download link Windows install video macOS installation video FFmpeg version 4.3 \u00b6 FFmpeg is an open source library for loading videos of different codecs. Using FFmpeg means that zamba can be flexible in terms of the video formats we support. FFmpeg can be installed on all different platforms, but requires some additional configuration depending on the platform. Here are some videos and instructions walking through FFmpeg installation: FFmpeg download link Install on Ubuntu or Linux . In the command line, enter sudo apt update and then sudo apt install ffmpeg . MacOS install video First, install Homebrew . Then run brew install ffmpeg To check that FFmpeg is installed, run ffmpeg : $ ffmpeg ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers built with Apple clang version 12.0.0 (clang-1200.0.32.29) ... To check your installed version, run ffmpeg -version . 2. Install zamba \u00b6 On macOS, run these commands in the terminal (\u2318+space, \"Terminal\"). On Windows, run them in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt). To install for development: $ pip install zamba To check what version of zamba you have installed: $ pip show zamba To update zamba to the most recent version if needed: $ pip install -U zamba Operating systems that have been tested \u00b6 macOS \u00b6 zamba has been tested on macOS High Sierra. Linux \u00b6 zamba has been tested on Ubuntu versions 16 and 17. Note: zamba has not been tested on Windows 10. Using GPU(s) \u00b6 zamba is much faster on a machine with a graphics processing unit (GPU), but has also been developed and tested for machines without GPU(s). To use a GPU, you must be using an NVIDIA GPU , have installed and configured CUDA , and have installed and configured CuDNN per their specifications.","title":"Installing zamba"},{"location":"install/#installing-zamba","text":"Zamba has been developed and tested on macOS and Ubuntu Linux for both CPU and GPU configurations.","title":"Installing zamba"},{"location":"install/#to-install-zamba","text":"","title":"To install zamba"},{"location":"install/#operating-systems-that-have-been-tested","text":"","title":"Operating systems that have been tested"},{"location":"install/#using-gpus","text":"zamba is much faster on a machine with a graphics processing unit (GPU), but has also been developed and tested for machines without GPU(s). To use a GPU, you must be using an NVIDIA GPU , have installed and configured CUDA , and have installed and configured CuDNN per their specifications.","title":"Using GPU(s)"},{"location":"predict-tutorial/","text":"User tutorial: Classifying unlabeled videos \u00b6 This section walks through how to classify videos using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have unlabeled videos that you want to generate labels for The possible class species labels for your videos are included in the list of possible zamba labels . If your species are not included in this list, you can retrain a model using your own labeled data and then run inference. Basic usage: command line interface \u00b6 Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction in the command line: $ zamba predict --data-dir example_vids/ Required arguments \u00b6 To run zamba predict in the command line, you must specify --data-dir and/or --filepaths . --data-dir PATH : Path to the folder containing your videos. --filepaths PATH : Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for filepath . Filepaths can be absolute or relative to the data directory. All other flags are optional. To choose a model, either --model or --checkpoint must be specified. Use --model to specify one of the three pretrained models that ship with zamba . Use --checkpoint to run inference with a locally saved model. --model defaults to time_distributed . Basic usage: Python package \u00b6 Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction using the Python package: from zamba.models.model_manager import predict_model from zamba.models.config import PredictConfig predict_config = PredictConfig ( data_dir = \"example_vids/\" ) predict_model ( predict_config = predict_config ) The only two arguments that can be passed to predict_model are predict_config and (optionally) video_loader_config . The first step is to instantiate PredictConfig . Optionally, you can also specify video loading arguments by instantiating and passing in VideoLoaderConfig . Required arguments \u00b6 To run predict_model in Python, you must specify either data_dir or filepaths when PredictConfig is instantiated. data_dir (DirectoryPath) : Path to the folder containing your videos. filepaths (FilePath) : Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for filepath . Filepaths can be absolute or relative to the data directory. For detailed explanations of all possible configuration arguments, see All Optional Arguments . Default behavior \u00b6 By default, the time_distributed model will be used. zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) can be interpreted as the probability that animal j is present in video i. By default, predictions will be saved to zamba_predictions.csv in your working directory. You can save predictions to a custom directory using the --save-dir argument. $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal eleph.MP4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 leopard.MP4,0.0,0.0,0.0,0.0,2e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 blank.MP4,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 chimp.MP4,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,4e-05,0.00162,0.0,0.0,0.0,0.0,0.0,2e-05,2e-05,0.0,1e-05,0.0,0.0038,4e-05,0.0 The full prediction and video loading configuration for the process will also be saved out, in the same folder as the predictions under predict_configuration.yaml . To run the exact same inference process a second time, you can pass this YAML file to zamba predict per the Using YAML Configuration Files page: $ zamba predict --config predict_configuration.yaml Step-by-step tutorial \u00b6 1. Specify the path to your videos \u00b6 Save all of your videos within one folder. They can be in nested subdirectories within the folder. Your videos should be in be saved in formats that are suppored by FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. By default, zamba will look for files with the following suffixes: .avi , .mp4 , .asf . To use other video suffixes that are supported by FFmpeg, set your VIDEO_SUFFIXES environment variable. Add the path to your video folder. For example, if your videos are in a folder called example_vids : CLI $ zamba predict --data-dir example_vids/ Python predict_config = PredictConfig ( data_dir = 'example_vids/' ) predict_model ( predict_config = predict_config ) 2. Choose a model for prediction \u00b6 If your camera videos contain species common to Central or West Africa, use either the time_distributed model or slowfast model model. slowfast is better for blank and small species detection. time_distributed performs better if you have many different species of interest, or are focused on duikers, chimpanzees, and/or gorillas. If your videos contain species common to Europe, use the european model . Add the model name to your command. The time_distributed model will be used if no model is specified. For example, if you want to use the slowfast model to classify the videos in example_vids : CLI $ zamba predict --data-dir example_vids/ --model slowfast Python predict_config = PredictConfig ( data_dir = 'example_vids/' , model_name = 'slowfast' ) predict_model ( predict_config = predict_config ) 3. Choose the output format \u00b6 There are three options for how to format predictions, listed from most information to least: Store all probabilities (default): Return predictions with a row for each filename and a column for each class label, with probabilities between 0 and 1. Cell (i,j) is the probability that animal j is present in video i . Presence/absence: Return predictions with a row for each filename and a column for each class label, with cells indicating either presence or absense based on a user-specified probability threshold. Cell (i, j) indicates whether animal j is present ( 1 ) or not present ( 0 ) in video i . The probability threshold cutoff is specified with --proba-threshold in the CLI. Most likely class: Return predictions with a row for each filename and one column for the most likely class in each video. The most likely class can also be blank. To get the most likely class, add --output-class-names to your command. In Python, it can be specified by adding output_class_names=True when PredictConfig is instantiated. This is not recommended if you'd like to detect more than one species in each video. Say we want to generate predictions for the videos in example_vids indicating which animals are present in each video based on a probability threshold of 50%: CLI $ zamba predict --data-dir example_vids/ --proba-threshold 0 .5 $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal eleph.MP4,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 leopard.MP4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0 blank.MP4,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 chimp.MP4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 Python predict_config = PredictConfig ( data_dir = \"example_vids/\" , proba_threshold = 0.5 ) predict_model ( predict_config = predict_config ) predictions = pd . read_csv ( \"zamba_predictions.csv\" ) predictions filepath aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal blank.MP4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 chimp.MP4 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 eleph.MP4 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 leopard.MP4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 4. Specify any additional parameters \u00b6 And there's so much more! You can also do things like specify your region for faster model download ( --weight-download-region ), use a saved model checkpoint ( --checkpoint ), or specify a different folder where your predictions should be saved ( --save-dir ). To read about a few common considerations, see the Guide to Common Optional Parameters page. 5. Test your configuration with a dry run \u00b6 Before kicking off a full run of inference, we recommend testing your code with a \"dry run\". This will run one batch of inference to quickly detect any bugs. See the Debugging page for details.","title":"Classifying unlabeled videos"},{"location":"predict-tutorial/#user-tutorial-classifying-unlabeled-videos","text":"This section walks through how to classify videos using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have unlabeled videos that you want to generate labels for The possible class species labels for your videos are included in the list of possible zamba labels . If your species are not included in this list, you can retrain a model using your own labeled data and then run inference.","title":"User tutorial: Classifying unlabeled videos"},{"location":"predict-tutorial/#basic-usage-command-line-interface","text":"Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction in the command line: $ zamba predict --data-dir example_vids/","title":"Basic usage: command line interface"},{"location":"predict-tutorial/#basic-usage-python-package","text":"Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction using the Python package: from zamba.models.model_manager import predict_model from zamba.models.config import PredictConfig predict_config = PredictConfig ( data_dir = \"example_vids/\" ) predict_model ( predict_config = predict_config ) The only two arguments that can be passed to predict_model are predict_config and (optionally) video_loader_config . The first step is to instantiate PredictConfig . Optionally, you can also specify video loading arguments by instantiating and passing in VideoLoaderConfig .","title":"Basic usage: Python package"},{"location":"predict-tutorial/#default-behavior","text":"By default, the time_distributed model will be used. zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) can be interpreted as the probability that animal j is present in video i. By default, predictions will be saved to zamba_predictions.csv in your working directory. You can save predictions to a custom directory using the --save-dir argument. $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal eleph.MP4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 leopard.MP4,0.0,0.0,0.0,0.0,2e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 blank.MP4,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 chimp.MP4,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,4e-05,0.00162,0.0,0.0,0.0,0.0,0.0,2e-05,2e-05,0.0,1e-05,0.0,0.0038,4e-05,0.0 The full prediction and video loading configuration for the process will also be saved out, in the same folder as the predictions under predict_configuration.yaml . To run the exact same inference process a second time, you can pass this YAML file to zamba predict per the Using YAML Configuration Files page: $ zamba predict --config predict_configuration.yaml","title":"Default behavior"},{"location":"predict-tutorial/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"quickstart/","text":"Quickstart \u00b6 This section assumes you have successfully installed zamba and are ready to train a model or identify species in your videos! zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. To perform inference, you simply need to run zamba predict followed by a set of arguments that let zamba know where your videos are located, which model you want to use, and where to save your output. To train a model, you can similarly run zamba train and specify your labels. The following sections provide details about these separate modules. There are two ways to interact with the zamba package: Use zamba as a command line interface tool. This page provides an overview of how to use the CLI. Import zamba in Python and use it as a Python package. For instructions on using the Python package, see the user tutorial that corresponds to your use case. Installation is the same for both the command line interface tool and the Python package. All of the commands on this page should be run at the command line. On macOS, this can be done in the terminal (\u2318+space, \"Terminal\"). On Windows, this can be done in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt). How do I organize my videos for zamba ? \u00b6 You can specify the path to a directory of videos or specify a list of filepaths in a .csv file. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. For example, say we have a directory of videos called example_vids that we want to generate predictions for using zamba . Let's list the videos: $ ls example_vids/ blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 Here are some screenshots from those videos: blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 In this example, the videos have meaningful names so that we can easily compare the predictions made by zamba . In practice, your videos will probably be named something much less useful! Generating predictions \u00b6 To generate and save predictions for your videos using the default settings, run: $ zamba predict --data-dir example_vids/ zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) is the probability that animal j is present in video i . Comprehensive predictions are helpful when a single video contains multiple species. Predictions will be saved to zamba_predictions.csv in the current working directory by default. You can save out predictions to a different folder using the --save-dir argument. Adding the argument --output-class-names will simplify the predictions to return only the most likely animal in each video: $ zamba predict --data-dir example_vids/ --output-class-names $ cat zamba_predictions.csv blank.mp4,blank chimp.mp4,chimpanzee_bonobo eleph.mp4,elephant leopard.mp4,leopard There are three pretrained models that ship with zamba : time_distributed , slowfast , and european . Which model you should use depends on your priorities and geography (see the Available Models page for more details). By default zamba will use the time_distributed model. Add the --model argument to specify one of other options: $ zamba predict --data-dir example_vids/ --model slowfast Training a model \u00b6 You can continue training one of the models that ships with zamba by either: Finetuning with additional labeled videos where the species are included in the list of zamba class labels Finetuning with labeled videos that include new species In either case, the commands for training are the same. Say that we have labels for the videos in the example_vids folder saved in example_labels.csv . To train a model, run: $ zamba train --data-dir example_vids/ --labels example_labels.csv The labels file must have columns for both filepath and label. The filepath column should contain either absolute paths or paths relative to the data-dir . Optionally, there can also be columns for split ( train , val , or holdout ) and site . Let's print the example labels: $ cat example_labels.csv filepath,label blank.MP4,blank chimp.MP4,chimpanzee_bonobo eleph.MP4,elephant leopard.MP4,leopard By default, the trained model and additional training output will be saved to a version_n folder in the current working directory. For example, $ zamba train --data-dir example_vids/ --labels example_labels.csv $ ls version_0/ hparams.yaml time_distributed.ckpt train_configuration.yamml val_metrics.json ... Downloading model weights \u00b6 zamba needs to download the \"weights\" files for the models it uses to make predictions. On first run, it will download ~200-500 MB of files with these weights depending which model you choose. Once a model's weights are downloaded, zamba will use the local version and will not need to perform this download again. If you are not in the United States, we recommend running the above command with the additional flag either --weight_download_region eu or --weight_download_region asia depending on your location. The closer you are to the server, the faster the downloads will be. Getting help \u00b6 Once zamba is installed, you can see more details of each function with --help . To get help with zamba predict : Usage: zamba predict [OPTIONS] Identify species in a video. This is a command line interface for prediction on camera trap footage. Given a path to camera trap footage, the predict function use a deep learning model to predict the presence or absense of a variety of species of common interest to wildlife researchers working with camera trap data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --filepaths PATH Path to csv containing `filepath` column with videos. --model [time_distributed|slowfast|european] Model to use for inference. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for inference. If provided, model is not required. --gpus INTEGER Number of GPUs to use for inference. If not specifiied, will use all GPUs found on machine. --batch-size INTEGER Batch size to use for training. --save / --no-save Whether to save out predictions. If you want to specify the output directory, use save_dir instead. --save-dir PATH An optional directory in which to save the model predictions and configuration yaml. Defaults to the current working directory if save is True. --dry-run / --no-dry-run Runs one batch of inference to check for bugs. --config PATH Specify options using yaml configuration file instead of through command line options. --proba-threshold FLOAT Probability threshold for classification between 0 and 1. If specified binary predictions are returned with 1 being greater than the threshold, 0 being less than or equal to. If not specified, probabilities between 0 and 1 are returned. --output-class-names / --no-output-class-names If True, we just return a video and the name of the most likely class. If False, we return a probability or indicator (depending on --proba_threshold) for every possible class. --num-workers INTEGER Number of subprocesses to use for data loading. --weight-download-region [us|eu|asia] Server region for downloading weights. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to inference. Only use if you're very confident all your videos can be loaded. -o, --overwrite Overwrite outputs in the save directory if they exist. -y, --yes Skip confirmation of configuration and proceed right to prediction. --help Show this message and exit. To get help with zamba train : $ zamba train --help Usage: zamba train [OPTIONS] Train a model on your labeled data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --labels PATH Path to csv containing video labels. --model [time_distributed|slowfast|european] Model to train. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for training. If provided, model is not required. --config PATH Specify options using yaml configuration file instead of through command line options. --batch-size INTEGER Batch size to use for training. --gpus INTEGER Number of GPUs to use for training. If not specifiied, will use all GPUs found on machine. --dry-run / --no-dry-run Runs one batch of train and validation to check for bugs. --save-dir PATH An optional directory in which to save the model checkpoint and configuration file. If not specified, will save to a `version_n` folder in your working directory. --num-workers INTEGER Number of subprocesses to use for data loading. --weight-download-region [us|eu|asia] Server region for downloading weights. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to training. Only use if you're very confident all your videos can be loaded. -y, --yes Skip confirmation of configuration and proceed right to training. --help Show this message and exit.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This section assumes you have successfully installed zamba and are ready to train a model or identify species in your videos! zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. To perform inference, you simply need to run zamba predict followed by a set of arguments that let zamba know where your videos are located, which model you want to use, and where to save your output. To train a model, you can similarly run zamba train and specify your labels. The following sections provide details about these separate modules. There are two ways to interact with the zamba package: Use zamba as a command line interface tool. This page provides an overview of how to use the CLI. Import zamba in Python and use it as a Python package. For instructions on using the Python package, see the user tutorial that corresponds to your use case. Installation is the same for both the command line interface tool and the Python package. All of the commands on this page should be run at the command line. On macOS, this can be done in the terminal (\u2318+space, \"Terminal\"). On Windows, this can be done in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt).","title":"Quickstart"},{"location":"quickstart/#how-do-i-organize-my-videos-for-zamba","text":"You can specify the path to a directory of videos or specify a list of filepaths in a .csv file. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. For example, say we have a directory of videos called example_vids that we want to generate predictions for using zamba . Let's list the videos: $ ls example_vids/ blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 Here are some screenshots from those videos: blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 In this example, the videos have meaningful names so that we can easily compare the predictions made by zamba . In practice, your videos will probably be named something much less useful!","title":"How do I organize my videos for zamba?"},{"location":"quickstart/#generating-predictions","text":"To generate and save predictions for your videos using the default settings, run: $ zamba predict --data-dir example_vids/ zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) is the probability that animal j is present in video i . Comprehensive predictions are helpful when a single video contains multiple species. Predictions will be saved to zamba_predictions.csv in the current working directory by default. You can save out predictions to a different folder using the --save-dir argument. Adding the argument --output-class-names will simplify the predictions to return only the most likely animal in each video: $ zamba predict --data-dir example_vids/ --output-class-names $ cat zamba_predictions.csv blank.mp4,blank chimp.mp4,chimpanzee_bonobo eleph.mp4,elephant leopard.mp4,leopard There are three pretrained models that ship with zamba : time_distributed , slowfast , and european . Which model you should use depends on your priorities and geography (see the Available Models page for more details). By default zamba will use the time_distributed model. Add the --model argument to specify one of other options: $ zamba predict --data-dir example_vids/ --model slowfast","title":"Generating predictions"},{"location":"quickstart/#training-a-model","text":"You can continue training one of the models that ships with zamba by either: Finetuning with additional labeled videos where the species are included in the list of zamba class labels Finetuning with labeled videos that include new species In either case, the commands for training are the same. Say that we have labels for the videos in the example_vids folder saved in example_labels.csv . To train a model, run: $ zamba train --data-dir example_vids/ --labels example_labels.csv The labels file must have columns for both filepath and label. The filepath column should contain either absolute paths or paths relative to the data-dir . Optionally, there can also be columns for split ( train , val , or holdout ) and site . Let's print the example labels: $ cat example_labels.csv filepath,label blank.MP4,blank chimp.MP4,chimpanzee_bonobo eleph.MP4,elephant leopard.MP4,leopard By default, the trained model and additional training output will be saved to a version_n folder in the current working directory. For example, $ zamba train --data-dir example_vids/ --labels example_labels.csv $ ls version_0/ hparams.yaml time_distributed.ckpt train_configuration.yamml val_metrics.json ...","title":"Training a model"},{"location":"quickstart/#downloading-model-weights","text":"zamba needs to download the \"weights\" files for the models it uses to make predictions. On first run, it will download ~200-500 MB of files with these weights depending which model you choose. Once a model's weights are downloaded, zamba will use the local version and will not need to perform this download again. If you are not in the United States, we recommend running the above command with the additional flag either --weight_download_region eu or --weight_download_region asia depending on your location. The closer you are to the server, the faster the downloads will be.","title":"Downloading model weights"},{"location":"quickstart/#getting-help","text":"Once zamba is installed, you can see more details of each function with --help . To get help with zamba predict : Usage: zamba predict [OPTIONS] Identify species in a video. This is a command line interface for prediction on camera trap footage. Given a path to camera trap footage, the predict function use a deep learning model to predict the presence or absense of a variety of species of common interest to wildlife researchers working with camera trap data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --filepaths PATH Path to csv containing `filepath` column with videos. --model [time_distributed|slowfast|european] Model to use for inference. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for inference. If provided, model is not required. --gpus INTEGER Number of GPUs to use for inference. If not specifiied, will use all GPUs found on machine. --batch-size INTEGER Batch size to use for training. --save / --no-save Whether to save out predictions. If you want to specify the output directory, use save_dir instead. --save-dir PATH An optional directory in which to save the model predictions and configuration yaml. Defaults to the current working directory if save is True. --dry-run / --no-dry-run Runs one batch of inference to check for bugs. --config PATH Specify options using yaml configuration file instead of through command line options. --proba-threshold FLOAT Probability threshold for classification between 0 and 1. If specified binary predictions are returned with 1 being greater than the threshold, 0 being less than or equal to. If not specified, probabilities between 0 and 1 are returned. --output-class-names / --no-output-class-names If True, we just return a video and the name of the most likely class. If False, we return a probability or indicator (depending on --proba_threshold) for every possible class. --num-workers INTEGER Number of subprocesses to use for data loading. --weight-download-region [us|eu|asia] Server region for downloading weights. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to inference. Only use if you're very confident all your videos can be loaded. -o, --overwrite Overwrite outputs in the save directory if they exist. -y, --yes Skip confirmation of configuration and proceed right to prediction. --help Show this message and exit. To get help with zamba train : $ zamba train --help Usage: zamba train [OPTIONS] Train a model on your labeled data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --labels PATH Path to csv containing video labels. --model [time_distributed|slowfast|european] Model to train. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for training. If provided, model is not required. --config PATH Specify options using yaml configuration file instead of through command line options. --batch-size INTEGER Batch size to use for training. --gpus INTEGER Number of GPUs to use for training. If not specifiied, will use all GPUs found on machine. --dry-run / --no-dry-run Runs one batch of train and validation to check for bugs. --save-dir PATH An optional directory in which to save the model checkpoint and configuration file. If not specified, will save to a `version_n` folder in your working directory. --num-workers INTEGER Number of subprocesses to use for data loading. --weight-download-region [us|eu|asia] Server region for downloading weights. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to training. Only use if you're very confident all your videos can be loaded. -y, --yes Skip confirmation of configuration and proceed right to training. --help Show this message and exit.","title":"Getting help"},{"location":"train-tutorial/","text":"User tutorial: Training a model on labaled videos \u00b6 This section walks through how to train a model using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have labeled videos that you want to use to train or finetune a model zamba can run two types of model training: Finetuning a model with labels that are a subset of the possible zamba labels Finetuning a model to predict an entirely new set of labels The process is the same for both cases. Basic usage: command line interface \u00b6 Say that we want to finetune the time_distributed model based on the videos in example_vids and the labels in example_labels.csv . Minimum example for training in the command line: $ zamba train --data-dir example_vids/ --labels example_labels.csv Required arguments \u00b6 To run zamba train in the command line, you must specify both --data-directory and --labels . --data-dir PATH : Path to the folder containing your labeled videos. zamba will generate predictions for videos in the top level directory and in any nested folders. --labels PATH : Path to a CSV containing the video labels to use as ground truth during training. There must be columns for both filepath and label. Filepaths should be either absolute paths or relative to data-dir . Optionally, there can also be columns for split ( train , val , or holdout ) and site . If your labels file does not have a column for split , you can alternately use the split_proportions argument. $ cat example_labels.csv filepath,label blank.MP4,blank chimp.MP4,chimpanzee_bonobo eleph.MP4,elephant leopard.MP4,leopard Basic usage: Python package \u00b6 Say that we want to finetune the time_distributed model based on the videos in example_vids and the labels in example_labels.csv . Minimum example for training using the Python package: from zamba.models.model_manager import train_model from zamba.models.config import TrainConfig train_config = TrainConfig ( data_dir = \"example_vids/\" , labels = \"example_labels.csv\" ) train_model ( train_config = train_config ) The only two arguments that can be passed to train_model are train_config and (optionally) video_loader_config . The first step is to instantiate TrainConfig . Optionally, you can also specify video loading arguments by instantiating and passing in VideoLoaderConfig . Required arguments \u00b6 To run train_model in Python, you must specify both data_dir and labels when TrainConfig is instantiated. data_dir (DirectoryPath) : Path to the folder containing your videos. labels (FilePath or pd.DataFrame) : Either the path to a CSV file with labels for training, or a dataframe of the training labels. There must be columns for filename and label . For detailed explanations of all possible configuration arguments, see All Configuration Options . Default behavior \u00b6 By default, the time_distributed model will be used as a starting point. You can specify where the outputs should be saved with --save-dir . If no save directory is specified, zamba will write out incremental version_n folders to your current working directory. For example, a model finetuned from the provided time_distributed model (the default) will be saved in version_0 . version_0 contains: train_configuration.yaml : The full model configuration used to generate the given model, including video_loader_config and train_config . To continue training using the same configuration, or to train another model using the same configuration, you can pass in train_configurations.yaml (see Specifying Model Configurations with a YAML File ) along with the labels filepath. hparams.yaml : Model hyperparameters. These are included in the checkpoint file as well. time_distributed.ckpt : Model checkpoint. You can continue training from this checkpoint by passing it to zamba train with the --checkpoint flag: $ zamba train --checkpoint version_0/time_distributed.ckpt --data-dir example_vids/ --labels example_labels.csv events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 : TensorBoard logs. You can view these with tensorboard: $ tensorboard --logdir version_0/ val_metrics.json : The model's performance on the validation subset test_metrics.json : The model's performance on the test (holdout) subset splits.csv : Which files were used for training, validation, and as a holdout set. If split is specified in the labels file passed to training, splits.csv will not be saved out. Step-by-step tutorial \u00b6 1. Specify the path to your videos \u00b6 Save all of your videos in a folder. They can be in nested directories within the folder. Your videos should all be saved in formats that are suppored by FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. By default, zamba will look for files with the following suffixes: .avi , .mp4 , .asf . To use other video suffixes that are supported by FFmpeg, set your VIDEO_SUFFIXES environment variable. Add the path to your video folder with --data-dir . For example, if your videos are in a folder called example_vids , add --data-dir example_vids/ to your command. CLI $ zamba train --data-dir example_vids Python from zamba.models.config import TrainConfig from zamba.models.model_manager import train_model train_config = TrainConfig ( data_dir = 'example_vids/' ) train_model ( train_config = train_config ) Note that the above will not run yet because labels are not specified. The more training data you have, the better the resulting model will be. We recommend having a minimum of 100 videos per species . Having an imbalanced dataset - for example, where most of the videos are blank - is okay as long as there are enough examples of each individual species. 2. Specify your labels \u00b6 Your labels should be saved in a .csv file with columns for filepath and label. For example: $ cat example_labels.csv filepath,label eleph.MP4,elephant leopard.MP4,leopard blank.MP4,blank chimp.MP4,chimpanzee_bonobo Add the path to your labels with --labels . For example, if your videos are in a folder called example_vids and your labels are saved in example_labels.csv : CLI $ zamba train --data-dir example_vids/ --labels example_labels.csv Python In Python, the labels are passed in when TrainConfig is instantiated. The Python package allows you to pass in labels as either a file path or a pandas dataframe: labels_dataframe = pd . read_csv ( 'example_labels.csv' , index_col = 'filepath' ) train_config = TrainConfig ( data_dir = 'example_vids/' , labels = labels_dataframe ) train_model ( train_config = train_config ) Labels zamba has seen before \u00b6 Your labels may be included in the list of zamba class labels that the provided models are trained to predict. If so, the relevant model that ships with zamba will essentially be used as a checkpoint, and model training will resume from that checkpoint. Completely new labels \u00b6 You can also train a model to predict completely new labels - the world is your oyster! (We'd love to see a model trained to predict oysters.) If this is the case, the model architecture will replace the final neural network layer with a new head that predicts your labels instead of those that ship with zamba . 3. Choose a model for training \u00b6 Any of the models that ship with zamba can be trained. If you're training on entirely new species or new ecologies, we recommend starting with the time_distributed model as this model is less computationally intensive than the slowfast model . However, if you're tuning a model to a subset of species (e.g. a european_beaver or blank model), use the model that was trained on data that is most similar to your new data. Add the model name to your command with --model . The time_distributed model will be used if no model is specified. For example, if you want to continue training the european model based on the videos in example_euro_vids and the labels in example_euro_labels.csv : CLI $ zamba train --data-dir example_euro_vids/ --labels example_euro_labels.csv --model european Python train_config = TrainConfig ( data_dir = \"example_euro_vids/\" , labels = \"example_euro_labels.csv\" , model_name = \"european\" , ) train_model ( train_config = train_config ) 4. Specify any additional parameters \u00b6 And there's so much more! You can also do things like specify your region for faster model download ( --weight-download-region ), start training from a saved model checkpoint ( --checkpoint ), or specify a different path where your model should be saved ( --save-dir ). To read about a few common considerations, see the Guide to Common Optional Parameters page. 5. Test your configuration with a dry run \u00b6 Before kicking off the full model training, we recommend testing your code with a \"dry run\". This will run one training and validation batch for one epoch to quickly detect any bugs. See the Debugging page for details.","title":"Training a model on labeled videos"},{"location":"train-tutorial/#user-tutorial-training-a-model-on-labaled-videos","text":"This section walks through how to train a model using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have labeled videos that you want to use to train or finetune a model zamba can run two types of model training: Finetuning a model with labels that are a subset of the possible zamba labels Finetuning a model to predict an entirely new set of labels The process is the same for both cases.","title":"User tutorial: Training a model on labaled videos"},{"location":"train-tutorial/#basic-usage-command-line-interface","text":"Say that we want to finetune the time_distributed model based on the videos in example_vids and the labels in example_labels.csv . Minimum example for training in the command line: $ zamba train --data-dir example_vids/ --labels example_labels.csv","title":"Basic usage: command line interface"},{"location":"train-tutorial/#basic-usage-python-package","text":"Say that we want to finetune the time_distributed model based on the videos in example_vids and the labels in example_labels.csv . Minimum example for training using the Python package: from zamba.models.model_manager import train_model from zamba.models.config import TrainConfig train_config = TrainConfig ( data_dir = \"example_vids/\" , labels = \"example_labels.csv\" ) train_model ( train_config = train_config ) The only two arguments that can be passed to train_model are train_config and (optionally) video_loader_config . The first step is to instantiate TrainConfig . Optionally, you can also specify video loading arguments by instantiating and passing in VideoLoaderConfig .","title":"Basic usage: Python package"},{"location":"train-tutorial/#default-behavior","text":"By default, the time_distributed model will be used as a starting point. You can specify where the outputs should be saved with --save-dir . If no save directory is specified, zamba will write out incremental version_n folders to your current working directory. For example, a model finetuned from the provided time_distributed model (the default) will be saved in version_0 . version_0 contains: train_configuration.yaml : The full model configuration used to generate the given model, including video_loader_config and train_config . To continue training using the same configuration, or to train another model using the same configuration, you can pass in train_configurations.yaml (see Specifying Model Configurations with a YAML File ) along with the labels filepath. hparams.yaml : Model hyperparameters. These are included in the checkpoint file as well. time_distributed.ckpt : Model checkpoint. You can continue training from this checkpoint by passing it to zamba train with the --checkpoint flag: $ zamba train --checkpoint version_0/time_distributed.ckpt --data-dir example_vids/ --labels example_labels.csv events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 : TensorBoard logs. You can view these with tensorboard: $ tensorboard --logdir version_0/ val_metrics.json : The model's performance on the validation subset test_metrics.json : The model's performance on the test (holdout) subset splits.csv : Which files were used for training, validation, and as a holdout set. If split is specified in the labels file passed to training, splits.csv will not be saved out.","title":"Default behavior"},{"location":"train-tutorial/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"yaml-config/","text":"Using YAML configuration files \u00b6 In both the command line and the Python module, options for video loading, training, and prediction can be set by passing a YAML file instead of passing arguments directly. YAML files ( .yml or .yaml ) are commonly used to serialize data in an easily readable way. The basic structure of a YAML model configuration is: $ cat basic_config.yaml video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 # other video loading parameters train_config : model_name : time_distributed data_dir : example_vids/ labels : example_labels.csv # other training parameters, eg. batch_size predict_config : model_name : time_distributed data_directoty : example_vids/ # other training parameters, eg. batch_size For example, the configuration below will predict labels for the videos in example_vids using the time_distributed model. When videos are loaded, each will be resized to 240x426 pixels and 16 frames will be selected: video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 predict_config : model_name : time_distributed data_directoty : example_vids/ Required arguments \u00b6 Either predict_config or train_config is required, based on whether you will be running inference or training a model. See All Configuration Options for a full list of what can be specified under each class. To run inference, data_dir and/or filepaths must be specified. To train a model, labels must be specified. In video_loader_config , you must specify at least model_input_height , model_input_width , and total_frames . While this is the minimum required, we strongly recommend being intentional in your choice of frame selection method. total_frames by itself will just take the first n frames. For a full list of frame selection methods, see the section on Video loading arguments . For time_distributed or european , total_frames must be 16 For slowfast , total_frames must be 32 Command line interface \u00b6 A YAML configuration file can be passed to the command line interface with the --config argument. For example, say the example configuration above is saved as example_config.yaml . To run prediction: $ zamba predict --config example_config.yaml Only some of the possible parameters can be passed directly as arguments to the command line. Those not listed in zamba predict --help or zamba train --help must be passed in a YAML file (see the Quickstart guide for details). Python package \u00b6 The main API for zamba is the ModelManager class that can be accessed with: from zamba.models.manager import ModelManager The ModelManager class is used by zamba \u2019s command line interface to handle preprocessing the filenames, loading the videos, training the model, performing inference, and saving predictions. Therefore any functionality available to the command line interface is accessible via the ModelManager class. To instantiate the ModelManager based on a configuration file saved at test_config.yaml : >>> manager = ModelManager . from_yaml ( 'test_config.yaml' ) We can now run inference or model training without specifying any additional parameters, because they are already associated with our instance of the ModelManager class. To run inference or training: manager . predict () # inference manager . train () # training In our user tutorials, we refer to train_model and predict_model functions. The ModelManager class calls these same functions behind the scenes when .predict() or .train() is run. Default configurations \u00b6 In the command line, the default configuration for each model is passed in using a specified YAML file that ships with zamba . You can see the default configuration YAML files on Github in the config.yaml file within each model's folder. For example, the default configuration for the time_distributed model is: train_config : scheduler_config : scheduler : MultiStepLR scheduler_params : gamma : 0.5 milestones : - 3 verbose : true model_name : time_distributed backbone_finetune_config : backbone_initial_ratio_lr : 0.01 multiplier : 1 pre_train_bn : true train_bn : false unfreeze_backbone_at_epoch : 3 verbose : true early_stopping_config : patience : 5 video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 4 total_frames : 16 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 16 predict_config : model_name : time_distributed public_checkpoint : time_distributed_9e710aa8c92d25190a64b3b04b9122bdcb456982.ckpt Templates \u00b6 To make modifying existing mod easier, we've set up the official models as templates in the templates folder . Just fill in your data directory and labels, make any desired tweaks to the model config, and then kick off some training . Happy modeling!","title":"Using YAML configuration files"},{"location":"yaml-config/#using-yaml-configuration-files","text":"In both the command line and the Python module, options for video loading, training, and prediction can be set by passing a YAML file instead of passing arguments directly. YAML files ( .yml or .yaml ) are commonly used to serialize data in an easily readable way. The basic structure of a YAML model configuration is: $ cat basic_config.yaml video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 # other video loading parameters train_config : model_name : time_distributed data_dir : example_vids/ labels : example_labels.csv # other training parameters, eg. batch_size predict_config : model_name : time_distributed data_directoty : example_vids/ # other training parameters, eg. batch_size For example, the configuration below will predict labels for the videos in example_vids using the time_distributed model. When videos are loaded, each will be resized to 240x426 pixels and 16 frames will be selected: video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 predict_config : model_name : time_distributed data_directoty : example_vids/","title":"Using YAML configuration files"},{"location":"yaml-config/#required-arguments","text":"Either predict_config or train_config is required, based on whether you will be running inference or training a model. See All Configuration Options for a full list of what can be specified under each class. To run inference, data_dir and/or filepaths must be specified. To train a model, labels must be specified. In video_loader_config , you must specify at least model_input_height , model_input_width , and total_frames . While this is the minimum required, we strongly recommend being intentional in your choice of frame selection method. total_frames by itself will just take the first n frames. For a full list of frame selection methods, see the section on Video loading arguments . For time_distributed or european , total_frames must be 16 For slowfast , total_frames must be 32","title":"Required arguments"},{"location":"yaml-config/#command-line-interface","text":"A YAML configuration file can be passed to the command line interface with the --config argument. For example, say the example configuration above is saved as example_config.yaml . To run prediction: $ zamba predict --config example_config.yaml Only some of the possible parameters can be passed directly as arguments to the command line. Those not listed in zamba predict --help or zamba train --help must be passed in a YAML file (see the Quickstart guide for details).","title":"Command line interface"},{"location":"yaml-config/#python-package","text":"The main API for zamba is the ModelManager class that can be accessed with: from zamba.models.manager import ModelManager The ModelManager class is used by zamba \u2019s command line interface to handle preprocessing the filenames, loading the videos, training the model, performing inference, and saving predictions. Therefore any functionality available to the command line interface is accessible via the ModelManager class. To instantiate the ModelManager based on a configuration file saved at test_config.yaml : >>> manager = ModelManager . from_yaml ( 'test_config.yaml' ) We can now run inference or model training without specifying any additional parameters, because they are already associated with our instance of the ModelManager class. To run inference or training: manager . predict () # inference manager . train () # training In our user tutorials, we refer to train_model and predict_model functions. The ModelManager class calls these same functions behind the scenes when .predict() or .train() is run.","title":"Python package"},{"location":"yaml-config/#default-configurations","text":"In the command line, the default configuration for each model is passed in using a specified YAML file that ships with zamba . You can see the default configuration YAML files on Github in the config.yaml file within each model's folder. For example, the default configuration for the time_distributed model is: train_config : scheduler_config : scheduler : MultiStepLR scheduler_params : gamma : 0.5 milestones : - 3 verbose : true model_name : time_distributed backbone_finetune_config : backbone_initial_ratio_lr : 0.01 multiplier : 1 pre_train_bn : true train_bn : false unfreeze_backbone_at_epoch : 3 verbose : true early_stopping_config : patience : 5 video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 4 total_frames : 16 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 16 predict_config : model_name : time_distributed public_checkpoint : time_distributed_9e710aa8c92d25190a64b3b04b9122bdcb456982.ckpt","title":"Default configurations"},{"location":"yaml-config/#templates","text":"To make modifying existing mod easier, we've set up the official models as templates in the templates folder . Just fill in your data directory and labels, make any desired tweaks to the model config, and then kick off some training . Happy modeling!","title":"Templates"},{"location":"api-reference/data-metadata/","text":"zamba.data.metadata \u00b6 Functions \u00b6 create_site_specific_splits ( site : Series , proportions : Dict [ str , int ], random_state : Union [ int , numpy . random . mtrand . RandomState ] = 989 ) \u00b6 Splits sites into distinct groups whose sizes roughly matching the given proportions. Null sites are randomly assigned to groups using the provided proportions. Parameters: Name Type Description Default site pd.Series A series of sites, one element per observation, required proportions dict A dict whose keys are the resulting groups and whose values are the rough proportion of data in each group. required seed int Seed for random split of null sites. required Examples: Split data into groups where each site is in one and only one group with roughly 50-25-25 train-val-holdout proportions. >>> create_site_specific_splits ( site , proportions = { \"train\" : 2 , \"val\" : 1 , \"holdout\" : 1 }) Returns: Type Description pd.Series A series containing the resulting split, one element per observation. Source code in zamba/data/metadata.py def create_site_specific_splits ( site : pd . Series , proportions : Dict [ str , int ], random_state : Optional [ Union [ int , np . random . mtrand . RandomState ]] = 989 , ): \"\"\"Splits sites into distinct groups whose sizes roughly matching the given proportions. Null sites are randomly assigned to groups using the provided proportions. Args: site (pd.Series): A series of sites, one element per observation, proportions (dict): A dict whose keys are the resulting groups and whose values are the rough proportion of data in each group. seed (int): Seed for random split of null sites. Example: Split data into groups where each site is in one and only one group with roughly 50-25-25 train-val-holdout proportions. >>> create_site_specific_splits(site, proportions={\"train\": 2, \"val\": 1, \"holdout\": 1}) Returns: pd.Series: A series containing the resulting split, one element per observation. \"\"\" assignments = {} sites = site . value_counts ( dropna = True ) . sort_values ( ascending = False ) . index n_subgroups = sum ( proportions . values ()) for i , subset in enumerate ( roundrobin ( * ([ subset ] * proportions [ subset ] for subset in proportions )) ): for group in sites [ i :: n_subgroups ]: assignments [ group ] = subset # Divide null sites among the groups null_sites = site . isnull () if null_sites . sum () > 0 : logger . debug ( f \" { null_sites . sum () : , } null sites randomly assigned to groups.\" ) null_groups = [] for group , group_proportion in proportions . items (): null_group = f \" { group } - { uuid4 () } \" null_groups . append ( null_group ) assignments [ null_group ] = group rng = ( np . random . RandomState ( random_state ) if isinstance ( random_state , int ) else random_state ) site = site . copy () site . loc [ null_sites ] = rng . choice ( null_groups , p = np . asarray ( list ( proportions . values ())) / sum ( proportions . values ()), size = null_sites . sum (), replace = True , ) return site . replace ( assignments ) one_hot_to_labels ( one_hot : DataFrame , column_prefix : Optional [ str ] = 'species_' ) -> DataFrame \u00b6 Source code in zamba/data/metadata.py def one_hot_to_labels ( one_hot : pd . DataFrame , column_prefix : Optional [ str ] = r \"species_\" ) -> pd . DataFrame : if column_prefix : one_hot = one_hot . filter ( regex = column_prefix ) # remove prefix one_hot . columns = [ c . split ( column_prefix , 1 )[ 1 ] for c in one_hot . columns ] one_hot . index = one_hot . index . rename ( \"filepath\" ) one_hot . columns = one_hot . columns . rename ( \"label\" ) labels = ( one_hot == 1 ) . stack () labels = labels [ labels ] return labels . reset_index () . drop ( 0 , axis = 1 ) roundrobin ( * iterables ) \u00b6 roundrobin('ABC', 'D', 'EF') --> A D E B F C Source code in zamba/data/metadata.py def roundrobin ( * iterables ): \"roundrobin('ABC', 'D', 'EF') --> A D E B F C\" # From https://docs.python.org/3/library/itertools.html#recipes # Recipe credited to George Sakkis num_active = len ( iterables ) nexts = itertools . cycle ( iter ( it ) . __next__ for it in iterables ) while num_active : try : for next in nexts : yield next () except StopIteration : # Remove the iterator we just exhausted from the cycle. num_active -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , num_active ))","title":"zamba.data.metadata"},{"location":"api-reference/data-metadata/#zambadatametadata","text":"","title":"zamba.data.metadata"},{"location":"api-reference/data-metadata/#zamba.data.metadata-functions","text":"","title":"Functions"},{"location":"api-reference/data-video/","text":"zamba.data.video \u00b6 Classes \u00b6 VideoLoaderConfig ( BaseModel ) pydantic-model \u00b6 Configuration for load_video_frames. Parameters: Name Type Description Default crop_bottom_pixels int Number of pixels to crop from the bottom of the video (prior to resizing to video_height ). required i_frames bool Only load the I-Frames. See https://en.wikipedia.org/wiki/Video_compression_picture_types#Intra-coded_(I) frames/slices (key_frames) required scene_threshold float Only load frames that correspond to scene changes. See http://www.ffmpeg.org/ffmpeg-filters.html#select_002c-aselect required megadetector_lite_config MegadetectorLiteYoloXConfig Configuration of MegadetectorLiteYoloX frame selection model. required frame_selection_height int Resize the video to this height in pixels, prior to frame selection. If None, the full size video will be used for frame selection. Using full size images (setting to None) is recommended for MegadetectorLite, especially if your species of interest are smaller. required frame_selection_width int Resize the video to this width in pixels, prior to frame selection. required total_frames int Number of frames that should ultimately be returned. required ensure_total_frames bool Selecting the number of frames by resampling may result in one more or fewer frames due to rounding. If True, ensure the requested number of frames is returned by either clipping or duplicating the final frame. Raises an error if no frames have been selected. Otherwise, return the array unchanged. required fps int Resample the video evenly from the entire duration to a specific number of frames per second. required early_bias bool Resamples to 24 fps and selects 16 frames biased toward the front (strategy used by competition winner). required frame_indices list(int) Select specific frame numbers. Note: frame selection is done after any resampling. required evenly_sample_total_frames bool Reach the total number of frames specified by evenly sampling from the duration of the video. Defaults to False. required pix_fmt str ffmpeg pixel format, defaults to 'rgb24' for RGB channels; can be changed to 'bgr24' for BGR. required model_input_height int After frame selection, resize the video to this height in pixels. required model_input_width int After frame selection, resize the video to this width in pixels. required cache_dir Path Cache directory where preprocessed videos will be saved upon first load. Alternatively, can be set with VIDEO_CACHE_DIR environment variable. Defaults to None, which means videos will not be cached. Provided there is enough space on your machine, it is highly encouraged to cache videos for training as this will speed up all subsequent epochs. If you are predicting on the same videos with the same video loader configuration, this will save time on future runs. required cleanup_cache bool Whether to delete the cache dir after training or predicting ends. Defaults to False. required cache_dir : Path pydantic-field \u00b6 cleanup_cache : bool pydantic-field \u00b6 crop_bottom_pixels : int pydantic-field \u00b6 early_bias : bool pydantic-field \u00b6 ensure_total_frames : bool pydantic-field \u00b6 evenly_sample_total_frames : bool pydantic-field \u00b6 fps : float pydantic-field \u00b6 frame_indices : List [ int ] pydantic-field \u00b6 frame_selection_height : int pydantic-field \u00b6 frame_selection_width : int pydantic-field \u00b6 i_frames : bool pydantic-field \u00b6 megadetector_lite_config : MegadetectorLiteYoloXConfig pydantic-field \u00b6 model_input_height : int pydantic-field \u00b6 model_input_width : int pydantic-field \u00b6 pix_fmt : str pydantic-field \u00b6 scene_threshold : float pydantic-field \u00b6 total_frames : int pydantic-field \u00b6 Config \u00b6 Methods \u00b6 check_early_bias_compatibility ( values ) classmethod \u00b6 Source code in zamba/data/video.py @root_validator ( skip_on_failure = True ) def check_early_bias_compatibility ( cls , values ): if values [ \"early_bias\" ] and ( values [ \"i_frames\" ] or values [ \"scene_threshold\" ] or values [ \"total_frames\" ] or values [ \"evenly_sample_total_frames\" ] or values [ \"fps\" ] ): raise ValueError ( f \"early_bias cannot be used with i_frames, scene_threshold, total_frames, evenly_sample_total_frames, or fps. Values provided are { values } .\" ) return values check_evenly_sample_total_frames_compatibility ( values ) classmethod \u00b6 Source code in zamba/data/video.py @root_validator ( skip_on_failure = True ) def check_evenly_sample_total_frames_compatibility ( cls , values ): if values [ \"evenly_sample_total_frames\" ] is True and values [ \"total_frames\" ] is None : raise ValueError ( f \"total_frames must be specified if evenly_sample_total_frames is used. Values provided are { values } .\" ) if values [ \"evenly_sample_total_frames\" ] and ( values [ \"scene_threshold\" ] or values [ \"i_frames\" ] or values [ \"fps\" ] or values [ \"early_bias\" ] ): raise ValueError ( f \"evenly_sample_total_frames cannot be used with scene_threshold, i_frames, fps, or early_bias. Values provided are { values } .\" ) return values check_fps_compatibility ( values ) classmethod \u00b6 Source code in zamba/data/video.py @root_validator ( skip_on_failure = True ) def check_fps_compatibility ( cls , values ): if values [ \"fps\" ] and ( values [ \"evenly_sample_total_frames\" ] or values [ \"i_frames\" ] or values [ \"scene_threshold\" ] ): raise ValueError ( f \"fps cannot be used with evenly_sample_total_frames, i_frames, or scene_threshold. Values provided are { values } .\" ) return values check_frame_indices_compatibility ( values ) classmethod \u00b6 Source code in zamba/data/video.py @root_validator ( skip_on_failure = True ) def check_frame_indices_compatibility ( cls , values ): if values [ \"frame_indices\" ] and ( values [ \"total_frames\" ] or values [ \"scene_threshold\" ] or values [ \"i_frames\" ] or values [ \"early_bias\" ] or values [ \"evenly_sample_total_frames\" ] ): raise ValueError ( f \"frame_indices cannot be used with total_frames, scene_threshold, i_frames, early_bias, or evenly_sample_total_frames. Values provided are { values } .\" ) return values check_height_and_width ( values ) classmethod \u00b6 Source code in zamba/data/video.py @root_validator ( skip_on_failure = True ) def check_height_and_width ( cls , values ): if ( values [ \"frame_selection_height\" ] is None ) ^ ( values [ \"frame_selection_width\" ] is None ): raise ValueError ( f \"Must provide both frame_selection_height and frame_selection_width or neither. Values provided are { values } .\" ) if ( values [ \"model_input_height\" ] is None ) ^ ( values [ \"model_input_width\" ] is None ): raise ValueError ( f \"Must provide both model_input_height and model_input_width or neither. Values provided are { values } .\" ) return values check_i_frame_compatibility ( values ) classmethod \u00b6 Source code in zamba/data/video.py @root_validator ( skip_on_failure = True ) def check_i_frame_compatibility ( cls , values ): if values [ \"scene_threshold\" ] and values [ \"i_frames\" ]: raise ValueError ( f \"i_frames cannot be used with scene_threshold. Values provided are { values } .\" ) return values check_megadetector_lite_compatibility ( values ) classmethod \u00b6 Source code in zamba/data/video.py @root_validator ( skip_on_failure = True ) def check_megadetector_lite_compatibility ( cls , values ): if values [ \"megadetector_lite_config\" ] and ( values [ \"early_bias\" ] or values [ \"evenly_sample_total_frames\" ] ): raise ValueError ( f \"megadetector_lite_config cannot be used with early_bias or evenly_sample_total_frames. Values provided are { values } .\" ) return values validate_total_frames ( values ) classmethod \u00b6 Source code in zamba/data/video.py @root_validator ( skip_on_failure = True ) def validate_total_frames ( cls , values ): if values [ \"megadetector_lite_config\" ] is not None : # set n frames for megadetector_lite_config if only specified by total_frames if values [ \"megadetector_lite_config\" ] . n_frames is None : values [ \"megadetector_lite_config\" ] . n_frames = values [ \"total_frames\" ] # set total frames if only specified in megadetector_lite_config if values [ \"total_frames\" ] is None : values [ \"total_frames\" ] = values [ \"megadetector_lite_config\" ] . n_frames return values validate_video_cache_dir ( cache_dir ) classmethod \u00b6 Set up cache directory for preprocessed videos. Config argument takes precedence over environment variable. Source code in zamba/data/video.py @validator ( \"cache_dir\" , always = True ) def validate_video_cache_dir ( cls , cache_dir ): \"\"\"Set up cache directory for preprocessed videos. Config argument takes precedence over environment variable. \"\"\" if cache_dir is None : cache_dir = os . getenv ( \"VIDEO_CACHE_DIR\" , None ) if cache_dir is not None : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) return cache_dir VideoMetadata ( BaseModel ) pydantic-model \u00b6 duration_s : float pydantic-field required \u00b6 fps : int pydantic-field required \u00b6 height : int pydantic-field required \u00b6 n_frames : int pydantic-field required \u00b6 width : int pydantic-field required \u00b6 Classes \u00b6 Config inherited \u00b6 Classes \u00b6 getter_dict ( Representation ) \u00b6 Hack to make object's smell just enough like dicts for validate_model. We can't inherit from Mapping[str, Any] because it upsets cython so we have to implement all methods ourselves. Methods \u00b6 get_field_info ( name : unicode ) -> Dict [ str , Any ] classmethod \u00b6 Get properties of FieldInfo from the fields property of the config class. json_dumps ( obj , * , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , default = None , sort_keys = False , ** kw ) \u00b6 Serialize obj to a JSON formatted str . If skipkeys is true then dict keys that are not basic types ( str , int , float , bool , None ) will be skipped instead of raising a TypeError . If ensure_ascii is false, then the return value can contain non-ASCII characters if they appear in strings contained in obj . Otherwise, all such characters are escaped in JSON strings. If check_circular is false, then the circular reference check for container types will be skipped and a circular reference will result in an OverflowError (or worse). If allow_nan is false, then it will be a ValueError to serialize out of range float values ( nan , inf , -inf ) in strict compliance of the JSON specification, instead of using the JavaScript equivalents ( NaN , Infinity , -Infinity ). If indent is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. If specified, separators should be an (item_separator, key_separator) tuple. The default is (', ', ': ') if indent is None and (',', ': ') otherwise. To get the most compact JSON representation, you should specify (',', ':') to eliminate whitespace. default(obj) is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If sort_keys is true (default: False ), then the output of dictionaries will be sorted by key. To use a custom JSONEncoder subclass (e.g. one that overrides the .default() method to serialize additional types), specify it with the cls kwarg; otherwise JSONEncoder is used. Source code in zamba/data/video.py def dumps ( obj , * , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , default = None , sort_keys = False , ** kw ): \"\"\"Serialize ``obj`` to a JSON formatted ``str``. If ``skipkeys`` is true then ``dict`` keys that are not basic types (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped instead of raising a ``TypeError``. If ``ensure_ascii`` is false, then the return value can contain non-ASCII characters if they appear in strings contained in ``obj``. Otherwise, all such characters are escaped in JSON strings. If ``check_circular`` is false, then the circular reference check for container types will be skipped and a circular reference will result in an ``OverflowError`` (or worse). If ``allow_nan`` is false, then it will be a ``ValueError`` to serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in strict compliance of the JSON specification, instead of using the JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). If ``indent`` is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact representation. If specified, ``separators`` should be an ``(item_separator, key_separator)`` tuple. The default is ``(', ', ': ')`` if *indent* is ``None`` and ``(',', ': ')`` otherwise. To get the most compact JSON representation, you should specify ``(',', ':')`` to eliminate whitespace. ``default(obj)`` is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If *sort_keys* is true (default: ``False``), then the output of dictionaries will be sorted by key. To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the ``.default()`` method to serialize additional types), specify it with the ``cls`` kwarg; otherwise ``JSONEncoder`` is used. \"\"\" # cached encoder if ( not skipkeys and ensure_ascii and check_circular and allow_nan and cls is None and indent is None and separators is None and default is None and not sort_keys and not kw ): return _default_encoder . encode ( obj ) if cls is None : cls = JSONEncoder return cls ( skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) . encode ( obj ) json_loads ( s , * , cls = None , object_hook = None , parse_float = None , parse_int = None , parse_constant = None , object_pairs_hook = None , ** kw ) \u00b6 Deserialize s (a str , bytes or bytearray instance containing a JSON document) to a Python object. object_hook is an optional function that will be called with the result of any object literal decode (a dict ). The return value of object_hook will be used instead of the dict . This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). object_pairs_hook is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of object_pairs_hook will be used instead of the dict . This feature can be used to implement custom decoders. If object_hook is also defined, the object_pairs_hook takes priority. parse_float , if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). parse_int , if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). parse_constant , if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom JSONDecoder subclass, specify it with the cls kwarg; otherwise JSONDecoder is used. The encoding argument is ignored and deprecated since Python 3.1. Source code in zamba/data/video.py def loads ( s , * , cls = None , object_hook = None , parse_float = None , parse_int = None , parse_constant = None , object_pairs_hook = None , ** kw ): \"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance containing a JSON document) to a Python object. ``object_hook`` is an optional function that will be called with the result of any object literal decode (a ``dict``). The return value of ``object_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). ``object_pairs_hook`` is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of ``object_pairs_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders. If ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority. ``parse_float``, if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). ``parse_int``, if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). ``parse_constant``, if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom ``JSONDecoder`` subclass, specify it with the ``cls`` kwarg; otherwise ``JSONDecoder`` is used. The ``encoding`` argument is ignored and deprecated since Python 3.1. \"\"\" if isinstance ( s , str ): if s . startswith ( ' \\ufeff ' ): raise JSONDecodeError ( \"Unexpected UTF-8 BOM (decode using utf-8-sig)\" , s , 0 ) else : if not isinstance ( s , ( bytes , bytearray )): raise TypeError ( f 'the JSON object must be str, bytes or bytearray, ' f 'not { s . __class__ . __name__ } ' ) s = s . decode ( detect_encoding ( s ), 'surrogatepass' ) if \"encoding\" in kw : import warnings warnings . warn ( \"'encoding' is ignored and deprecated. It will be removed in Python 3.9\" , DeprecationWarning , stacklevel = 2 ) del kw [ 'encoding' ] if ( cls is None and object_hook is None and parse_int is None and parse_float is None and parse_constant is None and object_pairs_hook is None and not kw ): return _default_decoder . decode ( s ) if cls is None : cls = JSONDecoder if object_hook is not None : kw [ 'object_hook' ] = object_hook if object_pairs_hook is not None : kw [ 'object_pairs_hook' ] = object_pairs_hook if parse_float is not None : kw [ 'parse_float' ] = parse_float if parse_int is not None : kw [ 'parse_int' ] = parse_int if parse_constant is not None : kw [ 'parse_constant' ] = parse_constant return cls ( ** kw ) . decode ( s ) prepare_field ( field : ModelField ) -> None classmethod \u00b6 Optional hook to check or modify fields during model creation. from_video ( path : PathLike ) classmethod \u00b6 Source code in zamba/data/video.py @classmethod def from_video ( cls , path : os . PathLike ): stream = get_video_stream ( path ) return cls ( height = int ( stream [ \"height\" ]), width = int ( stream [ \"width\" ]), n_frames = int ( stream [ \"nb_frames\" ]), duration_s = float ( stream [ \"duration\" ]), fps = int ( Fraction ( stream [ \"r_frame_rate\" ])), # reported, not average ) npy_cache \u00b6 __init__ ( self , cache_path : Optional [ pathlib . Path ] = None , cleanup : bool = False ) special \u00b6 Source code in zamba/data/video.py def __init__ ( self , cache_path : Optional [ Path ] = None , cleanup : bool = False ): self . cache_path = cache_path self . cleanup = cleanup Functions \u00b6 ensure_frame_number ( arr , total_frames : int ) \u00b6 Ensures the array contains the requested number of frames either by clipping frames from the end or dulpicating the last frame. Parameters: Name Type Description Default arr np.ndarray Array of video frames with shape (frames, height, width, channel). required total_frames int Desired number of frames in output array. required Source code in zamba/data/video.py def ensure_frame_number ( arr , total_frames : int ): \"\"\"Ensures the array contains the requested number of frames either by clipping frames from the end or dulpicating the last frame. Args: arr (np.ndarray): Array of video frames with shape (frames, height, width, channel). total_frames (int): Desired number of frames in output array. \"\"\" if ( total_frames is None ) or ( arr . shape [ 0 ] == total_frames ): return arr elif arr . shape [ 0 ] == 0 : logger . warning ( \"No frames selected. Returning an array in the desired shape with all zeros.\" ) return np . zeros (( total_frames , arr . shape [ 1 ], arr . shape [ 2 ], arr . shape [ 3 ]), dtype = \"int\" ) elif arr . shape [ 0 ] > total_frames : logger . info ( f \"Clipping { arr . shape [ 0 ] - total_frames } frames \" f \"(original: { arr . shape [ 0 ] } , requested: { total_frames } ).\" ) return arr [: total_frames ] elif arr . shape [ 0 ] < total_frames : logger . info ( f \"Duplicating last frame { total_frames - arr . shape [ 0 ] } times \" f \"(original: { arr . shape [ 0 ] } , requested: { total_frames } ).\" ) return np . concatenate ( [ arr , np . tile ( arr [ - 1 ], ( total_frames - arr . shape [ 0 ], 1 , 1 , 1 ))], axis = 0 ) ffprobe ( path : PathLike ) -> Series \u00b6 Source code in zamba/data/video.py def ffprobe ( path : os . PathLike ) -> pd . Series : def flatten_json ( j , name = \"\" ): for k in j : if isinstance ( j [ k ], dict ): yield from flatten_json ( j [ k ], f \" { name } . { k } \" ) elif isinstance ( j [ k ], list ): for i in range ( len ( j [ k ])): yield from flatten_json ( j [ k ][ i ], f \" { name } . { k } [ { i } ]\" ) else : yield { f \" { name } . { k } \" . strip ( \".\" ): j [ k ]} output = subprocess . check_output ( [ \"ffprobe\" , \"-v\" , \"quiet\" , \"-show_entries\" , \"stream:format\" , \"-select_streams\" , \"v\" , \"-of\" , \"json\" , path , ] ) output = json . loads ( output ) result = reduce ( lambda a , b : { ** a , ** b }, flatten_json ( output )) return pd . Series ( result ) get_frame_time_estimates ( path : PathLike ) \u00b6 Source code in zamba/data/video.py def get_frame_time_estimates ( path : os . PathLike ): probe = ffmpeg . probe ( str ( path ), show_entries = \"frame=best_effort_timestamp_time\" ) return [ float ( x [ \"best_effort_timestamp_time\" ]) for x in probe [ \"frames\" ]] get_video_stream ( path : Union [ os . PathLike , cloudpathlib . s3 . s3path . S3Path ]) -> dict \u00b6 Source code in zamba/data/video.py def get_video_stream ( path : Union [ os . PathLike , S3Path ]) -> dict : try : probe = ffmpeg . probe ( str ( path )) except ffmpeg . Error as exc : raise ZambaFfmpegException ( exc . stderr ) return next (( stream for stream in probe [ \"streams\" ] if stream [ \"codec_type\" ] == \"video\" ), None ) load_video_frames ( filepath : PathLike , config : Optional [ zamba . data . video . VideoLoaderConfig ] = None , ** kwargs ) \u00b6 Loads frames from videos using fast ffmpeg commands. Parameters: Name Type Description Default filepath os.PathLike Path to the video. required config VideoLoaderConfig Configuration for video loading. None **kwargs Optionally, arguments for VideoLoaderConfig can be passed in directly. {} Returns: Type Description np.ndarray An array of video frames with dimensions (time x height x width x channels). Source code in zamba/data/video.py def load_video_frames ( filepath : os . PathLike , config : Optional [ VideoLoaderConfig ] = None , ** kwargs , ): \"\"\"Loads frames from videos using fast ffmpeg commands. Args: filepath (os.PathLike): Path to the video. config (VideoLoaderConfig, optional): Configuration for video loading. **kwargs: Optionally, arguments for VideoLoaderConfig can be passed in directly. Returns: np.ndarray: An array of video frames with dimensions (time x height x width x channels). \"\"\" if not Path ( filepath ) . exists (): raise FileNotFoundError ( f \"No file found at { filepath } \" ) if config is None : config = VideoLoaderConfig ( ** kwargs ) video_stream = get_video_stream ( filepath ) w = int ( video_stream [ \"width\" ]) h = int ( video_stream [ \"height\" ]) pipeline = ffmpeg . input ( str ( filepath )) pipeline_kwargs = {} if ( config . crop_bottom_pixels is not None ) and ( config . crop_bottom_pixels > 0 ): # scale to ensure all frames are the same height and we can crop pipeline = pipeline . filter ( \"scale\" , f \" { w } , { h } \" ) pipeline = pipeline . crop ( \"0\" , \"0\" , \"iw\" , f \"ih- { config . crop_bottom_pixels } \" ) h = h - config . crop_bottom_pixels if config . evenly_sample_total_frames : config . fps = config . total_frames / float ( video_stream [ \"duration\" ]) if config . early_bias : config . fps = 24 # competition frame selection assumes 24 frames per second config . total_frames = 16 # used for ensure_total_frames if config . fps : pipeline = pipeline . filter ( \"fps\" , fps = config . fps , round = \"up\" ) if config . i_frames : pipeline = pipeline . filter ( \"select\" , \"eq(pict_type,PICT_TYPE_I)\" ) if config . scene_threshold : pipeline = pipeline . filter ( \"select\" , f \"gt(scene, { config . scene_threshold } )\" ) if config . frame_selection_height and config . frame_selection_width : pipeline = pipeline . filter ( \"scale\" , f \" { config . frame_selection_width } , { config . frame_selection_height } \" ) w , h = config . frame_selection_width , config . frame_selection_height if config . early_bias : config . frame_indices = [ 2 , 8 , 12 , 18 , 24 , 36 , 48 , 60 , 72 , 84 , 96 , 108 , 120 , 132 , 144 , 156 ] if config . frame_indices : pipeline = pipeline . filter ( \"select\" , \"+\" . join ( f \"eq(n, { f } )\" for f in config . frame_indices )) pipeline_kwargs = { \"vsync\" : 0 } pipeline = pipeline . output ( \"pipe:\" , format = \"rawvideo\" , pix_fmt = config . pix_fmt , ** pipeline_kwargs ) try : out , err = pipeline . run ( capture_stdout = True , capture_stderr = True ) except ffmpeg . Error as exc : raise ZambaFfmpegException ( exc . stderr ) arr = np . frombuffer ( out , np . uint8 ) . reshape ([ - 1 , h , w , 3 ]) if config . megadetector_lite_config is not None : mdlite = MegadetectorLiteYoloX ( config = config . megadetector_lite_config ) detection_probs = mdlite . detect_video ( frames = arr ) arr = mdlite . filter_frames ( arr , detection_probs ) if ( config . model_input_height is not None ) and ( config . model_input_width is not None ): resized_frames = np . zeros ( ( arr . shape [ 0 ], config . model_input_height , config . model_input_width , 3 ), np . uint8 ) for ix , f in enumerate ( arr ): if ( f . shape [ 0 ] != config . model_input_height ) or ( f . shape [ 1 ] != config . model_input_width ): f = cv2 . resize ( f , ( config . model_input_width , config . model_input_height ), # https://stackoverflow.com/a/51042104/1692709 interpolation = ( cv2 . INTER_LINEAR if f . shape [ 1 ] < config . model_input_width else cv2 . INTER_AREA ), ) resized_frames [ ix , ... ] = f arr = np . array ( resized_frames ) if config . ensure_total_frames : arr = ensure_frame_number ( arr , total_frames = config . total_frames ) return arr num_frames ( stream_or_path : Union [ dict , os . PathLike , cloudpathlib . s3 . s3path . S3Path ]) -> Optional [ int ] \u00b6 Source code in zamba/data/video.py def num_frames ( stream_or_path : Union [ dict , os . PathLike , S3Path ]) -> Optional [ int ]: if not isinstance ( stream_or_path , dict ): stream = get_video_stream ( stream_or_path ) else : stream = stream_or_path if not stream : return if \"nb_frames\" in stream : return int ( stream [ \"nb_frames\" ]) if \"duration\" in stream : duration = float ( stream [ \"duration\" ]) if \"r_frame_rate\" in stream : frame_rate = float ( Fraction ( stream [ \"r_frame_rate\" ])) elif \"avg_frame_rate\" in stream : frame_rate = float ( stream [ \"avg_frame_rate\" ]) duration -= float ( stream . get ( \"start_time\" , 0 )) return floor ( duration * frame_rate )","title":"zamba.data.video"},{"location":"api-reference/data-video/#zambadatavideo","text":"","title":"zamba.data.video"},{"location":"api-reference/data-video/#zamba.data.video-classes","text":"","title":"Classes"},{"location":"api-reference/data-video/#zamba.data.video-functions","text":"","title":"Functions"},{"location":"api-reference/densepose_config/","text":"zamba.models.densepose.config \u00b6 Classes \u00b6 DensePoseConfig ( ZambaBaseModel ) pydantic-model \u00b6 Configuration for running dense pose on videos. Parameters: Name Type Description Default video_loader_config VideoLoaderConfig Configuration for loading videos required output_type str one of DensePoseOutputEnum (currently \"segmentation\" or \"chimp_anatomy\"). required render_output bool Whether to save a version of the video with the output overlaid on top. Defaults to False. required embeddings_in_json bool Whether to save the embeddings matrices in the json of the DensePose result. Setting to True can result in large json files. Defaults to False. required data_dir Path Where to find the files listed in filepaths (or where to look if filepaths is not provided). required filepaths Path Path to a CSV file with a list of filepaths to process. required save_dir Path Directory for where to save the output files; defaults to os.getcwd(). required cache_dir Path Path for downloading and saving model weights. Defaults to env var MODEL_CACHE_DIR or the OS app cache dir. required weight_download_region RegionEnum region where to download weights; should be one of RegionEnum (currently 'us', 'asia', and 'eu'). Defaults to 'us'. required cache_dir : Path pydantic-field \u00b6 data_dir : Path pydantic-field required \u00b6 embeddings_in_json : bool pydantic-field \u00b6 filepaths : Path pydantic-field \u00b6 output_type : DensePoseOutputEnum pydantic-field required \u00b6 render_output : bool pydantic-field \u00b6 save_dir : Path pydantic-field \u00b6 video_loader_config : VideoLoaderConfig pydantic-field required \u00b6 weight_download_region : RegionEnum pydantic-field \u00b6 Config inherited \u00b6 extra \u00b6 use_enum_values \u00b6 validate_assignment \u00b6 Methods \u00b6 get_filepaths ( values ) classmethod \u00b6 If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column filepath contains files with valid suffixes. Source code in zamba/models/densepose/config.py @root_validator ( pre = False , skip_on_failure = True ) def get_filepaths ( cls , values ): \"\"\"If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column `filepath` contains files with valid suffixes. \"\"\" if values [ \"filepaths\" ] is None : logger . info ( f \"Getting files in { values [ 'data_dir' ] } .\" ) files = [] new_suffixes = [] # iterate over all files in data directory for f in values [ \"data_dir\" ] . rglob ( \"*\" ): if f . is_file (): # keep just files with supported suffixes if f . suffix . lower () in VIDEO_SUFFIXES : files . append ( f . resolve ()) else : new_suffixes . append ( f . suffix . lower ()) if len ( new_suffixes ) > 0 : logger . warning ( f \"Ignoring { len ( new_suffixes ) } file(s) with suffixes { set ( new_suffixes ) } . To include, specify all video suffixes with a VIDEO_SUFFIXES environment variable.\" ) if len ( files ) == 0 : raise ValueError ( f \"No video files found in { values [ 'data_dir' ] } .\" ) logger . info ( f \"Found { len ( files ) } videos in { values [ 'data_dir' ] } .\" ) values [ \"filepaths\" ] = pd . DataFrame ( files , columns = [ \"filepath\" ]) return values run_model ( self ) \u00b6 Use this configuration to execute DensePose via the DensePoseManager Source code in zamba/models/densepose/config.py def run_model ( self ): \"\"\"Use this configuration to execute DensePose via the DensePoseManager\"\"\" if not isinstance ( self . output_type , DensePoseOutputEnum ): self . output_type = DensePoseOutputEnum ( self . output_type ) if self . output_type == DensePoseOutputEnum . segmentation . value : model = MODELS [ \"animals\" ] elif self . output_type == DensePoseOutputEnum . chimp_anatomy . value : model = MODELS [ \"chimps\" ] else : raise Exception ( f \"invalid { self . output_type } \" ) output_dir = Path ( os . getcwd ()) if self . save_dir is None else self . save_dir dpm = DensePoseManager ( model , model_cache_dir = self . cache_dir , download_region = self . weight_download_region ) for fp in tqdm ( self . filepaths . filepath , desc = \"Videos\" ): fp = Path ( fp ) vid_arr , labels = dpm . predict_video ( fp , video_loader_config = self . video_loader_config ) # serialize the labels generated by densepose to json output_path = output_dir / f \" { fp . stem } _denspose_labels.json\" dpm . serialize_video_output ( labels , filename = output_path , write_embeddings = self . embeddings_in_json ) # re-render the video with the densepose labels visualized on top of the video if self . render_output : output_path = output_dir / f \" { fp . stem } _denspose_video { '' . join ( fp . suffixes ) } \" visualized_video = dpm . visualize_video ( vid_arr , labels , output_path = output_path , fps = self . video_loader_config . fps ) # write out the anatomy present in each frame to a csv for later analysis if self . output_type == DensePoseOutputEnum . chimp_anatomy . value : output_path = output_dir / f \" { fp . stem } _denspose_anatomy.csv\" dpm . anatomize_video ( visualized_video , labels , output_path = output_path , fps = self . video_loader_config . fps , ) validate_files ( values ) classmethod \u00b6 Source code in zamba/models/densepose/config.py @root_validator ( skip_on_failure = True ) def validate_files ( cls , values ): # if globbing from data directory, already have valid dataframe if isinstance ( values [ \"filepaths\" ], pd . DataFrame ): files_df = values [ \"filepaths\" ] else : # make into dataframe even if only one column for clearer indexing files_df = pd . DataFrame ( pd . read_csv ( values [ \"filepaths\" ])) if \"filepath\" not in files_df . columns : raise ValueError ( f \" { values [ 'filepaths' ] } must contain a `filepath` column.\" ) # can only contain one row per filepath num_duplicates = len ( files_df ) - files_df . filepath . nunique () if num_duplicates > 0 : logger . warning ( f \"Found { num_duplicates } duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\" ) files_df = files_df [[ \"filepath\" ]] . drop_duplicates () values [ \"filepaths\" ] = check_files_exist_and_load ( df = files_df , data_dir = values [ \"data_dir\" ], skip_load_validation = True , ) return values DensePoseOutputEnum ( Enum ) \u00b6 An enumeration. chimp_anatomy \u00b6 segmentation \u00b6","title":"zamba.models.densepose.config"},{"location":"api-reference/densepose_config/#zambamodelsdenseposeconfig","text":"","title":"zamba.models.densepose.config"},{"location":"api-reference/densepose_config/#zamba.models.densepose.config-classes","text":"","title":"Classes"},{"location":"api-reference/densepose_manager/","text":"zamba.models.densepose.densepose_manager \u00b6 DENSEPOSE_AVAILABLE \u00b6 DensePoseOutputsTextureVisualizer \u00b6 DensePoseOutputsVertexVisualizer \u00b6 MODELS \u00b6 Classes \u00b6 DensePoseManager \u00b6 Methods \u00b6 __init__ ( self , model = { 'config' : '/home/runner/work/zamba/zamba/zamba/models/densepose/assets/densepose_rcnn_R_50_FPN_soft_chimps_finetune_4k.yaml' , 'densepose_weights_url' : 'https://dl.fbaipublicfiles.com/densepose/cse/densepose_rcnn_R_50_FPN_soft_chimps_finetune_4k/253146869/model_final_52f649.pkl' , 'weights' : 'zamba_densepose_model_final_52f649.pkl' , 'viz_class' : None , 'viz_class_kwargs' : { 'texture_atlases_dict' : { 'chimp_5029' : None }}, 'anatomy_color_mapping' : '/home/runner/work/zamba/zamba/zamba/models/densepose/assets/chimp_5029_parts.csv' }, model_cache_dir : Path = PosixPath ( '.zamba_cache' ), download_region = < RegionEnum . us : 'us' > ) special \u00b6 Create a DensePoseManager object. Parameters \u00b6 model : dict, optional (default MODELS['chimps']) A dictionary with the densepose model defintion like those defined in MODELS. Source code in zamba/models/densepose/densepose_manager.py def __init__ ( self , model = MODELS [ \"chimps\" ], model_cache_dir : Path = Path ( \".zamba_cache\" ), download_region = RegionEnum ( \"us\" ), ): \"\"\"Create a DensePoseManager object. Parameters ---------- model : dict, optional (default MODELS['chimps']) A dictionary with the densepose model defintion like those defined in MODELS. \"\"\" if not DENSEPOSE_AVAILABLE : raise ImportError ( \"Densepose not installed; install it as an extra with `pip install zamba[densepose]`.\" ) # setup configuration for densepose self . cfg = get_cfg () add_densepose_config ( self . cfg ) self . cfg . merge_from_file ( model [ \"config\" ]) if not ( model_cache_dir / model [ \"weights\" ]) . exists (): model_cache_dir . mkdir ( parents = True , exist_ok = True ) self . cfg . MODEL . WEIGHTS = download_weights ( model [ \"weights\" ], model_cache_dir , download_region ) # automatically use CPU if no cuda available if not torch . cuda . is_available (): self . cfg . MODEL . DEVICE = \"cpu\" self . cfg . freeze () logging . getLogger ( \"fvcore\" ) . setLevel ( \"CRITICAL\" ) # silence noisy detectron2 logging # set up predictor with the configuration self . predictor = DefaultPredictor ( self . cfg ) # we have a specific texture atlas for chimps with relevant regions # labeled that we can use instead of the default segmentation self . visualizer = model [ \"viz_class\" ]( self . cfg , device = self . cfg . MODEL . DEVICE , ** model . get ( \"viz_class_kwargs\" , {}), ) # set up utilities for use with visualizer self . vis_extractor = create_extractor ( self . visualizer ) self . vis_embedder = build_densepose_embedder ( self . cfg ) self . vis_class_to_mesh_name = get_class_to_mesh_name_mapping ( self . cfg ) self . vis_mesh_vertex_embeddings = { mesh_name : self . vis_embedder ( mesh_name ) . to ( self . cfg . MODEL . DEVICE ) for mesh_name in self . vis_class_to_mesh_name . values () if self . vis_embedder . has_embeddings ( mesh_name ) } if \"anatomy_color_mapping\" in model : self . anatomy_color_mapping = pd . read_csv ( model [ \"anatomy_color_mapping\" ], index_col = 0 ) else : self . anatomy_color_mapping = None anatomize_image ( self , visualized_img_arr , outputs , output_path = None ) \u00b6 Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in an image. Parameters \u00b6 visualized_img_arr : numpy array (unit8) BGR The numpy array the image after the texture has been visualized (by calling DensePoseManager.visualize_image). outputs : The outputs from running DensePoseManager.predict* Returns \u00b6 pandas.DataFrame DataFrame with percent of pixels of the bounding box that correspond to each anatomical part Source code in zamba/models/densepose/densepose_manager.py def anatomize_image ( self , visualized_img_arr , outputs , output_path = None ): \"\"\"Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in an image. Parameters ---------- visualized_img_arr : numpy array (unit8) BGR The numpy array the image after the texture has been visualized (by calling DensePoseManager.visualize_image). outputs : The outputs from running DensePoseManager.predict* Returns ------- pandas.DataFrame DataFrame with percent of pixels of the bounding box that correspond to each anatomical part \"\"\" if self . anatomy_color_mapping is None : raise ValueError ( \"No anatomy_color_mapping provided to track anatomy; did you mean to use a different MODEL?\" ) # no detections, return empty df for joining later (e.g., in anatomize_video) if not outputs : return pd . DataFrame ([]) _ , _ , N , bboxes_xywh , pred_classes = self . visualizer . extract_and_check_outputs_and_boxes ( self . vis_extractor ( outputs ) ) all_detections = [] for n in range ( N ): x , y , w , h = bboxes_xywh [ n ] . int () . cpu () . numpy () detection_area = visualized_img_arr [ y : y + h , x : x + w ] detection_stats = { name : ( detection_area == np . array ([[[ color . B , color . G , color . R ]]])) . all ( axis =- 1 ) . sum () / ( h * w ) # calc percent of bounding box with this color for name , color in self . anatomy_color_mapping . iterrows () } detection_stats [ \"x\" ] = x detection_stats [ \"y\" ] = y detection_stats [ \"h\" ] = h detection_stats [ \"w\" ] = w all_detections . append ( detection_stats ) results = pd . DataFrame ( all_detections ) if output_path is not None : results . to_csv ( output_path , index = False ) return results anatomize_video ( self , visualized_video_arr , outputs , output_path = None , fps = 30 ) \u00b6 Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in a video. Parameters \u00b6 visualized_video_arr : numpy array (unit8) BGR The numpy array the video after the texture has been visualized (by calling DensePoseManager.visualize_video). outputs : The outputs from running DensePoseManager.predict* Returns \u00b6 numpy array (unit8) BGR DensePose outputs visualized on top of the image. Source code in zamba/models/densepose/densepose_manager.py def anatomize_video ( self , visualized_video_arr , outputs , output_path = None , fps = 30 ): \"\"\"Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in a video. Parameters ---------- visualized_video_arr : numpy array (unit8) BGR The numpy array the video after the texture has been visualized (by calling DensePoseManager.visualize_video). outputs : The outputs from running DensePoseManager.predict* Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" all_detections = [] for ix in range ( visualized_video_arr . shape [ 0 ]): detection_df = self . anatomize_image ( visualized_video_arr [ ix , ... ], outputs [ ix ]) detection_df [ \"frame\" ] = ix detection_df [ \"seconds\" ] = ix / fps all_detections . append ( detection_df ) results = pd . concat ( all_detections ) if output_path is not None : results . to_csv ( output_path , index = False ) return results deserialize_output ( self , instances_dict = None , filename = None ) \u00b6 Source code in zamba/models/densepose/densepose_manager.py def deserialize_output ( self , instances_dict = None , filename = None ): if filename is not None : with Path ( filename ) . open ( \"r\" ) as f : instances_dict = json . load ( f ) # handle image case is_image = False if \"frames\" not in instances_dict : instances_dict = { \"frames\" : [ instances_dict ]} is_image = True frames = [] for frame in instances_dict [ \"frames\" ]: heights , widths , boxes , scores , labels , embeddings , segmentations = zip ( * [ ( i [ \"img_height\" ], i [ \"img_width\" ], i [ \"box\" ], i [ \"score\" ], i [ \"label\" ][ \"value\" ], i [ \"embedding\" ] if i [ \"embedding\" ] is not None else [ np . nan ], i [ \"segmentation\" ] if i [ \"segmentation\" ] is not None else [ np . nan ], ) for i in frame [ \"instances\" ] ] ) frames . append ( Instances ( ( heights [ 0 ], widths [ 0 ]), pred_boxes = boxes , scores = scores , pred_classes = labels , pred_densepose = DensePoseEmbeddingPredictorOutput ( embedding = torch . tensor ( embeddings ), coarse_segm = torch . tensor ( segmentations ), ), ) ) # if image or single frame, just return the instance if is_image : return frames [ 0 ] else : return frames predict ( self , image_arr ) \u00b6 Main call to DensePose for inference. Runs inference on an image array. Parameters \u00b6 image_arr : numpy array BGR image array Returns \u00b6 Instances Detection instances with boxes, scores, and densepose estimates. Source code in zamba/models/densepose/densepose_manager.py def predict ( self , image_arr ): \"\"\"Main call to DensePose for inference. Runs inference on an image array. Parameters ---------- image_arr : numpy array BGR image array Returns ------- Instances Detection instances with boxes, scores, and densepose estimates. \"\"\" with torch . no_grad (): instances = self . predictor ( image_arr )[ \"instances\" ] return instances predict_image ( self , image ) \u00b6 Run inference to get the densepose results for an image. Parameters \u00b6 image : numpy array (unit8) of an image in BGR format or path to an image Returns \u00b6 tuple Returns the image array as passed or loaded and the the densepose Instances as results. Source code in zamba/models/densepose/densepose_manager.py def predict_image ( self , image ): \"\"\"Run inference to get the densepose results for an image. Parameters ---------- image : numpy array (unit8) of an image in BGR format or path to an image Returns ------- tuple Returns the image array as passed or loaded and the the densepose Instances as results. \"\"\" if isinstance ( image , ( str , Path )): image = read_image ( image , format = \"BGR\" ) return image , self . predict ( image ) predict_video ( self , video , video_loader_config = None , pbar = True ) \u00b6 Run inference to get the densepose results for a video. Parameters \u00b6 video : numpy array (uint8) of a a video in BGR layout with time dimension first or path to a video video_loader_config : VideoLoaderConfig, optional A video loader config for loading videos (uses all defaults except pix_fmt=\"bgr24\") pbar : bool, optional Whether to display a progress bar, by default True Returns \u00b6 tuple Tuple of (video_array, list of densepose results per frame) Source code in zamba/models/densepose/densepose_manager.py def predict_video ( self , video , video_loader_config = None , pbar = True ): \"\"\"Run inference to get the densepose results for a video. Parameters ---------- video : numpy array (uint8) of a a video in BGR layout with time dimension first or path to a video video_loader_config : VideoLoaderConfig, optional A video loader config for loading videos (uses all defaults except pix_fmt=\"bgr24\") pbar : bool, optional Whether to display a progress bar, by default True Returns ------- tuple Tuple of (video_array, list of densepose results per frame) \"\"\" if isinstance ( video , ( str , Path )): video = load_video_frames ( video , config = video_loader_config ) pbar = tqdm if pbar else lambda x , ** kwargs : x return video , [ self . predict_image ( img )[ 1 ] for img in pbar ( video , desc = \"Frames\" ) ] # just the predictions serialize_image_output ( self , instances , filename = None , write_embeddings = False ) \u00b6 Convert the densepose output into Python-native objects that can be written and read with json. Parameters \u00b6 instances : Instance The output from the densepose model filename : (str, Path), optional If not None, the filename to write the output to, by default None Source code in zamba/models/densepose/densepose_manager.py def serialize_image_output ( self , instances , filename = None , write_embeddings = False ): \"\"\"Convert the densepose output into Python-native objects that can be written and read with json. Parameters ---------- instances : Instance The output from the densepose model filename : (str, Path), optional If not None, the filename to write the output to, by default None \"\"\" if isinstance ( instances , list ): img_height , img_width = instances [ 0 ] . image_size else : img_height , img_width = instances . image_size boxes = instances . get ( \"pred_boxes\" ) . tensor scores = instances . get ( \"scores\" ) . tolist () labels = instances . get ( \"pred_classes\" ) . tolist () try : pose_result = instances . get ( \"pred_densepose\" ) except KeyError : pose_result = None # include embeddings + segmentation if they exist and they are requested write_embeddings = write_embeddings and ( pose_result is not None ) serialized = { \"instances\" : [ { \"img_height\" : img_height , \"img_width\" : img_width , \"box\" : boxes [ i ] . cpu () . tolist (), \"score\" : scores [ i ], \"label\" : { \"value\" : labels [ i ], \"mesh_name\" : self . vis_class_to_mesh_name [ labels [ i ]], }, \"embedding\" : pose_result . embedding [[ i ], ... ] . cpu () . tolist () if write_embeddings else None , \"segmentation\" : pose_result . coarse_segm [[ i ], ... ] . cpu () . tolist () if write_embeddings else None , } for i in range ( len ( instances )) ] } if filename is not None : with Path ( filename ) . open ( \"w\" ) as f : json . dump ( serialized , f , indent = 2 ) return serialized serialize_video_output ( self , instances , filename = None , write_embeddings = False ) \u00b6 Source code in zamba/models/densepose/densepose_manager.py def serialize_video_output ( self , instances , filename = None , write_embeddings = False ): serialized = { \"frames\" : [ self . serialize_image_output ( frame_instances , filename = None , write_embeddings = write_embeddings ) for frame_instances in instances ] } if filename is not None : with Path ( filename ) . open ( \"w\" ) as f : json . dump ( serialized , f , indent = 2 ) return serialized visualize_image ( self , image_arr , outputs , output_path = None ) \u00b6 Visualize the pose information. Parameters \u00b6 image_arr : numpy array (unit8) BGR The numpy array representing the image. outputs : The outputs from running DensePoseManager.predict* output_path : str or Path, optional If not None, write visualization to this path; by default None Returns \u00b6 numpy array (unit8) BGR DensePose outputs visualized on top of the image. Source code in zamba/models/densepose/densepose_manager.py def visualize_image ( self , image_arr , outputs , output_path = None ): \"\"\"Visualize the pose information. Parameters ---------- image_arr : numpy array (unit8) BGR The numpy array representing the image. outputs : The outputs from running DensePoseManager.predict* output_path : str or Path, optional If not None, write visualization to this path; by default None Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" bw_image = cv2 . cvtColor ( image_arr , cv2 . COLOR_BGR2GRAY ) bw_image = np . tile ( bw_image [:, :, np . newaxis ], [ 1 , 1 , 3 ]) data = self . vis_extractor ( outputs ) image_vis = self . visualizer . visualize ( bw_image , data ) if output_path is not None : cv2 . imwrite ( str ( output_path ), image_vis ) return image_vis visualize_video ( self , video_arr , outputs , output_path = None , frame_size = None , fps = 30 , pbar = True ) \u00b6 Visualize the pose information on a video Parameters \u00b6 video_arr : numpy array (unit8) BGR, time first The numpy array representing the video. outputs : The outputs from running DensePoseManager.predict* output_path : str or Path, optional If not None, write visualization to this path (should be .mp4); by default None frame_size : (innt, float), optional If frame_size is float, scale up or down by that float value; if frame_size is an integer, set width to that size and scale height appropriately. fps : int frames per second for output video if writing; defaults to 30 pbar : bool display a progress bar Returns \u00b6 numpy array (unit8) BGR DensePose outputs visualized on top of the image. Source code in zamba/models/densepose/densepose_manager.py def visualize_video ( self , video_arr , outputs , output_path = None , frame_size = None , fps = 30 , pbar = True ): \"\"\"Visualize the pose information on a video Parameters ---------- video_arr : numpy array (unit8) BGR, time first The numpy array representing the video. outputs : The outputs from running DensePoseManager.predict* output_path : str or Path, optional If not None, write visualization to this path (should be .mp4); by default None frame_size : (innt, float), optional If frame_size is float, scale up or down by that float value; if frame_size is an integer, set width to that size and scale height appropriately. fps : int frames per second for output video if writing; defaults to 30 pbar : bool display a progress bar Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" pbar = tqdm if pbar else lambda x , ** kwargs : x out_frames = np . array ( [ self . visualize_image ( image_arr , output , ) for image_arr , output in pbar ( zip ( video_arr , outputs ), total = video_arr . shape [ 0 ], desc = \"Visualize frames\" ) ] ) if output_path is not None : # get new size for output video if scaling if frame_size is None : frame_size = video_arr . shape [ 2 ] # default to same size # if float, scale as a multiple if isinstance ( frame_size , float ): frame_width = round ( video_arr . shape [ 2 ] * frame_size ) frame_height = round ( video_arr . shape [ 1 ] * frame_size ) # if int, use as width of the video and scale height proportionally elif isinstance ( frame_size , int ): frame_width = frame_size scale = frame_width / video_arr . shape [ 2 ] frame_height = round ( video_arr . shape [ 1 ] * scale ) # setup output for writing output_path = output_path . with_suffix ( \".mp4\" ) out = cv2 . VideoWriter ( str ( output_path ), cv2 . VideoWriter_fourcc ( * \"mp4v\" ), max ( 1 , int ( fps )), ( frame_width , frame_height ), ) for f in pbar ( out_frames , desc = \"Write frames\" ): if ( f . shape [ 0 ] != frame_height ) or ( f . shape [ 1 ] != frame_width ): f = cv2 . resize ( f , ( frame_width , frame_height ), # https://stackoverflow.com/a/51042104/1692709 interpolation = ( cv2 . INTER_LINEAR if f . shape [ 1 ] < frame_width else cv2 . INTER_AREA ), ) out . write ( f ) out . release () return out_frames get_texture_atlas ( x ) \u00b6 Source code in zamba/models/densepose/densepose_manager.py get_texture_atlas = lambda x : None # noqa: E731","title":"zamba.models.densepose.densepose_manager"},{"location":"api-reference/densepose_manager/#zambamodelsdenseposedensepose_manager","text":"","title":"zamba.models.densepose.densepose_manager"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager-classes","text":"","title":"Classes"},{"location":"api-reference/exceptions/","text":"zamba.exceptions \u00b6 ZambaFfmpegException ( Exception ) \u00b6 __init__ ( self , stderr : Union [ bytes , str ]) special \u00b6 Source code in zamba/exceptions.py def __init__ ( self , stderr : Union [ bytes , str ]): message = stderr . decode ( \"utf8\" , errors = \"replace\" ) if isinstance ( stderr , bytes ) else stderr super () . __init__ ( f \"Video loading failer with error: \\n { message } \" )","title":"zamba.exceptions"},{"location":"api-reference/exceptions/#zambaexceptions","text":"","title":"zamba.exceptions"},{"location":"api-reference/metrics/","text":"zamba.metrics \u00b6 Functions \u00b6 compute_species_specific_metrics ( y_true : ndarray , y_pred : ndarray , labels : Optional [ List [ str ]] = None ) -> Generator [ Tuple [ str , int , float ], NoneType , NoneType ] \u00b6 Computes species-specific accuracy, F1, precision, and recall. Parameters: Name Type Description Default y_true np.ndarray An array with shape (samples, species) where each value indicates the presence of a species in a sample. required y_pred np.ndarray An array with shape (samples, species) where each value indicates the predicted presence of a species in a sample. required !!! yields str, int, float: The metric name, species label index, and metric value. Source code in zamba/metrics.py def compute_species_specific_metrics ( y_true : np . ndarray , y_pred : np . ndarray , labels : Optional [ List [ str ]] = None , ) -> Generator [ Tuple [ str , int , float ], None , None ]: \"\"\"Computes species-specific accuracy, F1, precision, and recall. Args: y_true (np.ndarray): An array with shape (samples, species) where each value indicates the presence of a species in a sample. y_pred (np.ndarray): An array with shape (samples, species) where each value indicates the predicted presence of a species in a sample. Yields: str, int, float: The metric name, species label index, and metric value. \"\"\" if labels is None : labels = range ( y_true . shape [ 1 ]) elif len ( labels ) != y_true . shape [ 1 ]: raise ValueError ( f \"The number of labels ( { len ( labels ) } ) must match the number of columns in y_true ( { y_true . shape [ 1 ] } ).\" ) for index , label in enumerate ( labels ): yield \"accuracy\" , label , accuracy_score ( y_true [:, index ], y_pred [:, index ]) yield \"f1\" , label , f1_score ( y_true [:, index ], y_pred [:, index ], zero_division = 0 ) yield \"precision\" , label , precision_score ( y_true [:, index ], y_pred [:, index ], zero_division = 0 ) yield \"recall\" , label , recall_score ( y_true [:, index ], y_pred [:, index ], zero_division = 0 )","title":"zamba.metrics"},{"location":"api-reference/metrics/#zambametrics","text":"","title":"zamba.metrics"},{"location":"api-reference/metrics/#zamba.metrics-functions","text":"","title":"Functions"},{"location":"api-reference/models-config/","text":"zamba.models.config \u00b6 GPUS_AVAILABLE \u00b6 MODEL_MAPPING \u00b6 WEIGHT_LOOKUP \u00b6 Classes \u00b6 BackboneFinetuneConfig ( ZambaBaseModel ) pydantic-model \u00b6 Configuration containing parameters to be used for backbone finetuning. Parameters: Name Type Description Default unfreeze_backbone_at_epoch int Epoch at which the backbone will be unfrozen. Defaults to 5. required backbone_initial_ratio_lr float Used to scale down the backbone learning rate compared to rest of model. Defaults to 0.01. required multiplier int or float Multiply the learning rate by a constant value at the end of each epoch. Defaults to 1. required pre_train_bn bool Train batch normalization layers prior to finetuning. False is recommended for slowfast models and True is recommended for time distributed models. Defaults to False. required train_bn bool Make batch normalization trainable. Defaults to False. required verbose bool Display current learning rate for model and backbone. Defaults to True. required backbone_initial_ratio_lr : float pydantic-field \u00b6 multiplier : Union [ int , float ] pydantic-field \u00b6 pre_train_bn : bool pydantic-field \u00b6 train_bn : bool pydantic-field \u00b6 unfreeze_backbone_at_epoch : int pydantic-field \u00b6 verbose : bool pydantic-field \u00b6 Config inherited \u00b6 extra \u00b6 use_enum_values \u00b6 validate_assignment \u00b6 EarlyStoppingConfig ( ZambaBaseModel ) pydantic-model \u00b6 Configuration containing parameters to be used for early stopping. Parameters: Name Type Description Default monitor str Metric to be monitored. Options are \"val_macro_f1\" or \"val_loss\". Defaults to \"val_macro_f1\". required patience int Number of epochs with no improvement after which training will be stopped. Defaults to 5. required verbose bool Verbosity mode. Defaults to True. required mode str Options are \"min\" or \"max\". In \"min\" mode, training will stop when the quantity monitored has stopped decreasing and in \"max\" mode it will stop when the quantity monitored has stopped increasing. If None, mode will be inferred from monitor. Defaults to None. required mode : str pydantic-field \u00b6 monitor : MonitorEnum pydantic-field \u00b6 patience : int pydantic-field \u00b6 verbose : bool pydantic-field \u00b6 Config inherited \u00b6 extra \u00b6 use_enum_values \u00b6 validate_assignment \u00b6 validate_mode ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator def validate_mode ( cls , values ): mode = { \"val_macro_f1\" : \"max\" , \"val_loss\" : \"min\" }[ values . get ( \"monitor\" )] user_mode = values . get ( \"mode\" ) if user_mode is None : values [ \"mode\" ] = mode elif user_mode != mode : raise ValueError ( f \"Provided mode { user_mode } is incorrect for { values . get ( 'monitor' ) } monitor.\" ) return values ModelConfig ( ZambaBaseModel ) pydantic-model \u00b6 Contains all configs necessary to use a model for training or inference. Must contain a train_config or a predict_config at a minimum. Parameters: Name Type Description Default video_loader_config VideoLoaderConfig An instantiated VideoLoaderConfig. If None, will use default video loader config for model specified in TrainConfig or PredictConfig. required train_config TrainConfig An instantiated TrainConfig. Defaults to None. required predict_config PredictConfig An instantiated PredictConfig. Defaults to None. required predict_config : PredictConfig pydantic-field \u00b6 train_config : TrainConfig pydantic-field \u00b6 video_loader_config : VideoLoaderConfig pydantic-field \u00b6 Classes \u00b6 Config \u00b6 Methods \u00b6 json_loads ( stream ) \u00b6 Parse the first YAML document in a stream and produce the corresponding Python object. Resolve only basic YAML tags. This is known to be safe for untrusted input. Source code in zamba/models/config.py def safe_load ( stream ): \"\"\" Parse the first YAML document in a stream and produce the corresponding Python object. Resolve only basic YAML tags. This is known to be safe for untrusted input. \"\"\" return load ( stream , SafeLoader ) get_default_video_loader_config ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def get_default_video_loader_config ( cls , values ): if values [ \"video_loader_config\" ] is None : model_name = ( values [ \"train_config\" ] . model_name if values [ \"train_config\" ] is not None else values [ \"predict_config\" ] . model_name ) logger . info ( f \"No video loader config specified. Using default for { model_name } .\" ) config_file = MODELS_DIRECTORY / f \" { model_name } /config.yaml\" with config_file . open () as f : config_dict = yaml . safe_load ( f ) values [ \"video_loader_config\" ] = VideoLoaderConfig ( ** config_dict [ \"video_loader_config\" ]) return values one_config_must_exist ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def one_config_must_exist ( cls , values ): if values [ \"train_config\" ] is None and values [ \"predict_config\" ] is None : raise ValueError ( \"Must provide either `train_config` or `predict_config`.\" ) else : return values ModelEnum ( str , Enum ) \u00b6 Shorthand names of models supported by zamba. european \u00b6 slowfast \u00b6 time_distributed \u00b6 MonitorEnum ( str , Enum ) \u00b6 Validation metric to monitor for early stopping. Training is stopped when no improvement is observed. val_loss \u00b6 val_macro_f1 \u00b6 PredictConfig ( ZambaBaseModel ) pydantic-model \u00b6 Configuration for using a model for inference. Parameters: Name Type Description Default filepaths FilePath Path to a CSV containing videos for inference, with one row per video in the data_dir. There must be a column called 'filepath' (absolute or relative to the data_dir). If None, uses all files in data_dir. Defaults to None. required data_dir DirectoryPath Path to a directory containing videos for inference. Defaults to the working directory. required model_name str Name of the model to use for inference. Options are: time_distributed, slowfast, european. Defaults to time_distributed. required checkpoint FilePath Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to generate predictions. If None, defaults to a pretrained model. Defaults to None. required gpus int Number of GPUs to use for inference. Defaults to all of the available GPUs found on the machine. required num_workers int Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. The maximum value is the number of CPUs in the system. Defaults to 3. required batch_size int Batch size to use for inference. Defaults to 2. required save bool Whether to save out predictions. If False, predictions are not saved. Defaults to True. required save_dir Path An optional directory in which to save the model predictions and configuration yaml. If no save_dir is specified and save=True, outputs will be written to the current working directory. Defaults to None. required overwrite bool If True, overwrite outputs in save_dir if they exist. Defaults to False. required dry_run bool Perform inference on a single batch for testing. Predictions will not be saved. Defaults to False. required proba_threshold float Probability threshold for classification. If specified, binary predictions are returned with 1 being greater than the threshold and 0 being less than or equal to the threshold. If None, return probability scores for each species. Defaults to None. required output_class_names bool Output the species with the highest probability score as a single prediction for each video. If False, return probabilty scores for each species. Defaults to False. required weight_download_region str s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (European Union), or \"asia\" (Asia Pacific). Defaults to \"us\". required skip_load_validation bool By default, zamba runs a check to verify that all videos can be loaded and skips files that cannot be loaded. This can be time intensive, depending on how many videos there are. If you are very confident all your videos can be loaded, you can set this to True and skip this check. Defaults to False. required model_cache_dir Path Cache directory where downloaded model weights will be saved. If None and no environment variable is set, will use your default cache directory. Defaults to None. required batch_size : int pydantic-field \u00b6 checkpoint : FilePath pydantic-field \u00b6 data_dir : DirectoryPath pydantic-field \u00b6 dry_run : bool pydantic-field \u00b6 filepaths : FilePath pydantic-field \u00b6 gpus : int pydantic-field \u00b6 model_cache_dir : Path pydantic-field \u00b6 model_name : ModelEnum pydantic-field \u00b6 num_workers : int pydantic-field \u00b6 output_class_names : bool pydantic-field \u00b6 overwrite : bool pydantic-field \u00b6 proba_threshold : float pydantic-field \u00b6 save : bool pydantic-field \u00b6 save_dir : Path pydantic-field \u00b6 skip_load_validation : bool pydantic-field \u00b6 weight_download_region : RegionEnum pydantic-field \u00b6 Config inherited \u00b6 extra \u00b6 use_enum_values \u00b6 validate_assignment \u00b6 Methods \u00b6 get_filepaths ( values ) classmethod \u00b6 If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column filepath contains files with valid suffixes. Source code in zamba/models/config.py @root_validator ( pre = False , skip_on_failure = True ) def get_filepaths ( cls , values ): \"\"\"If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column `filepath` contains files with valid suffixes. \"\"\" if values [ \"filepaths\" ] is None : logger . info ( f \"Getting files in { values [ 'data_dir' ] } .\" ) files = [] new_suffixes = [] # iterate over all files in data directory for f in values [ \"data_dir\" ] . rglob ( \"*\" ): if f . is_file (): # keep just files with supported suffixes if f . suffix . lower () in VIDEO_SUFFIXES : files . append ( f . resolve ()) else : new_suffixes . append ( f . suffix . lower ()) if len ( new_suffixes ) > 0 : logger . warning ( f \"Ignoring { len ( new_suffixes ) } file(s) with suffixes { set ( new_suffixes ) } . To include, specify all video suffixes with a VIDEO_SUFFIXES environment variable.\" ) if len ( files ) == 0 : raise ValueError ( f \"No video files found in { values [ 'data_dir' ] } .\" ) logger . info ( f \"Found { len ( files ) } videos in { values [ 'data_dir' ] } .\" ) values [ \"filepaths\" ] = pd . DataFrame ( files , columns = [ \"filepath\" ]) return values validate_dry_run_and_save ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def validate_dry_run_and_save ( cls , values ): if values [ \"dry_run\" ] and ( ( values [ \"save\" ] is not False ) or ( values [ \"save_dir\" ] is not None ) ): logger . warning ( \"Cannot save when predicting with dry_run=True. Setting save=False and save_dir=None.\" ) values [ \"save\" ] = False values [ \"save_dir\" ] = None return values validate_files ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def validate_files ( cls , values ): # if globbing from data directory, already have valid dataframe if isinstance ( values [ \"filepaths\" ], pd . DataFrame ): files_df = values [ \"filepaths\" ] else : # make into dataframe even if only one column for clearer indexing files_df = pd . DataFrame ( pd . read_csv ( values [ \"filepaths\" ])) if \"filepath\" not in files_df . columns : raise ValueError ( f \" { values [ 'filepaths' ] } must contain a `filepath` column.\" ) # can only contain one row per filepath num_duplicates = len ( files_df ) - files_df . filepath . nunique () if num_duplicates > 0 : logger . warning ( f \"Found { num_duplicates } duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\" ) files_df = files_df [[ \"filepath\" ]] . drop_duplicates () values [ \"filepaths\" ] = check_files_exist_and_load ( df = files_df , data_dir = values [ \"data_dir\" ], skip_load_validation = values [ \"skip_load_validation\" ], ) return values validate_proba_threshold ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def validate_proba_threshold ( cls , values ): if values [ \"proba_threshold\" ] is not None : if ( values [ \"proba_threshold\" ] <= 0 ) or ( values [ \"proba_threshold\" ] >= 1 ): raise ValueError ( \"Setting proba_threshold outside of the range (0, 1) will cause all probabilities to be rounded to the same value.\" ) if values [ \"output_class_names\" ] is True : logger . warning ( \"`output_class_names` will be ignored because `proba_threshold` is specified.\" ) return values validate_save_dir ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def validate_save_dir ( cls , values ): save_dir = values [ \"save_dir\" ] save = values [ \"save\" ] # if no save_dir but save is True, use current working directory if save_dir is None and save : save_dir = Path . cwd () if save_dir is not None : # check if files exist if ( ( save_dir / \"zamba_predictions.csv\" ) . exists () or ( save_dir / \"predict_configuration.yaml\" ) . exists () ) and not values [ \"overwrite\" ]: raise ValueError ( f \"zamba_predictions.csv and/or predict_configuration.yaml already exist in { save_dir } . If you would like to overwrite, set overwrite=True\" ) # make a directory if needed save_dir . mkdir ( parents = True , exist_ok = True ) # set save to True if save_dir is set if not save : save = True values [ \"save_dir\" ] = save_dir values [ \"save\" ] = save return values SchedulerConfig ( ZambaBaseModel ) pydantic-model \u00b6 Configuration containing parameters for a custom pytorch learning rate scheduler. See https://pytorch.org/docs/stable/optim.html for options. Parameters: Name Type Description Default scheduler str Name of learning rate scheduler to use. See https://pytorch.org/docs/stable/optim.html for options. required scheduler_params dict Parameters passed to learning rate scheduler upon initialization (eg. {\"milestones\": [1], \"gamma\": 0.5, \"verbose\": True}). Defaults to None. required scheduler : str pydantic-field \u00b6 scheduler_params : dict pydantic-field \u00b6 Config inherited \u00b6 extra \u00b6 use_enum_values \u00b6 validate_assignment \u00b6 validate_scheduler ( scheduler ) classmethod \u00b6 Source code in zamba/models/config.py @validator ( \"scheduler\" , always = True ) def validate_scheduler ( cls , scheduler ): if scheduler is None : return None elif scheduler not in torch . optim . lr_scheduler . __dict__ . keys (): raise ValueError ( \"Scheduler is not a `torch.optim.lr_scheduler`. \" \"See https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py \" \"for options.\" ) else : return scheduler TrainConfig ( ZambaBaseModel ) pydantic-model \u00b6 Configuration for training a model. Parameters: Name Type Description Default labels FilePath or pandas DataFrame Path to a CSV or pandas DataFrame containing labels for training, with one row per label. There must be columns called 'filepath' (absolute or relative to the data_dir) and 'label', and optionally columns called 'split' (\"train\", \"val\", or \"holdout\") and 'site'. Labels must be specified to train a model. required data_dir DirectoryPath Path to a directory containing training videos. Defaults to the working directory. required model_name str Name of the model to use for training. Options are: time_distributed, slowfast, european. Defaults to time_distributed. required checkpoint FilePath Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to resume training. If None, defaults to a pretrained model. Defaults to None. required scheduler_config SchedulerConfig or str Config for setting up the learning rate scheduler on the model. If \"default\", uses scheduler that was used for training. If None, will not use a scheduler. Defaults to \"default\". required dry_run bool or int, Optional Run one training and validation batch for one epoch to detect any bugs prior to training the full model. Disables tuners, checkpoint callbacks, loggers, and logger callbacks. Defaults to False. required batch_size int Batch size to use for training. Defaults to 2. required auto_lr_find bool Use a learning rate finder algorithm when calling trainer.tune() to try to find an optimal initial learning rate. Defaults to False. The learning rate finder is not guaranteed to find a good learning rate; depending on the dataset, it can select a learning rate that leads to poor model training. Use with caution. required backbone_finetune_params BackboneFinetuneConfig Set parameters to finetune a backbone model to align with the current learning rate. Defaults to a BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5, backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False, train_bn=False, verbose=True). required gpus int Number of GPUs to train on applied per node. Defaults to all of the available GPUs found on the machine. required num_workers int Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. The maximum value is the number of CPUs in the system. Defaults to 3. required max_epochs int Stop training once this number of epochs is reached. Disabled by default (None), which stops training at 1000 epochs. required early_stopping_config EarlyStoppingConfig Configuration for early stopping, which monitors a metric during training and stops training when the metric stops improving. Defaults to EarlyStoppingConfig(monitor='val_macro_f1', patience=5, verbose=True, mode='max'). required weight_download_region str s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (European Union), or \"asia\" (Asia Pacific). Defaults to \"us\". required split_proportions dict Proportions used to divide data into training, validation, and holdout sets if a if a \"split\" column is not included in labels. Defaults to \"train\": 3, \"val\": 1, \"holdout\": 1. required save_dir Path Path to a directory where training files will be saved. Files include the best model checkpoint ( model_name .ckpt), training configuration (configuration.yaml), Tensorboard logs (events.out.tfevents...), test metrics (test_metrics.json), validation metrics (val_metrics.json), and model hyperparameters (hparams.yml). If None, a folder is created in the working directory. Defaults to None. required overwrite bool If True, will save outputs in save_dir overwriting if those exist. If False, will create auto-incremented version_n folder in save_dir with model outputs. Defaults to False. required from_scratch bool Instantiate the model with base weights. This means starting with ImageNet weights for image-based models (time_distributed and european) and Kinetics weights for video-based models (slowfast). Defaults to False. required predict_all_zamba_species bool Output all zamba species rather than only the species in the labels file. required model_cache_dir Path Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, uses your default cache directory. Defaults to None. required auto_lr_find : bool pydantic-field \u00b6 backbone_finetune_config : BackboneFinetuneConfig pydantic-field \u00b6 batch_size : int pydantic-field \u00b6 checkpoint : FilePath pydantic-field \u00b6 data_dir : DirectoryPath pydantic-field \u00b6 dry_run : Union [ bool , int ] pydantic-field \u00b6 early_stopping_config : EarlyStoppingConfig pydantic-field \u00b6 from_scratch : bool pydantic-field \u00b6 gpus : int pydantic-field \u00b6 labels : Union [ pydantic . types . FilePath , pandas . core . frame . DataFrame ] pydantic-field required \u00b6 max_epochs : int pydantic-field \u00b6 model_cache_dir : Path pydantic-field \u00b6 model_name : ModelEnum pydantic-field \u00b6 num_workers : int pydantic-field \u00b6 overwrite : bool pydantic-field \u00b6 predict_all_zamba_species : bool pydantic-field \u00b6 save_dir : Path pydantic-field \u00b6 scheduler_config : Union [ str , zamba . models . config . SchedulerConfig ] pydantic-field \u00b6 skip_load_validation : bool pydantic-field \u00b6 split_proportions : Dict [ str , int ] pydantic-field \u00b6 weight_download_region : RegionEnum pydantic-field \u00b6 Config \u00b6 preprocess_labels ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def preprocess_labels ( cls , values ): logger . info ( \"Preprocessing labels into one hot encoded labels with one row per video.\" ) # one hot encode collapse to one row per video labels = ( pd . get_dummies ( values [ \"labels\" ] . rename ( columns = { \"label\" : \"species\" }), columns = [ \"species\" ] ) . groupby ( \"filepath\" ) . max () ) # if no \"split\" column, set up train, val, and holdout split if \"split\" not in labels . columns : logger . info ( f \"Dividing videos into train, val, and holdout sets using the following split proportions: { values [ 'split_proportions' ] } .\" ) # use site info if we have it if \"site\" in labels . columns : logger . info ( \"Using provided 'site' column to do a site-specific split\" ) labels [ \"split\" ] = create_site_specific_splits ( labels [ \"site\" ], proportions = values [ \"split_proportions\" ] ) else : logger . info ( \"No 'site' column found so videos will be randomly allocated to splits.\" ) # otherwise randomly allocate random . seed ( SPLIT_SEED ) labels [ \"split\" ] = random . choices ( list ( values [ \"split_proportions\" ] . keys ()), weights = list ( values [ \"split_proportions\" ] . values ()), k = len ( labels ), ) logger . info ( f \"Writing out split information to { values [ 'save_dir' ] / 'splits.csv' } .\" ) # create the directory to save if we need to. values [ \"save_dir\" ] . mkdir ( parents = True , exist_ok = True ) labels . reset_index ()[[ \"filepath\" , \"split\" ]] . drop_duplicates () . to_csv ( values [ \"save_dir\" ] / \"splits.csv\" , index = False ) # filepath becomes column instead of index values [ \"labels\" ] = labels . reset_index () return values turn_off_load_validation_if_dry_run ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def turn_off_load_validation_if_dry_run ( cls , values ): if values [ \"dry_run\" ] and not values [ \"skip_load_validation\" ]: logger . info ( \"Turning off video loading check since dry_run=True.\" ) values [ \"skip_load_validation\" ] = True return values validate_filepaths_and_labels ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def validate_filepaths_and_labels ( cls , values ): logger . info ( \"Validating labels csv.\" ) labels = ( pd . read_csv ( values [ \"labels\" ]) if not isinstance ( values [ \"labels\" ], pd . DataFrame ) else values [ \"labels\" ] ) if not set ([ \"label\" , \"filepath\" ]) . issubset ( labels . columns ): raise ValueError ( f \" { values [ 'labels' ] } must contain `filepath` and `label` columns.\" ) # validate split column has no partial nulls or invalid values if \"split\" in labels . columns : # if split is entirely null, warn, drop column, and generate splits automatically if labels . split . isnull () . all (): logger . warning ( \"Split column is entirely null. Will generate splits automatically using `split_proportions`.\" ) labels = labels . drop ( \"split\" , axis = 1 ) # error if split column has null values elif labels . split . isnull () . any (): raise ValueError ( f \"Found { labels . split . isnull () . sum () } row(s) with null `split`. Fill in these rows with either `train`, `val`, or `holdout`. Alternatively, do not include a `split` column in your labels and we'll generate splits for you using `split_proportions`.\" ) # otherwise check that split values are valid elif not set ( labels . split ) . issubset ({ \"train\" , \"val\" , \"holdout\" }): raise ValueError ( f \"Found the following invalid values for `split`: { set ( labels . split ) . difference ({ 'train' , 'val' , 'holdout' }) } . `split` can only contain `train`, `val`, or `holdout.`\" ) elif values [ \"split_proportions\" ] is not None : logger . warning ( \"Labels contains split column yet split_proprtions are also provided. Split column in labels takes precendece.\" ) # set to None for clarity in final configuration.yaml values [ \"split_proportions\" ] = None # error if labels are entirely null null_labels = labels . label . isnull () if sum ( null_labels ) == len ( labels ): raise ValueError ( \"Species cannot be null for all videos.\" ) # skip and warn about any videos without species label elif sum ( null_labels ) > 0 : logger . warning ( f \"Found { sum ( null_labels ) } filepath(s) with no label. Will skip.\" ) labels = labels [ ~ null_labels ] # check that all videos exist and can be loaded values [ \"labels\" ] = check_files_exist_and_load ( df = labels , data_dir = values [ \"data_dir\" ], skip_load_validation = values [ \"skip_load_validation\" ], ) return values validate_from_scratch_and_checkpoint ( values ) classmethod \u00b6 Source code in zamba/models/config.py @root_validator ( skip_on_failure = True ) def validate_from_scratch_and_checkpoint ( cls , values ): if values [ \"from_scratch\" ]: if values [ \"checkpoint\" ] is not None : raise ValueError ( \"If from_scratch=True, you cannot specify a checkpoint.\" ) if values [ \"model_name\" ] is None : raise ValueError ( \"If from_scratch=True, model_name cannot be None.\" ) return values validate_scheduler_config ( scheduler_config ) classmethod \u00b6 Source code in zamba/models/config.py @validator ( \"scheduler_config\" , always = True ) def validate_scheduler_config ( cls , scheduler_config ): if scheduler_config is None : return SchedulerConfig ( scheduler = None ) elif isinstance ( scheduler_config , str ) and scheduler_config != \"default\" : raise ValueError ( \"Scheduler can either be 'default', None, or a SchedulerConfig.\" ) else : return scheduler_config ZambaBaseModel ( BaseModel ) pydantic-model \u00b6 Set defaults for all models that inherit from the pydantic base model. Config \u00b6 extra \u00b6 use_enum_values \u00b6 validate_assignment \u00b6 Functions \u00b6 check_files_exist_and_load ( df : DataFrame , data_dir : DirectoryPath , skip_load_validation : bool ) \u00b6 Check whether files in file list exist and can be loaded with ffmpeg. Warn and skip files that don't exist or can't be loaded. Parameters: Name Type Description Default df pd.DataFrame DataFrame with a \"filepath\" column required data_dir Path Data folder to prepend if filepath is not an absolute path. required skip_load_validation bool Skip ffprobe check that verifies all videos can be loaded. required Returns: Type Description pd.DataFrame DataFrame with valid and loadable videos. Source code in zamba/models/config.py def check_files_exist_and_load ( df : pd . DataFrame , data_dir : DirectoryPath , skip_load_validation : bool ): \"\"\"Check whether files in file list exist and can be loaded with ffmpeg. Warn and skip files that don't exist or can't be loaded. Args: df (pd.DataFrame): DataFrame with a \"filepath\" column data_dir (Path): Data folder to prepend if filepath is not an absolute path. skip_load_validation (bool): Skip ffprobe check that verifies all videos can be loaded. Returns: pd.DataFrame: DataFrame with valid and loadable videos. \"\"\" # update filepath column to prepend data_dir if filepath column is not an absolute path data_dir = Path ( data_dir ) . resolve () df [ \"filepath\" ] = str ( data_dir ) / df . filepath . path # we can have multiple rows per file with labels so limit just to one row per file for these checks files_df = df [[ \"filepath\" ]] . drop_duplicates () # check data exists logger . info ( f \"Checking all { len ( files_df ) : , } filepaths exist. Can take up to a minute for every couple thousand files.\" ) invalid_files = files_df [ ~ files_df . filepath . path . exists ()] # if no files exist if len ( invalid_files ) == len ( files_df ): raise ValueError ( f \"None of the video filepaths exist. Are you sure they're specified correctly? Here's an example invalid path: { invalid_files . filepath . values [ 0 ] } . Either specify absolute filepaths in the csv or provide filepaths relative to `data_dir`.\" ) # if at least some files exist elif len ( invalid_files ) > 0 : logger . debug ( f \"The following files could not be found: { '/n' . join ( invalid_files . filepath . values . tolist ()) } \" ) logger . warning ( f \"Skipping { len ( invalid_files ) } file(s) that could not be found. For example, { invalid_files . filepath . values [ 0 ] } .\" ) # remove invalid files to prep for ffprobe check on remaining files_df = files_df [ ~ files_df . filepath . isin ( invalid_files . filepath )] bad_load = [] if not skip_load_validation : logger . info ( \"Checking that all videos can be loaded. If you're very confident all your videos can be loaded, you can skip this with `skip_load_validation`, but it's not recommended.\" ) # ffprobe check for f in tqdm ( files_df . filepath ): try : ffmpeg . probe ( str ( f )) except ffmpeg . Error as exc : logger . debug ( ZambaFfmpegException ( exc . stderr )) bad_load . append ( f ) if len ( bad_load ) > 0 : logger . warning ( f \"Skipping { len ( bad_load ) } file(s) that could not be loaded with ffmpeg.\" ) df = df [ ( ~ df . filepath . isin ( bad_load )) & ( ~ df . filepath . isin ( invalid_files . filepath )) ] . reset_index ( drop = True ) return df validate_gpus ( gpus : int ) \u00b6 Ensure the number of GPUs requested is equal to or less than the number of GPUs available on the machine. Source code in zamba/models/config.py def validate_gpus ( gpus : int ): \"\"\"Ensure the number of GPUs requested is equal to or less than the number of GPUs available on the machine.\"\"\" if gpus > GPUS_AVAILABLE : raise ValueError ( f \"Found only { GPUS_AVAILABLE } GPU(s). Cannot use { gpus } .\" ) else : return gpus validate_model_cache_dir ( model_cache_dir : Optional [ pathlib . Path ]) \u00b6 Set up cache directory for downloading model weight. Order of priority is: config argument, environment variable, or user's default cache dir. Source code in zamba/models/config.py def validate_model_cache_dir ( model_cache_dir : Optional [ Path ]): \"\"\"Set up cache directory for downloading model weight. Order of priority is: config argument, environment variable, or user's default cache dir. \"\"\" if model_cache_dir is None : model_cache_dir = os . getenv ( \"MODEL_CACHE_DIR\" , Path ( appdirs . user_cache_dir ()) / \"zamba\" ) model_cache_dir = Path ( model_cache_dir ) model_cache_dir . mkdir ( parents = True , exist_ok = True ) return model_cache_dir validate_model_name_and_checkpoint ( cls , values ) \u00b6 Ensures a checkpoint file or model name is provided. If a model name is provided, looks up the corresponding public checkpoint file from the official configs. Source code in zamba/models/config.py def validate_model_name_and_checkpoint ( cls , values ): \"\"\"Ensures a checkpoint file or model name is provided. If a model name is provided, looks up the corresponding public checkpoint file from the official configs. \"\"\" checkpoint = values . get ( \"checkpoint\" ) model_name = values . get ( \"model_name\" ) # must specify either checkpoint or model name if checkpoint is None and model_name is None : raise ValueError ( \"Must provide either model_name or checkpoint path.\" ) # checkpoint supercedes model elif checkpoint is not None and model_name is not None : logger . info ( f \"Using checkpoint file: { checkpoint } .\" ) # set model name to None so proper model class is retrieved from ckpt up upon instantiation values [ \"model_name\" ] = None elif checkpoint is None and model_name is not None : if not values . get ( \"from_scratch\" ): # get public weights file from official models config values [ \"checkpoint\" ] = get_model_checkpoint_filename ( model_name ) return values","title":"zamba.models.config"},{"location":"api-reference/models-config/#zambamodelsconfig","text":"","title":"zamba.models.config"},{"location":"api-reference/models-config/#zamba.models.config-classes","text":"","title":"Classes"},{"location":"api-reference/models-config/#zamba.models.config-functions","text":"","title":"Functions"},{"location":"api-reference/models-efficientnet_models/","text":"zamba.models.efficientnet_models \u00b6 Classes \u00b6 TimeDistributedEfficientNet ( ZambaVideoClassificationLightningModule ) \u00b6 Attributes \u00b6 CHECKPOINT_HYPER_PARAMS_KEY inherited \u00b6 CHECKPOINT_HYPER_PARAMS_NAME inherited \u00b6 CHECKPOINT_HYPER_PARAMS_TYPE inherited \u00b6 T_destination inherited \u00b6 automatic_optimization : bool inherited property writable \u00b6 If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch : int inherited property readonly \u00b6 The current epoch in the Trainer. If no Trainer is attached, this propery is 0. datamodule : Any inherited property writable \u00b6 device : Union [ str , torch . device ] inherited property readonly \u00b6 dtype : Union [ str , torch . dtype ] inherited property writable \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. example_input_array : Any inherited property writable \u00b6 The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank : int inherited property readonly \u00b6 The index of the current process across all nodes and devices. global_step : int inherited property readonly \u00b6 Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams : Union [ pytorch_lightning . utilities . parsing . AttributeDict , dict , argparse . Namespace ] inherited property readonly \u00b6 hparams_initial : AttributeDict inherited property readonly \u00b6 loaded_optimizer_states_dict : dict inherited property writable \u00b6 local_rank : int inherited property readonly \u00b6 The index of the current process within a single node. logger inherited property readonly \u00b6 Reference to the logger object in the Trainer. model_size : float inherited property readonly \u00b6 The model's size in megabytes. The computation includes everything in the :meth: ~torch.nn.Module.state_dict , i.e., by default the parameteters and buffers. on_gpu inherited property readonly \u00b6 Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps : int inherited property writable \u00b6 Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the :meth: training_step receives an additional argument hiddens and is expected to return a hidden state. Methods \u00b6 __init__ ( self , num_frames = 16 , finetune_from : Union [ str , os . PathLike ] = None , ** kwargs ) special \u00b6 Source code in zamba/models/efficientnet_models.py def __init__ ( self , num_frames = 16 , finetune_from : Optional [ Union [ os . PathLike , str ]] = None , ** kwargs ): super () . __init__ ( ** kwargs ) if finetune_from is None : efficientnet = timm . create_model ( \"efficientnetv2_rw_m\" , pretrained = True ) efficientnet . classifier = nn . Identity () else : efficientnet = self . load_from_checkpoint ( finetune_from ) . base . module # freeze base layers for param in efficientnet . parameters (): param . requires_grad = False num_backbone_final_features = efficientnet . num_features self . backbone = torch . nn . ModuleList ( [ efficientnet . get_submodule ( \"blocks.5\" ), efficientnet . conv_head , efficientnet . bn2 , efficientnet . act2 , efficientnet . global_pool , ] ) self . base = TimeDistributed ( efficientnet , tdim = 1 ) self . classifier = nn . Sequential ( nn . Linear ( num_backbone_final_features , 256 ), nn . Dropout ( 0.2 ), nn . ReLU (), nn . Linear ( 256 , 64 ), nn . Flatten (), nn . Linear ( 64 * num_frames , self . num_classes ), ) self . save_hyperparameters ( \"num_frames\" ) add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/models/efficientnet_models.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7f45559664f0 >> ) -> None inherited \u00b6 Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Parameters: Name Type Description Default queue <bound method BaseContext.SimpleQueue of <multiprocessing.context.DefaultContext object at 0x7f45559664f0>> the instance of the queue to append the data. required Source code in zamba/models/efficientnet_models.py def add_to_queue ( self , queue : torch . multiprocessing . SimpleQueue ) -> None : \"\"\" Appends the :attr:`trainer.callback_metrics` dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. \"\"\" callback_metrics : dict = apply_to_collection ( self . trainer . callback_metrics , torch . Tensor , lambda x : x . cpu () . numpy () ) # send as numpy to avoid issues with memory sharing queue . put ( callback_metrics ) aggregate_step_outputs ( outputs : Dict [ str , numpy . ndarray ]) -> Tuple [ numpy . ndarray , numpy . ndarray , numpy . ndarray ] inherited \u00b6 Source code in zamba/models/efficientnet_models.py @staticmethod def aggregate_step_outputs ( outputs : Dict [ str , np . ndarray ] ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: y_true = np . vstack ([ output [ \"y_true\" ] for output in outputs ]) y_pred = np . vstack ([ output [ \"y_pred\" ] for output in outputs ]) y_proba = np . vstack ([ output [ \"y_proba\" ] for output in outputs ]) return y_true , y_pred , y_proba all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) inherited \u00b6 Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Parameters: Name Type Description Default data Union[torch.Tensor, Dict, List, Tuple] int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. required group Optional[Any] the process group to gather results from. Defaults to all processes (world) None sync_grads bool flag that allows users to synchronize gradients for the all_gather operation False Returns: Type Description A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. Source code in zamba/models/efficientnet_models.py def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ): r \"\"\" Allows users to call ``self.all_gather()`` from the LightningModule, thus making the ``all_gather`` operation accelerator agnostic. ``all_gather`` is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. \"\"\" group = group if group is not None else torch . distributed . group . WORLD all_gather = self . trainer . accelerator . all_gather data = convert_to_tensors ( data , device = self . device ) return apply_to_collection ( data , torch . Tensor , all_gather , group = group , sync_grads = sync_grads ) apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/models/efficientnet_models.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self backward ( self , loss : Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None inherited \u00b6 Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss Tensor The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). required optimizer Optional[torch.optim.optimizer.Optimizer] Current optimizer being used. None if using manual optimization. required optimizer_idx Optional[int] Index of the current optimizer being used. None if using manual optimization. required Example:: def backward(self, loss, optimizer, optimizer_idx): loss.backward() Source code in zamba/models/efficientnet_models.py def backward ( self , loss : Tensor , optimizer : Optional [ Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None : \"\"\" Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own implementation if you need to. Args: loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). optimizer: Current optimizer being used. ``None`` if using manual optimization. optimizer_idx: Index of the current optimizer being used. ``None`` if using manual optimization. Example:: def backward(self, loss, optimizer, optimizer_idx): loss.backward() \"\"\" loss . backward ( * args , ** kwargs ) bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/models/efficientnet_models.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/models/efficientnet_models.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module compute_and_log_metrics ( self , y_true : ndarray , y_pred : ndarray , y_proba : ndarray , subset : str ) inherited \u00b6 Source code in zamba/models/efficientnet_models.py def compute_and_log_metrics ( self , y_true : np . ndarray , y_pred : np . ndarray , y_proba : np . ndarray , subset : str ): self . log ( f \" { subset } _macro_f1\" , f1_score ( y_true , y_pred , average = \"macro\" , zero_division = 0 )) # if only two classes, skip top_k accuracy since not enough classes if self . num_classes > 2 : for k in DEFAULT_TOP_K : if k < self . num_classes : self . log ( f \" { subset } _top_ { k } _accuracy\" , top_k_accuracy_score ( y_true . argmax ( axis = 1 ), # top k accuracy only supports single label case y_proba , labels = np . arange ( y_proba . shape [ 1 ]), k = k , ), ) else : self . log ( f \" { subset } _accuracy\" , accuracy_score ( y_true , y_pred )) for metric_name , label , metric in compute_species_specific_metrics ( y_true , y_pred , self . species ): self . log ( f \"species/ { subset } _ { metric_name } / { label } \" , metric ) configure_callbacks ( self ) inherited \u00b6 Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Returns: Type Description A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] !!! note Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here. Source code in zamba/models/efficientnet_models.py def configure_callbacks ( self ): \"\"\" Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth:`~pytorch_lightning.callbacks.base.Callback.on_init_start` will never be invoked on the new callbacks returned here. \"\"\" return [] configure_optimizers ( self ) inherited \u00b6 Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. Source code in zamba/models/efficientnet_models.py def configure_optimizers ( self ): \"\"\" Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. \"\"\" optim = self . _get_optimizer () if self . scheduler is None : return optim else : return { \"optimizer\" : optim , \"lr_scheduler\" : self . scheduler ( optim , ** ({} if self . scheduler_params is None else self . scheduler_params ) ), } configure_sharded_model ( self ) -> None inherited \u00b6 Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. The accelerator manages whether to call this hook at every given stage. For sharded plugins where model parallelism is required, the hook is usually on called once to initialize the sharded parameters, and not called again in the same process. By default for accelerators/plugins that do not use model sharding techniques, this hook is called during each fit/val/test/predict stages. Source code in zamba/models/efficientnet_models.py def configure_sharded_model ( self ) -> None : \"\"\" Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. The accelerator manages whether to call this hook at every given stage. For sharded plugins where model parallelism is required, the hook is usually on called once to initialize the sharded parameters, and not called again in the same process. By default for accelerators/plugins that do not use model sharding techniques, this hook is called during each fit/val/test/predict stages. \"\"\" cpu ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves all model parameters and buffers to the CPU. Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def cpu ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" self . __update_properties ( device = torch . device ( \"cpu\" )) return super () . cpu () cuda ( self , device : Union [ torch . device , int ] = None ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device Union[torch.device, int] if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def cuda ( self , device : Optional [ Union [ torch . device , int ]] = None ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self \"\"\" if device is None or isinstance ( device , int ): device = torch . device ( \"cuda\" , index = device ) self . __update_properties ( device = device ) return super () . cuda ( device = device ) double ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def double ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . double ) return super () . double () eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/models/efficientnet_models.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def float ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . float ) return super () . float () forward ( self , x ) \u00b6 Same as :meth: torch.nn.Module.forward() . Parameters: Name Type Description Default *args Whatever you decide to pass into the forward method. required **kwargs Keyword arguments are also possible. required Returns: Type Description Your model's output Source code in zamba/models/efficientnet_models.py def forward ( self , x ): self . base . eval () x = self . base ( x ) return self . classifier ( x ) freeze ( self ) -> None inherited \u00b6 Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() Source code in zamba/models/efficientnet_models.py def freeze ( self ) -> None : r \"\"\" Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() \"\"\" for param in self . parameters (): param . requires_grad = False self . eval () get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/models/efficientnet_models.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/models/efficientnet_models.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7f45559664f0 >> ) -> None inherited \u00b6 Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Parameters: Name Type Description Default queue <bound method BaseContext.SimpleQueue of <multiprocessing.context.DefaultContext object at 0x7f45559664f0>> the instance of the queue from where to get the data. required Source code in zamba/models/efficientnet_models.py def get_from_queue ( self , queue : torch . multiprocessing . SimpleQueue ) -> None : \"\"\" Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we cast back the data to ``torch.Tensor``. Args: queue: the instance of the queue from where to get the data. \"\"\" # NOTE: `add_to_queue` needs to be called before callback_metrics : dict = queue . get () self . trainer . callback_metrics . update ( apply_to_collection ( callback_metrics , np . ndarray , lambda x : torch . tensor ( x )) ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/models/efficientnet_models.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] inherited \u00b6 Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. .. code-block:: Epoch 1: 4%|\u258e | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10] Here is an example how to override the defaults: .. code-block:: python def get_progress_bar_dict(self): # don't show the version number items = super().get_progress_bar_dict() items.pop(\"v_num\", None) return items Returns: Type Description Dict[str, Union[int, str]] Dictionary with the items to be displayed in the progress bar. Source code in zamba/models/efficientnet_models.py def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]: r \"\"\" Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. .. code-block:: Epoch 1: 4%|\u258e | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10] Here is an example how to override the defaults: .. code-block:: python def get_progress_bar_dict(self): # don't show the version number items = super().get_progress_bar_dict() items.pop(\"v_num\", None) return items Return: Dictionary with the items to be displayed in the progress bar. \"\"\" # call .item() only once but store elements without graphs running_train_loss = self . trainer . fit_loop . running_loss . mean () avg_training_loss = None if running_train_loss is not None : avg_training_loss = running_train_loss . cpu () . item () elif self . automatic_optimization : avg_training_loss = float ( \"NaN\" ) tqdm_dict = {} if avg_training_loss is not None : tqdm_dict [ \"loss\" ] = f \" { avg_training_loss : .3g } \" module_tbptt_enabled = self . truncated_bptt_steps > 0 trainer_tbptt_enabled = self . trainer . truncated_bptt_steps is not None and self . trainer . truncated_bptt_steps > 0 if module_tbptt_enabled or trainer_tbptt_enabled : tqdm_dict [ \"split_idx\" ] = self . trainer . fit_loop . split_idx if self . trainer . logger is not None and self . trainer . logger . version is not None : version = self . trainer . logger . version # show last 4 places of long version strings version = version [ - 4 :] if isinstance ( version , str ) else version tqdm_dict [ \"v_num\" ] = version return tqdm_dict get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/models/efficientnet_models.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ] inherited \u00b6 Compute each parameter's gradient's norm and their overall norm. .. deprecated:: v1.3 Will be removed in v1.5.0. Use :func: pytorch_lightning.utilities.grads.grad_norm instead. Source code in zamba/models/efficientnet_models.py def grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ]: \"\"\"Compute each parameter's gradient's norm and their overall norm. .. deprecated:: v1.3 Will be removed in v1.5.0. Use :func:`pytorch_lightning.utilities.grads.grad_norm` instead. \"\"\" rank_zero_deprecation ( \"LightningModule.grad_norm is deprecated in v1.3 and will be removed in v1.5.\" \" Use grad_norm from pytorch_lightning.utilities.grads instead.\" ) return new_grad_norm ( self , norm_type ) half ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def half ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . half ) return super () . half () load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/models/efficientnet_models.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None inherited \u00b6 Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name str key to log required value Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number, Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number]]] value to log. Can be a float , Tensor , Metric , or a dictionary of the former. required prog_bar bool if True logs to the progress bar False logger bool if True logs to the logger True on_step Optional[bool] if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch Optional[bool] if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx Union[str, Callable] reduction function over step values for end of epoch. :meth: torch.mean by default. 'default' enable_graph bool if True, will not auto detach the graph False sync_dist bool if True, reduces the metric across GPUs/TPUs False sync_dist_group Optional[Any] the ddp group to sync across None add_dataloader_idx bool if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values True batch_size Optional[int] Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute Optional[str] To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only Optional[bool] Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None Source code in zamba/models/efficientnet_models.py def log ( self , name : str , value : _METRIC_COLLECTION , prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = \"default\" , # TODO: change to 'mean' when `sync_dist_op` is removed in 1.6 tbptt_reduce_fx : Optional = None , # noqa: Remove in 1.6 tbptt_pad_token : Optional = None , # noqa: Remove in 1.6 enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , # noqa: Remove in 1.6 sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None , ) -> None : \"\"\" Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: ``*`` also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step*\", \"F\", \"T\", \"F\", \"T\" \"validation_step_end*\", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Args: name: key to log value: value to log. Can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former. prog_bar: if True logs to the progress bar logger: if True logs to the logger on_step: if True logs at this step. None auto-logs at the training_step but not validation/test_step on_epoch: if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default. enable_graph: if True, will not auto detach the graph sync_dist: if True, reduces the metric across GPUs/TPUs sync_dist_group: the ddp group to sync across add_dataloader_idx: if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values batch_size: Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. metric_attribute: To restore the metric state, Lightning requires the reference of the :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute. rank_zero_only: Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. \"\"\" if tbptt_reduce_fx is not None : rank_zero_deprecation ( \"`self.log(tbptt_reduce_fx=...)` is no longer supported. The flag will be removed in v1.6.\" \" Please, open a discussion explaining your use-case in\" \" `https://github.com/PyTorchLightning/pytorch-lightning/discussions`\" ) if tbptt_pad_token is not None : rank_zero_deprecation ( \"`self.log(tbptt_pad_token=...)` is no longer supported. The flag will be removed in v1.6.\" \" Please, open a discussion explaining your use-case in\" \" `https://github.com/PyTorchLightning/pytorch-lightning/discussions`\" ) if sync_dist_op is not None : rank_zero_deprecation ( f \"`self.log(sync_dist_op=' { sync_dist_op } ')` is deprecated and will be removed in v.1.6.\" f \" Use `self.log(reduce_fx= { sync_dist_op } )` instead.\" ) if reduce_fx == \"default\" : reduce_fx = sync_dist_op elif reduce_fx == \"default\" : reduce_fx = \"mean\" # check for invalid values apply_to_collection ( value , dict , self . __check_not_nested , name ) apply_to_collection ( value , object , self . __check_allowed , name , value , wrong_dtype = ( numbers . Number , Metric , Tensor , dict ) ) # set the default depending on the fx_name on_step = self . __auto_choose_log_on_step ( on_step ) on_epoch = self . __auto_choose_log_on_epoch ( on_epoch ) results = self . trainer . _results assert results is not None assert self . _current_fx_name is not None FxValidator . check_logging ( self . _current_fx_name , on_step = on_step , on_epoch = on_epoch ) # make sure user doesn't introduce logic for multi-dataloaders if \"/dataloader_idx_\" in name : raise MisconfigurationException ( f \"You called `self.log` with the key ` { name } `\" \" but it should not contain information about `dataloader_idx`\" ) value = apply_to_collection ( value , numbers . Number , self . __to_tensor ) if self . trainer . logger_connector . should_reset_tensors ( self . _current_fx_name ): # if we started a new epoch (running it's first batch) the hook name has changed # reset any tensors for the new hook name results . reset ( metrics = False , fx = self . _current_fx_name ) if metric_attribute is None and isinstance ( value , Metric ): if self . _metric_attributes is None : # compute once self . _metric_attributes = { id ( module ): name for name , module in self . named_modules () if isinstance ( module , Metric ) } if not self . _metric_attributes : raise MisconfigurationException ( \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\" \" You can fix this by setting an attribute for the metric in your `LightningModule`.\" ) # try to find the passed metric in the LightningModule metric_attribute = self . _metric_attributes . get ( id ( value ), None ) if metric_attribute is None : raise MisconfigurationException ( \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\" f \" You can fix this by calling `self.log( { name } , ..., metric_attribute=name)` where `name` is one\" f \" of { list ( self . _metric_attributes . values ()) } \" ) results . log ( self . _current_fx_name , name , value , prog_bar = prog_bar , logger = logger , on_step = on_step , on_epoch = on_epoch , reduce_fx = reduce_fx , enable_graph = enable_graph , dataloader_idx = ( self . _current_dataloader_idx if add_dataloader_idx else None ), batch_size = batch_size , sync_dist = sync_dist and distributed_available (), sync_dist_fn = self . trainer . training_type_plugin . reduce or sync_ddp , sync_dist_group = sync_dist_group , metric_attribute = metric_attribute , rank_zero_only = rank_zero_only , ) self . trainer . logger_connector . _current_fx = self . _current_fx_name log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True ) -> None inherited \u00b6 Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number, Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number]]]] key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. required prog_bar bool if True logs to the progress base False logger bool if True logs to the logger True on_step Optional[bool] if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch Optional[bool] if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx Union[str, Callable] reduction function over step values for end of epoch. :meth: torch.mean by default. 'default' enable_graph bool if True, will not auto detach the graph False sync_dist bool if True, reduces the metric across GPUs/TPUs False sync_dist_group Optional[Any] the ddp group sync across None add_dataloader_idx bool if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values True Source code in zamba/models/efficientnet_models.py def log_dict ( self , dictionary : Mapping [ str , _METRIC_COLLECTION ], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = \"default\" , # TODO: change to 'mean' when `sync_dist_op` is removed in 1.6 tbptt_reduce_fx : Optional [ Any ] = None , # noqa: Remove in 1.6 tbptt_pad_token : Optional [ Any ] = None , # noqa: Remove in 1.6 enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , # noqa: Remove in 1.6 sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , ) -> None : \"\"\" Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Args: dictionary: key value pairs. The values can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former. prog_bar: if True logs to the progress base logger: if True logs to the logger on_step: if True logs at this step. None auto-logs for training_step but not validation/test_step on_epoch: if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default. enable_graph: if True, will not auto detach the graph sync_dist: if True, reduces the metric across GPUs/TPUs sync_dist_group: the ddp group sync across add_dataloader_idx: if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values \"\"\" for k , v in dictionary . items (): self . log ( name = k , value = v , prog_bar = prog_bar , logger = logger , on_step = on_step , on_epoch = on_epoch , reduce_fx = reduce_fx , enable_graph = enable_graph , sync_dist = sync_dist , sync_dist_group = sync_dist_group , sync_dist_op = sync_dist_op , tbptt_pad_token = tbptt_pad_token , tbptt_reduce_fx = tbptt_reduce_fx , add_dataloader_idx = add_dataloader_idx , ) log_grad_norm ( self , grad_norm_dict : Dict [ str , torch . Tensor ]) -> None inherited \u00b6 Override this method to change the default behaviour of log_grad_norm . Parameters: Name Type Description Default grad_norm_dict Dict[str, torch.Tensor] Dictionary containing current grad norm metrics required Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) Source code in zamba/models/efficientnet_models.py def log_grad_norm ( self , grad_norm_dict : Dict [ str , torch . Tensor ]) -> None : \"\"\"Override this method to change the default behaviour of ``log_grad_norm``. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) \"\"\" self . log_dict ( grad_norm_dict , on_step = True , on_epoch = True , prog_bar = True , logger = True ) lr_schedulers ( self ) -> Union [ Any , List [ Any ]] inherited \u00b6 Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no schedulers were returned in meth: configure_optimizers . Source code in zamba/models/efficientnet_models.py def lr_schedulers ( self ) -> Optional [ Union [ Any , List [ Any ]]]: \"\"\" Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no schedulers were returned in :meth:`configure_optimizers`. \"\"\" if not self . trainer . lr_schedulers : return None # ignore other keys \"interval\", \"frequency\", etc. lr_schedulers = [ s [ \"scheduler\" ] for s in self . trainer . lr_schedulers ] # single scheduler if len ( lr_schedulers ) == 1 : return lr_schedulers [ 0 ] # multiple schedulers return lr_schedulers manual_backward ( self , loss : Tensor , * args , ** kwargs ) -> None inherited \u00b6 Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss Tensor The tensor on which to compute gradients. Must have a graph attached. required *args Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward () **kwargs Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward {} Source code in zamba/models/efficientnet_models.py def manual_backward ( self , loss : Tensor , * args , ** kwargs ) -> None : \"\"\" Call this directly from your :meth:`training_step` when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref:`manual optimization<common/optimizers:Manual optimization>` for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Args: loss: The tensor on which to compute gradients. Must have a graph attached. *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward` **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward` \"\"\" # make sure we're using manual opt self . _verify_is_manual_optimization ( \"manual_backward\" ) # backward self . trainer . fit_loop . epoch_loop . batch_loop . backward ( loss , None , None , * args , ** kwargs ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/models/efficientnet_models.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/models/efficientnet_models.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/models/efficientnet_models.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/models/efficientnet_models.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/models/efficientnet_models.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem on_after_backward ( self ) -> None inherited \u00b6 Called after loss.backward() and before optimizers are stepped. !!! note If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients. Source code in zamba/models/efficientnet_models.py def on_after_backward ( self ) -> None : \"\"\" Called after ``loss.backward()`` and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the ``on_before_optimizer_step`` if you need the unscaled gradients. \"\"\" on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any inherited \u00b6 Override to alter or apply batch augmentations to your batch after it is transferred to the device. !!! note To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be altered or augmented. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch See Also: - :meth: on_before_batch_transfer - :meth: transfer_batch_to_device Source code in zamba/models/efficientnet_models.py def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any : \"\"\" Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be altered or augmented. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`on_before_batch_transfer` - :meth:`transfer_batch_to_device` \"\"\" return batch on_before_backward ( self , loss : Tensor ) -> None inherited \u00b6 Called before loss.backward() . Parameters: Name Type Description Default loss Tensor Loss divided by number of batches for gradient accumulation and scaled if using native AMP. required Source code in zamba/models/efficientnet_models.py def on_before_backward ( self , loss : torch . Tensor ) -> None : \"\"\" Called before ``loss.backward()``. Args: loss: Loss divided by number of batches for gradient accumulation and scaled if using native AMP. \"\"\" pass on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any inherited \u00b6 Override to alter or apply batch augmentations to your batch before it is transferred to the device. !!! note To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be altered or augmented. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch See Also: - :meth: on_after_batch_transfer - :meth: transfer_batch_to_device Source code in zamba/models/efficientnet_models.py def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any : \"\"\" Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be altered or augmented. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`on_after_batch_transfer` - :meth:`transfer_batch_to_device` \"\"\" return batch on_before_optimizer_step ( self , optimizer : Optimizer , optimizer_idx : int ) -> None inherited \u00b6 Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. Parameters: Name Type Description Default optimizer Optimizer Current optimizer being used. required optimizer_idx int Index of the current optimizer being used. required Example:: def on_before_optimizer_step(self, optimizer, optimizer_idx): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) Source code in zamba/models/efficientnet_models.py def on_before_optimizer_step ( self , optimizer : Optimizer , optimizer_idx : int ) -> None : \"\"\" Called before ``optimizer.step()``. The hook is only called if gradients do not need to be accumulated. See: :paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`. If using native AMP, the loss will be unscaled before calling this hook. See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__ for more information on the scaling of gradients. Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. Example:: def on_before_optimizer_step(self, optimizer, optimizer_idx): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) \"\"\" on_before_zero_grad ( self , optimizer : Optimizer ) -> None inherited \u00b6 Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer Optimizer The optimizer for which grads should be zeroed. required Source code in zamba/models/efficientnet_models.py def on_before_zero_grad ( self , optimizer : Optimizer ) -> None : \"\"\" Called after ``training_step()`` and before ``optimizer.zero_grad()``. Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Args: optimizer: The optimizer for which grads should be zeroed. \"\"\" on_epoch_end ( self ) -> None inherited \u00b6 Called when either of train/val/test epoch ends. Source code in zamba/models/efficientnet_models.py def on_epoch_end ( self ) -> None : \"\"\" Called when either of train/val/test epoch ends. \"\"\" on_epoch_start ( self ) -> None inherited \u00b6 Called when either of train/val/test epoch begins. Source code in zamba/models/efficientnet_models.py def on_epoch_start ( self ) -> None : \"\"\" Called when either of train/val/test epoch begins. \"\"\" on_fit_end ( self ) -> None inherited \u00b6 Called at the very end of fit. If on DDP it is called on every process Source code in zamba/models/efficientnet_models.py def on_fit_end ( self ) -> None : \"\"\" Called at the very end of fit. If on DDP it is called on every process \"\"\" on_fit_start ( self ) -> None inherited \u00b6 Called at the very beginning of fit. If on DDP it is called on every process Source code in zamba/models/efficientnet_models.py def on_fit_start ( self ) -> None : \"\"\" Called at the very beginning of fit. If on DDP it is called on every process \"\"\" on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary with variables from the checkpoint. required Source code in zamba/models/efficientnet_models.py def on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\" on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. required Source code in zamba/models/efficientnet_models.py def on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\" on_load_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Do something with the checkpoint. Gives model a chance to load something before state_dict is restored. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary with variables from the checkpoint. required Source code in zamba/models/efficientnet_models.py def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Do something with the checkpoint. Gives model a chance to load something before ``state_dict`` is restored. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\" on_post_move_to_device ( self ) -> None inherited \u00b6 Called in the parameter_validation decorator after :meth: ~pytorch_lightning.core.LightningModule.to is called. This is a good place to tie weights between modules after moving them to a device. Can be used when training models with weight sharing properties on TPU. Addresses the handling of shared weights on TPU: https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks Example:: def on_post_move_to_device(self): self.decoder.weight = self.encoder.weight Source code in zamba/models/efficientnet_models.py def on_post_move_to_device ( self ) -> None : \"\"\" Called in the ``parameter_validation`` decorator after :meth:`~pytorch_lightning.core.LightningModule.to` is called. This is a good place to tie weights between modules after moving them to a device. Can be used when training models with weight sharing properties on TPU. Addresses the handling of shared weights on TPU: https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks Example:: def on_post_move_to_device(self): self.decoder.weight = self.encoder.weight \"\"\" on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the predict loop after the batch. Parameters: Name Type Description Default outputs Optional[Any] The outputs of predict_step_end(test_step(x)) required batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/efficientnet_models.py def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the predict loop after the batch. Args: outputs: The outputs of predict_step_end(test_step(x)) batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/efficientnet_models.py def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the predict loop before anything happens for that batch. Args: batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_predict_dataloader ( self ) -> None inherited \u00b6 Called before requesting the predict dataloader. .. deprecated:: v1.5 :meth: on_predict_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: predict_dataloader() directly. Source code in zamba/models/efficientnet_models.py def on_predict_dataloader ( self ) -> None : \"\"\"Called before requesting the predict dataloader. .. deprecated:: v1.5 :meth:`on_predict_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`predict_dataloader()` directly. \"\"\" on_predict_end ( self ) -> None inherited \u00b6 Called at the end of predicting. Source code in zamba/models/efficientnet_models.py def on_predict_end ( self ) -> None : \"\"\" Called at the end of predicting. \"\"\" on_predict_epoch_end ( self , results : List [ Any ]) -> None inherited \u00b6 Called at the end of predicting. Source code in zamba/models/efficientnet_models.py def on_predict_epoch_end ( self , results : List [ Any ]) -> None : \"\"\" Called at the end of predicting. \"\"\" on_predict_epoch_start ( self ) -> None inherited \u00b6 Called at the beginning of predicting. Source code in zamba/models/efficientnet_models.py def on_predict_epoch_start ( self ) -> None : \"\"\" Called at the beginning of predicting. \"\"\" on_predict_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the predict loop Source code in zamba/models/efficientnet_models.py def on_predict_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the predict loop \"\"\" self . trainer . model . eval () on_predict_start ( self ) -> None inherited \u00b6 Called at the beginning of predicting. Source code in zamba/models/efficientnet_models.py def on_predict_start ( self ) -> None : \"\"\" Called at the beginning of predicting. \"\"\" on_pretrain_routine_end ( self ) -> None inherited \u00b6 Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start Source code in zamba/models/efficientnet_models.py def on_pretrain_routine_end ( self ) -> None : \"\"\" Called at the end of the pretrain routine (between fit and train start). - fit - pretrain_routine start - pretrain_routine end - training_start \"\"\" on_pretrain_routine_start ( self ) -> None inherited \u00b6 Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start Source code in zamba/models/efficientnet_models.py def on_pretrain_routine_start ( self ) -> None : \"\"\" Called at the beginning of the pretrain routine (between fit and train start). - fit - pretrain_routine start - pretrain_routine end - training_start \"\"\" on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Give the model a chance to add something to the checkpoint. state_dict is already there. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. required Source code in zamba/models/efficientnet_models.py def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Give the model a chance to add something to the checkpoint. ``state_dict`` is already there. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\" on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the test loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of test_step_end(test_step(x)) required batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/efficientnet_models.py def on_test_batch_end ( self , outputs : Optional [ STEP_OUTPUT ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the test loop after the batch. Args: outputs: The outputs of test_step_end(test_step(x)) batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/efficientnet_models.py def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the test loop before anything happens for that batch. Args: batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_test_dataloader ( self ) -> None inherited \u00b6 Called before requesting the test dataloader. .. deprecated:: v1.5 :meth: on_test_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: test_dataloader() directly. Source code in zamba/models/efficientnet_models.py def on_test_dataloader ( self ) -> None : \"\"\"Called before requesting the test dataloader. .. deprecated:: v1.5 :meth:`on_test_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`test_dataloader()` directly. \"\"\" on_test_end ( self ) -> None inherited \u00b6 Called at the end of testing. Source code in zamba/models/efficientnet_models.py def on_test_end ( self ) -> None : \"\"\" Called at the end of testing. \"\"\" on_test_epoch_end ( self ) -> None inherited \u00b6 Called in the test loop at the very end of the epoch. Source code in zamba/models/efficientnet_models.py def on_test_epoch_end ( self ) -> None : \"\"\" Called in the test loop at the very end of the epoch. \"\"\" on_test_epoch_start ( self ) -> None inherited \u00b6 Called in the test loop at the very beginning of the epoch. Source code in zamba/models/efficientnet_models.py def on_test_epoch_start ( self ) -> None : \"\"\" Called in the test loop at the very beginning of the epoch. \"\"\" on_test_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the test loop Source code in zamba/models/efficientnet_models.py def on_test_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the test loop \"\"\" self . trainer . model . eval () on_test_model_train ( self ) -> None inherited \u00b6 Sets the model to train during the test loop Source code in zamba/models/efficientnet_models.py def on_test_model_train ( self ) -> None : \"\"\" Sets the model to train during the test loop \"\"\" self . trainer . model . train () on_test_start ( self ) -> None inherited \u00b6 Called at the beginning of testing. Source code in zamba/models/efficientnet_models.py def on_test_start ( self ) -> None : \"\"\" Called at the beginning of testing. \"\"\" on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the training loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of training_step_end(training_step(x)) required batch Any The batched data as it is returned by the training DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/efficientnet_models.py def on_train_batch_end ( self , outputs : STEP_OUTPUT , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the training loop after the batch. Args: outputs: The outputs of training_step_end(training_step(x)) batch: The batched data as it is returned by the training DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_train_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the training DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/efficientnet_models.py def on_train_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_train_dataloader ( self ) -> None inherited \u00b6 Called before requesting the train dataloader. .. deprecated:: v1.5 :meth: on_train_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: train_dataloader() directly. Source code in zamba/models/efficientnet_models.py def on_train_dataloader ( self ) -> None : \"\"\"Called before requesting the train dataloader. .. deprecated:: v1.5 :meth:`on_train_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`train_dataloader()` directly. \"\"\" on_train_end ( self ) -> None inherited \u00b6 Called at the end of training before logger experiment is closed. Source code in zamba/models/efficientnet_models.py def on_train_end ( self ) -> None : \"\"\" Called at the end of training before logger experiment is closed. \"\"\" on_train_epoch_end ( self , unused : Optional = None ) -> None inherited \u00b6 Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook Source code in zamba/models/efficientnet_models.py def on_train_epoch_end ( self , unused : Optional = None ) -> None : \"\"\" Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: 1. Implement `training_epoch_end` in the LightningModule OR 2. Cache data across steps on the attribute(s) of the `LightningModule` and access them in this hook \"\"\" on_train_epoch_start ( self ) -> None inherited \u00b6 Called in the training loop at the very beginning of the epoch. Source code in zamba/models/efficientnet_models.py def on_train_epoch_start ( self ) -> None : \"\"\" Called in the training loop at the very beginning of the epoch. \"\"\" on_train_start ( self ) inherited \u00b6 Called at the beginning of training after sanity check. Source code in zamba/models/efficientnet_models.py def on_train_start ( self ): metrics = { \"val_macro_f1\" : {}} if self . num_classes > 2 : metrics . update ( { f \"val_top_ { k } _accuracy\" : {} for k in DEFAULT_TOP_K if k < self . num_classes } ) else : metrics . update ({ \"val_accuracy\" : {}}) # write hparams to hparams.yaml file, log metrics to tb hparams tab self . logger . log_hyperparams ( self . hparams , metrics ) on_val_dataloader ( self ) -> None inherited \u00b6 Called before requesting the val dataloader. .. deprecated:: v1.5 :meth: on_val_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: val_dataloader() directly. Source code in zamba/models/efficientnet_models.py def on_val_dataloader ( self ) -> None : \"\"\"Called before requesting the val dataloader. .. deprecated:: v1.5 :meth:`on_val_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`val_dataloader()` directly. \"\"\" on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the validation loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of validation_step_end(validation_step(x)) required batch Any The batched data as it is returned by the validation DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/efficientnet_models.py def on_validation_batch_end ( self , outputs : Optional [ STEP_OUTPUT ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the validation loop after the batch. Args: outputs: The outputs of validation_step_end(validation_step(x)) batch: The batched data as it is returned by the validation DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the validation DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/efficientnet_models.py def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the validation loop before anything happens for that batch. Args: batch: The batched data as it is returned by the validation DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_validation_end ( self ) -> None inherited \u00b6 Called at the end of validation. Source code in zamba/models/efficientnet_models.py def on_validation_end ( self ) -> None : \"\"\" Called at the end of validation. \"\"\" on_validation_epoch_end ( self ) -> None inherited \u00b6 Called in the validation loop at the very end of the epoch. Source code in zamba/models/efficientnet_models.py def on_validation_epoch_end ( self ) -> None : \"\"\" Called in the validation loop at the very end of the epoch. \"\"\" on_validation_epoch_start ( self ) -> None inherited \u00b6 Called in the validation loop at the very beginning of the epoch. Source code in zamba/models/efficientnet_models.py def on_validation_epoch_start ( self ) -> None : \"\"\" Called in the validation loop at the very beginning of the epoch. \"\"\" on_validation_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the val loop Source code in zamba/models/efficientnet_models.py def on_validation_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the val loop \"\"\" self . trainer . model . eval () on_validation_model_train ( self ) -> None inherited \u00b6 Sets the model to train during the val loop Source code in zamba/models/efficientnet_models.py def on_validation_model_train ( self ) -> None : \"\"\" Sets the model to train during the val loop \"\"\" self . trainer . model . train () on_validation_start ( self ) -> None inherited \u00b6 Called at the beginning of validation. Source code in zamba/models/efficientnet_models.py def on_validation_start ( self ) -> None : \"\"\" Called at the beginning of validation. \"\"\" optimizer_step ( self , epoch : int = None , batch_idx : int = None , optimizer : Optimizer = None , optimizer_idx : int = None , optimizer_closure : Optional [ Callable ] = None , on_tpu : bool = None , using_native_amp : bool = None , using_lbfgs : bool = None ) -> None inherited \u00b6 Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . !!! warning If you are overriding this method, make sure that you pass the optimizer_closure parameter to optimizer.step() function as shown in the examples. This ensures that training_step() , optimizer.zero_grad() , backward() are called within the training loop. Parameters: Name Type Description Default epoch int Current epoch None batch_idx int Index of current batch None optimizer Optimizer A PyTorch optimizer None optimizer_idx int If you used multiple optimizers, this indexes into that list. None optimizer_closure Optional[Callable] Closure for all optimizers None on_tpu bool True if TPU backward is required None using_native_amp bool True if using native amp None using_lbfgs bool True if the matching optimizer is :class: torch.optim.LBFGS None Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) Source code in zamba/models/efficientnet_models.py def optimizer_step ( self , epoch : int = None , batch_idx : int = None , optimizer : Optimizer = None , optimizer_idx : int = None , optimizer_closure : Optional [ Callable ] = None , on_tpu : bool = None , using_native_amp : bool = None , using_lbfgs : bool = None , ) -> None : r \"\"\" Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer. By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer. This method (and ``zero_grad()``) won't be called during the accumulation phase when ``Trainer(accumulate_grad_batches != 1)``. Warning: If you are overriding this method, make sure that you pass the ``optimizer_closure`` parameter to ``optimizer.step()`` function as shown in the examples. This ensures that ``training_step()``, ``optimizer.zero_grad()``, ``backward()`` are called within the training loop. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers on_tpu: ``True`` if TPU backward is required using_native_amp: ``True`` if using native amp using_lbfgs: True if the matching optimizer is :class:`torch.optim.LBFGS` Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) \"\"\" optimizer . step ( closure = optimizer_closure ) optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ) inherited \u00b6 Override this method to change the default behaviour of optimizer.zero_grad() . Parameters: Name Type Description Default epoch int Current epoch required batch_idx int Index of current batch required optimizer Optimizer A PyTorch optimizer required optimizer_idx int If you used multiple optimizers this indexes into that list. required Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example. Source code in zamba/models/efficientnet_models.py def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ): \"\"\"Override this method to change the default behaviour of ``optimizer.zero_grad()``. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example. \"\"\" optimizer . zero_grad () optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] inherited \u00b6 Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer bool If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. True Returns: Type Description Union[torch.optim.optimizer.Optimizer, pytorch_lightning.core.optimizer.LightningOptimizer, List[torch.optim.optimizer.Optimizer], List[pytorch_lightning.core.optimizer.LightningOptimizer]] A single optimizer, or a list of optimizers in case multiple ones are present. Source code in zamba/models/efficientnet_models.py def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ Optimizer , LightningOptimizer , List [ Optimizer ], List [ LightningOptimizer ]]: \"\"\" Returns the optimizer(s) that are being used during training. Useful for manual optimization. Args: use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a :class:`~pytorch_lightning.core.optimizer.LightningOptimizer` for automatic handling of precision and profiling. Returns: A single optimizer, or a list of optimizers in case multiple ones are present. \"\"\" if use_pl_optimizer : opts = list ( self . trainer . lightning_optimizers . values ()) else : opts = self . trainer . optimizers # single optimizer if isinstance ( opts , list ) and len ( opts ) == 1 and isinstance ( opts [ 0 ], ( Optimizer , LightningOptimizer )): return opts [ 0 ] # multiple opts return opts parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/models/efficientnet_models.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. !!! note In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. Source code in zamba/models/efficientnet_models.py def predict_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth:`prepare_data`. - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth:`predict` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" predict_step ( self , batch , batch_idx , dataloader_idx : Optional [ int ] = None ) inherited \u00b6 Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(accelerator=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Parameters: Name Type Description Default batch Current batch required batch_idx Index of current batch required dataloader_idx Optional[int] Index of the current dataloader None Returns: Type Description Predicted output Source code in zamba/models/efficientnet_models.py def predict_step ( self , batch , batch_idx , dataloader_idx : Optional [ int ] = None ): x , y = batch y_hat = self ( x ) pred = torch . sigmoid ( y_hat ) . cpu () . numpy () return pred prepare_data ( self ) -> None inherited \u00b6 Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() Source code in zamba/models/efficientnet_models.py def prepare_data ( self ) -> None : \"\"\" Use this to download and prepare data. .. warning:: DO NOT set state to the model (use `setup` instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): 1. Once per node. This is the default and is only called on LOCAL_RANK=0. 2. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() \"\"\" print ( self , * args , ** kwargs ) -> None inherited \u00b6 Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args The thing to print. The same as for Python's built-in print function. () **kwargs The same as for Python's built-in print function. {} Example:: def forward(self, x): self.print(x, 'in forward') Source code in zamba/models/efficientnet_models.py def print ( self , * args , ** kwargs ) -> None : r \"\"\" Prints only from process 0. Use this in any distributed mode to log only once. Args: *args: The thing to print. The same as for Python's built-in print function. **kwargs: The same as for Python's built-in print function. Example:: def forward(self, x): self.print(x, 'in forward') \"\"\" if self . trainer . is_global_zero : progress_bar = self . trainer . progress_bar_callback if progress_bar is not None and progress_bar . is_enabled : progress_bar . print ( * args , ** kwargs ) else : print ( * args , ** kwargs ) register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/models/efficientnet_models.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/models/efficientnet_models.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/models/efficientnet_models.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/models/efficientnet_models.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/models/efficientnet_models.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/models/efficientnet_models.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self save_hyperparameters ( self , * args , * , ignore : Union [ Sequence [ str ], str ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None inherited \u00b6 Save arguments to hparams attribute. Parameters: Name Type Description Default args single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ () ignore Union[Sequence[str], str] an argument name or a list of argument names from class __init__ to be ignored None frame Optional[frame] a frame object. Default is None None logger bool Whether to send the hyperparameters to the logger. Default: True True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 Source code in zamba/models/efficientnet_models.py def save_hyperparameters ( self , * args , ignore : Optional [ Union [ Sequence [ str ], str ]] = None , frame : Optional [ types . FrameType ] = None , logger : bool = True , ) -> None : \"\"\"Save arguments to ``hparams`` attribute. Args: args: single object of `dict`, `NameSpace` or `OmegaConf` or string names or arguments from class ``__init__`` ignore: an argument name or a list of argument names from class ``__init__`` to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 \"\"\" self . _log_hyperparams = logger # the frame needs to be created in this file. if not frame : frame = inspect . currentframe () . f_back save_hyperparameters ( self , * args , ignore = ignore , frame = frame ) set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/models/efficientnet_models.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) setup ( self , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) Source code in zamba/models/efficientnet_models.py def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Args: stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'`` Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) \"\"\" share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/models/efficientnet_models.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/models/efficientnet_models.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . core . memory . ModelSummary ] inherited \u00b6 Summarize this LightningModule. Parameters: Name Type Description Default mode Optional[str] Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of max_depth and will be removed in v1.6. 'top' max_depth Optional[int] The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. None Returns: Type Description Optional[pytorch_lightning.core.memory.ModelSummary] The model summary object Source code in zamba/models/efficientnet_models.py def summarize ( self , mode : Optional [ str ] = \"top\" , max_depth : Optional [ int ] = None ) -> Optional [ ModelSummary ]: \"\"\" Summarize this LightningModule. Args: mode: Can be either ``'top'`` (summarize only direct submodules) or ``'full'`` (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object \"\"\" model_summary = None # temporary mapping from mode to max_depth if max_depth is None : if mode in ModelSummary . MODES : max_depth = ModelSummary . MODES [ mode ] rank_zero_deprecation ( f \"Argument `mode` in `LightningModule.summarize` is deprecated in v1.4\" f \" and will be removed in v1.6. Use `max_depth= { max_depth } ` to replicate `mode= { mode } ` behavior.\" ) model_summary = ModelSummary ( self , max_depth = max_depth ) elif mode is not None : raise MisconfigurationException ( f \"`mode` can be None, { ', ' . join ( ModelSummary . MODES ) } , got { mode } \" ) else : model_summary = ModelSummary ( self , max_depth = max_depth ) log . info ( \" \\n \" + str ( model_summary )) return model_summary tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list inherited \u00b6 When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Parameters: Name Type Description Default batch Tensor Current batch required split_size int The size of the split required Returns: Type Description List of batch splits. Each split will be passed to meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits !!! note Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . Source code in zamba/models/efficientnet_models.py def tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list : r \"\"\" When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start` if :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` > 0. Each returned batch split is passed separately to :meth:`training_step`. \"\"\" time_dims = [ len ( x [ 0 ]) for x in batch if isinstance ( x , ( torch . Tensor , collections . Sequence ))] assert len ( time_dims ) >= 1 , \"Unable to determine batch time dimension\" assert all ( x == time_dims [ 0 ] for x in time_dims ), \"Batch time dimension length is ambiguous\" splits = [] for t in range ( 0 , time_dims [ 0 ], split_size ): batch_split = [] for i , x in enumerate ( batch ): if isinstance ( x , torch . Tensor ): split_x = x [:, t : t + split_size ] elif isinstance ( x , collections . Sequence ): split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )): split_x [ batch_idx ] = x [ batch_idx ][ t : t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits teardown ( self , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Source code in zamba/models/efficientnet_models.py def teardown ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Called at the end of fit (train + validate), validate, test, predict, or tune. Args: stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'`` \"\"\" test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a postive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] !!! note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. !!! note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in zamba/models/efficientnet_models.py def test_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a postive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`setup` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note: If you don't need a test dataset and a :meth:`test_step`, you don't need to implement this method. Note: In the case where you return multiple test dataloaders, the :meth:`test_step` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" test_epoch_end ( self , outputs : List [ Dict [ str , numpy . ndarray ]]) inherited \u00b6 Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs List[Dict[str, numpy.ndarray]] List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader required Returns: Type Description None !!! note If you didn't define a :meth: test_step , this won't be called. Examples: With a single dataloader: .. code-block:: python def test_epoch_end(self, outputs): # do something with the outputs of all test batches all_test_preds = test_step_outputs.predictions some_result = calc_all_results(all_test_preds) self.log(some_result) With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each test step for that dataloader. .. code-block:: python def test_epoch_end(self, outputs): final_value = 0 for dataloader_outputs in outputs: for test_step_out in dataloader_outputs: # do something final_value += test_step_out self.log(\"final_metric\", final_value) Source code in zamba/models/efficientnet_models.py def test_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"test\" ) test_step ( self , batch , batch_idx ) inherited \u00b6 Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch. required dataloader_idx int The index of the dataloader that produced this batch (only if multiple test dataloaders used). required Returns: Type Description Any of. Any object or value None - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... !!! note If you don't need to test you don't need to implement this method. !!! note When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. Source code in zamba/models/efficientnet_models.py def test_step ( self , batch , batch_idx ): return self . validation_step ( batch , batch_idx ) test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in :meth: test_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/models/efficientnet_models.py def test_step_end ( self , * args , ** kwargs ) -> Optional [ STEP_OUTPUT ]: \"\"\" Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`test_step` for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" to ( self , * args : Any , ** kwargs : Any ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. !!! note This method modifies the module in-place. Parameters: Name Type Description Default device the desired device of the parameters and buffers in this module required dtype the desired floating point type of the floating point parameters and buffers in this module required tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 Source code in zamba/models/efficientnet_models.py def to ( self , * args : Any , ** kwargs : Any ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def __init__(self, weight: torch.Tensor): ... super().__init__() ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 \"\"\" # there is diff nb vars in PT 1.5 out = torch . _C . _nn . _parse_to ( * args , ** kwargs ) self . __update_properties ( device = out [ 0 ], dtype = out [ 1 ]) return super () . to ( * args , ** kwargs ) to_disk ( self , path : PathLike ) inherited \u00b6 Source code in zamba/models/efficientnet_models.py def to_disk ( self , path : os . PathLike ): checkpoint = { \"state_dict\" : self . state_dict (), \"hyper_parameters\" : self . hparams , } torch . save ( checkpoint , path ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) inherited \u00b6 Saves the model in ONNX format. Parameters: Name Type Description Default file_path Union[str, pathlib.Path] The path of the file the onnx model should be saved to. required input_sample Optional[Any] An input for tracing. Default: None (Use self.example_input_array) None **kwargs Will be passed to torch.onnx.export function. {} Examples: >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) >>> with tempfile . NamedTemporaryFile ( suffix = '.onnx' , delete = False ) as tmpfile : ... model = SimpleModel () ... input_sample = torch . randn (( 1 , 64 )) ... model . to_onnx ( tmpfile . name , input_sample , export_params = True ) ... os . path . isfile ( tmpfile . name ) True Source code in zamba/models/efficientnet_models.py @torch . no_grad () def to_onnx ( self , file_path : Union [ str , Path ], input_sample : Optional [ Any ] = None , ** kwargs ): \"\"\" Saves the model in ONNX format. Args: file_path: The path of the file the onnx model should be saved to. input_sample: An input for tracing. Default: None (Use self.example_input_array) **kwargs: Will be passed to torch.onnx.export function. Example: >>> class SimpleModel(LightningModule): ... def __init__(self): ... super().__init__() ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) >>> with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmpfile: ... model = SimpleModel() ... input_sample = torch.randn((1, 64)) ... model.to_onnx(tmpfile.name, input_sample, export_params=True) ... os.path.isfile(tmpfile.name) True \"\"\" mode = self . training if input_sample is None : if self . example_input_array is None : raise ValueError ( \"Could not export to ONNX since neither `input_sample` nor\" \" `model.example_input_array` attribute is set.\" ) input_sample = self . example_input_array input_sample = self . _apply_batch_transfer_handler ( input_sample ) if \"example_outputs\" not in kwargs : self . eval () if isinstance ( input_sample , Tuple ): kwargs [ \"example_outputs\" ] = self ( * input_sample ) else : kwargs [ \"example_outputs\" ] = self ( input_sample ) torch . onnx . export ( self , input_sample , file_path , ** kwargs ) self . train ( mode ) to_torchscript ( self , file_path : Union [ str , pathlib . Path ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] inherited \u00b6 By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Parameters: Name Type Description Default file_path Union[str, pathlib.Path] Path where to save the torchscript. Default: None (no file saved). None method Optional[str] Whether to use TorchScript's script or trace method. Default: 'script' 'script' example_inputs Optional[Any] An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) None **kwargs Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. {} !!! note - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Examples: >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) ... >>> model = SimpleModel () >>> torch . jit . save ( model . to_torchscript (), \"model.pt\" ) # doctest: +SKIP >>> os . path . isfile ( \"model.pt\" ) # doctest: +SKIP >>> torch . jit . save ( model . to_torchscript ( file_path = \"model_trace.pt\" , method = 'trace' , # doctest: +SKIP ... example_inputs = torch . randn ( 1 , 64 ))) # doctest: +SKIP >>> os . path . isfile ( \"model_trace.pt\" ) # doctest: +SKIP True Returns: Type Description Union[torch._C.ScriptModule, Dict[str, torch._C.ScriptModule]] This LightningModule as a torchscript, regardless of whether file_path is defined or not. Source code in zamba/models/efficientnet_models.py @torch . no_grad () def to_torchscript ( self , file_path : Optional [ Union [ str , Path ]] = None , method : Optional [ str ] = \"script\" , example_inputs : Optional [ Any ] = None , ** kwargs , ) -> Union [ ScriptModule , Dict [ str , ScriptModule ]]: \"\"\" By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing, please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr:`example_input_array`) **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or :func:`torch.jit.trace` function. Note: - Requires the implementation of the :meth:`~pytorch_lightning.core.lightning.LightningModule.forward` method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod:`torch.jit` documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def __init__(self): ... super().__init__() ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether `file_path` is defined or not. \"\"\" mode = self . training if method == \"script\" : torchscript_module = torch . jit . script ( self . eval (), ** kwargs ) elif method == \"trace\" : # if no example inputs are provided, try to see if model has example_input_array set if example_inputs is None : if self . example_input_array is None : raise ValueError ( \"Choosing method=`trace` requires either `example_inputs`\" \" or `model.example_input_array` to be defined.\" ) example_inputs = self . example_input_array # automatically send example inputs to the right device and use trace example_inputs = self . _apply_batch_transfer_handler ( example_inputs ) torchscript_module = torch . jit . trace ( func = self . eval (), example_inputs = example_inputs , ** kwargs ) else : raise ValueError ( f \"The 'method' parameter only supports 'script' or 'trace', but value given was: { method } \" ) self . train ( mode ) if file_path is not None : fs = get_filesystem ( file_path ) with fs . open ( file_path , \"wb\" ) as f : torch . jit . save ( torchscript_module , f ) return torchscript_module toggle_optimizer ( self , optimizer : Optimizer , optimizer_idx : int ) inherited \u00b6 Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Override for your own behavior. Parameters: Name Type Description Default optimizer Optimizer Current optimizer used in the training loop required optimizer_idx int Current optimizer idx in the training loop required !!! note Only called when using multiple optimizers Source code in zamba/models/efficientnet_models.py def toggle_optimizer ( self , optimizer : Optimizer , optimizer_idx : int ): \"\"\" Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset. Override for your own behavior. Args: optimizer: Current optimizer used in the training loop optimizer_idx: Current optimizer idx in the training loop Note: Only called when using multiple optimizers \"\"\" # Iterate over all optimizer parameters to preserve their `requires_grad` information # in case these are pre-defined during `configure_optimizers` param_requires_grad_state = {} for opt in self . optimizers ( use_pl_optimizer = False ): for group in opt . param_groups : for param in group [ \"params\" ]: # If a param already appear in param_requires_grad_state, continue if param in param_requires_grad_state : continue param_requires_grad_state [ param ] = param . requires_grad param . requires_grad = False # Then iterate over the current optimizer's parameters and set its `requires_grad` # properties accordingly for group in optimizer . param_groups : for param in group [ \"params\" ]: param . requires_grad = param_requires_grad_state [ param ] self . _param_requires_grad_state = param_requires_grad_state train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] inherited \u00b6 Implement one or more PyTorch DataLoaders for training. Returns: Type Description A collection of class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in zamba/models/efficientnet_models.py def train_dataloader ( self ) -> TRAIN_DATALOADERS : \"\"\" Implement one or more PyTorch DataLoaders for training. Return: A collection of :class:`torch.utils.data.DataLoader` specifying training samples. In the case of multiple dataloaders, please see this :ref:`page <multiple-training-dataloaders>`. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`setup` - :meth:`train_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} \"\"\" rank_zero_warn ( \"`train_dataloader` must be implemented to be used with the Lightning Trainer\" ) training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]]) -> None inherited \u00b6 Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs List[Union[torch.Tensor, Dict[str, Any]]] List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. required Returns: Type Description None None !!! note If this method is not overridden, this won't be called. Example:: def training_epoch_end(self, training_step_outputs): # do something with all training_step outputs return result With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, training_step_outputs): for out in training_step_outputs: ... Source code in zamba/models/efficientnet_models.py def training_epoch_end ( self , outputs : EPOCH_OUTPUT ) -> None : \"\"\" Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth:`training_step`. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth:`training_step`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If this method is not overridden, this won't be called. Example:: def training_epoch_end(self, training_step_outputs): # do something with all training_step outputs return result With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, training_step_outputs): for out in training_step_outputs: ... \"\"\" training_step ( self , batch , batch_idx ) inherited \u00b6 Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int Integer displaying index of this batch required optimizer_idx int When using multiple optimizers, this argument will also be present. required hiddens( class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. required Returns: Type Description Any of. - class: ~torch.Tensor - The loss tensor - dict - A dictionary. Can include any keys, but must include the key 'loss' - None - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU or TPU, or using DeepSpeed . In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step ... out, hiddens = self.lstm(data, hiddens) ... return {\"loss\": loss, \"hiddens\": hiddens} !!! note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. Source code in zamba/models/efficientnet_models.py def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"train_loss\" , loss . detach ()) return loss training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in training_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} def training_step_end(self, training_step_outputs): gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/models/efficientnet_models.py def training_step_end ( self , * args , ** kwargs ) -> STEP_OUTPUT : \"\"\" Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in `training_step` for each batch part. Return: Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} def training_step_end(self, training_step_outputs): gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" transfer_batch_to_device ( self , batch : Any , device : device , dataloader_idx : int ) -> Any inherited \u00b6 Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). !!! note This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be transferred to a new device. required device device The target device as defined in PyTorch. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) !!! else batch = super().transfer_batch_to_device(data, device) return batch See Also: - :meth: move_data_to_device - :meth: apply_to_collection Source code in zamba/models/efficientnet_models.py def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any : \"\"\" Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: - :class:`torch.Tensor` or anything that implements `.to(...)` - :class:`list` - :class:`dict` - :class:`tuple` - :class:`torchtext.data.batch.Batch` For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`move_data_to_device` - :meth:`apply_to_collection` \"\"\" return move_data_to_device ( batch , device ) type ( self , dst_type : Union [ str , torch . dtype ]) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def type ( self , dst_type : Union [ str , torch . dtype ]) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\"\" self . __update_properties ( dtype = dst_type ) return super () . type ( dst_type = dst_type ) unfreeze ( self ) -> None inherited \u00b6 Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() Source code in zamba/models/efficientnet_models.py def unfreeze ( self ) -> None : \"\"\" Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() \"\"\" for param in self . parameters (): param . requires_grad = True self . train () untoggle_optimizer ( self , optimizer_idx : int ) inherited \u00b6 Resets the state of required gradients that were toggled with :meth: toggle_optimizer . Override for your own behavior. Parameters: Name Type Description Default optimizer_idx int Current optimizer idx in the training loop required !!! note Only called when using multiple optimizers Source code in zamba/models/efficientnet_models.py def untoggle_optimizer ( self , optimizer_idx : int ): \"\"\" Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`. Override for your own behavior. Args: optimizer_idx: Current optimizer idx in the training loop Note: Only called when using multiple optimizers \"\"\" for opt_idx , opt in enumerate ( self . optimizers ( use_pl_optimizer = False )): if optimizer_idx != opt_idx : for group in opt . param_groups : for param in group [ \"params\" ]: if param in self . _param_requires_grad_state : param . requires_grad = self . _param_requires_grad_state [ param ] # save memory self . _param_requires_grad_state = {} val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] !!! note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. !!! note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in zamba/models/efficientnet_models.py def val_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a positive integer. It's recommended that all data downloads and preparation happen in :meth:`prepare_data`. - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note: If you don't need a validation dataset and a :meth:`validation_step`, you don't need to implement this method. Note: In the case where you return multiple validation dataloaders, the :meth:`validation_step` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" validation_epoch_end ( self , outputs : List [ Dict [ str , numpy . ndarray ]]) inherited \u00b6 Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Parameters: Name Type Description Default outputs List[dict] list of output dictionaries from each validation step containing y_pred and y_true. required Source code in zamba/models/efficientnet_models.py def validation_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Args: outputs (List[dict]): list of output dictionaries from each validation step containing y_pred and y_true. \"\"\" y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"val\" ) validation_step ( self , batch , batch_idx ) inherited \u00b6 Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch required dataloader_idx int The index of the dataloader that produced this batch (only if multiple val dataloaders used) required Returns: Type Description Any object or value None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... !!! note If you don't need to validate you don't need to implement this method. !!! note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. Source code in zamba/models/efficientnet_models.py def validation_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"val_loss\" , loss . detach ()) y_proba = torch . sigmoid ( y_hat . cpu ()) . numpy () return { \"y_true\" : y . cpu () . numpy () . astype ( int ), \"y_pred\" : y_proba . round () . astype ( int ), \"y_proba\" : y_proba , } validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in :meth: validation_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/models/efficientnet_models.py def validation_step_end ( self , * args , ** kwargs ) -> Optional [ STEP_OUTPUT ]: \"\"\" Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`validation_step` for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" write_prediction ( self , name : str , value : Union [ torch . Tensor , List [ torch . Tensor ]], filename : str = 'predictions.pt' ) inherited \u00b6 Write predictions to disk using torch.save Example:: self.write_prediction('pred', torch.tensor(...), filename='my_predictions.pt') Parameters: Name Type Description Default name str a string indicating the name to save the predictions under required value Union[torch.Tensor, List[torch.Tensor]] the predictions, either a single :class: ~torch.Tensor or a list of them required filename str name of the file to save the predictions to 'predictions.pt' !!! note when running in distributed mode, calling write_prediction will create a file for each device with respective names: filename_rank_0.pt , filename_rank_1.pt , ... .. deprecated::v1.3 Will be removed in v1.5.0. Source code in zamba/models/efficientnet_models.py def write_prediction ( self , name : str , value : Union [ torch . Tensor , List [ torch . Tensor ]], filename : str = \"predictions.pt\" ): \"\"\" Write predictions to disk using ``torch.save`` Example:: self.write_prediction('pred', torch.tensor(...), filename='my_predictions.pt') Args: name: a string indicating the name to save the predictions under value: the predictions, either a single :class:`~torch.Tensor` or a list of them filename: name of the file to save the predictions to Note: when running in distributed mode, calling ``write_prediction`` will create a file for each device with respective names: ``filename_rank_0.pt``, ``filename_rank_1.pt``, ... .. deprecated::v1.3 Will be removed in v1.5.0. \"\"\" rank_zero_deprecation ( \"LightningModule method `write_prediction` was deprecated in v1.3 and will be removed in v1.5.\" ) self . trainer . _evaluation_loop . predictions . _add_prediction ( name , value , filename ) write_prediction_dict ( self , predictions_dict : Dict [ str , Any ], filename : str = 'predictions.pt' ) inherited \u00b6 Write a dictonary of predictions to disk at once using torch.save Example:: pred_dict = {'pred1': torch.tensor(...), 'pred2': torch.tensor(...)} self.write_prediction_dict(pred_dict) Parameters: Name Type Description Default predictions_dict Dict[str, Any] dict containing predictions, where each prediction should either be single :class: ~torch.Tensor or a list of them required !!! note when running in distributed mode, calling write_prediction_dict will create a file for each device with respective names: filename_rank_0.pt , filename_rank_1.pt , ... .. deprecated::v1.3 Will be removed in v1.5.0. Source code in zamba/models/efficientnet_models.py def write_prediction_dict ( self , predictions_dict : Dict [ str , Any ], filename : str = \"predictions.pt\" ): \"\"\" Write a dictonary of predictions to disk at once using ``torch.save`` Example:: pred_dict = {'pred1': torch.tensor(...), 'pred2': torch.tensor(...)} self.write_prediction_dict(pred_dict) Args: predictions_dict: dict containing predictions, where each prediction should either be single :class:`~torch.Tensor` or a list of them Note: when running in distributed mode, calling ``write_prediction_dict`` will create a file for each device with respective names: ``filename_rank_0.pt``, ``filename_rank_1.pt``, ... .. deprecated::v1.3 Will be removed in v1.5.0. \"\"\" rank_zero_deprecation ( \"LightningModule method `write_prediction_dict` was deprecated in v1.3 and will be removed in v1.5.\" ) for k , v in predictions_dict . items (): self . write_prediction ( k , v , filename ) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/models/efficientnet_models.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/models/efficientnet_models.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ ()","title":"zamba.models.efficientnet_models"},{"location":"api-reference/models-efficientnet_models/#zambamodelsefficientnet_models","text":"","title":"zamba.models.efficientnet_models"},{"location":"api-reference/models-efficientnet_models/#zamba.models.efficientnet_models-classes","text":"","title":"Classes"},{"location":"api-reference/models-megadetector_lite_yolox/","text":"zamba.models.megadetector_lite_yolox \u00b6 LOCAL_MD_LITE_MODEL \u00b6 Classes \u00b6 FillModeEnum ( str , Enum ) \u00b6 Enum for frame filtering fill modes Attributes: Name Type Description repeat Randomly resample qualifying frames to get to n_frames score_sorted Take up to n_frames in sort order (even if some have zero probability) weighted_euclidean Sample the remaining frames weighted by their euclidean distance in time to the frames over the threshold weighted_prob Sample the remaining frames weighted by their predicted probability repeat \u00b6 score_sorted \u00b6 weighted_euclidean \u00b6 weighted_prob \u00b6 MegadetectorLiteYoloX \u00b6 Methods \u00b6 __init__ ( self , path : PathLike = PosixPath ( '/home/runner/work/zamba/zamba/zamba/models/yolox_models/assets/yolox_nano_20210901.pth' ), config : Union [ zamba . models . megadetector_lite_yolox . MegadetectorLiteYoloXConfig , dict ] = None ) special \u00b6 MegadetectorLite based on YOLOX. Parameters: Name Type Description Default path pathlike Path to trained YoloX model checkpoint (.pth extension) PosixPath('/home/runner/work/zamba/zamba/zamba/models/yolox_models/assets/yolox_nano_20210901.pth') config MegadetectorLiteYoloXConfig YoloX configuration None Source code in zamba/models/megadetector_lite_yolox.py def __init__ ( self , path : os . PathLike = LOCAL_MD_LITE_MODEL , config : Optional [ Union [ MegadetectorLiteYoloXConfig , dict ]] = None , ): \"\"\"MegadetectorLite based on YOLOX. Args: path (pathlike): Path to trained YoloX model checkpoint (.pth extension) config (MegadetectorLiteYoloXConfig): YoloX configuration \"\"\" if config is None : config = MegadetectorLiteYoloXConfig () elif isinstance ( config , dict ): config = MegadetectorLiteYoloXConfig . parse_obj ( config ) checkpoint = torch . load ( path , map_location = config . device ) num_classes = checkpoint [ \"model\" ][ \"head.cls_preds.0.weight\" ] . shape [ 0 ] yolox = YoloXNano ( num_classes = num_classes ) model = yolox . get_model () model . load_state_dict ( checkpoint [ \"model\" ]) model = model . eval () . to ( config . device ) self . model = model self . yolox = yolox self . config = config self . num_classes = num_classes detect_image ( self , img_arr : ndarray ) -> Tuple [ numpy . ndarray , numpy . ndarray ] \u00b6 Runs object detection on an image. Parameters: Name Type Description Default img_arr np.ndarray An image array with dimensions (height, width, channels). required Returns: Type Description np.ndarray An array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). np.ndarray: An array of object detection confidence scores of length (object) where object is the number of objects detected. Source code in zamba/models/megadetector_lite_yolox.py def detect_image ( self , img_arr : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Runs object detection on an image. Args: img_arr (np.ndarray): An image array with dimensions (height, width, channels). Returns: np.ndarray: An array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). np.ndarray: An array of object detection confidence scores of length (object) where object is the number of objects detected. \"\"\" with torch . no_grad (): outputs = self . model ( torch . from_numpy ( self . _preprocess ( img_arr )) . unsqueeze ( 0 ) . to ( self . config . device ) ) output = postprocess ( outputs , self . num_classes , self . config . confidence , self . config . nms_threshold )[ 0 ] if output is None : return np . array ([]), np . array ([]) else : detections = pd . DataFrame ( output . cpu () . numpy (), columns = [ \"x1\" , \"y1\" , \"x2\" , \"y2\" , \"score1\" , \"score2\" , \"class_num\" ], ) . assign ( score = lambda row : row . score1 * row . score2 ) # Transform bounding box to be in terms of the original image dimensions original_height , original_width = img_arr . shape [: 2 ] ratio = min ( self . config . image_width / original_width , self . config . image_height / original_height , ) detections [[ \"x1\" , \"y1\" , \"x2\" , \"y2\" ]] /= ratio # Express bounding boxes in terms of proportions of original image dimensions detections [[ \"x1\" , \"x2\" ]] /= original_width detections [[ \"y1\" , \"y2\" ]] /= original_height return detections [[ \"x1\" , \"y1\" , \"x2\" , \"y2\" ]] . values , detections . score . values detect_video ( self , frames : ndarray , pbar : bool = False ) \u00b6 Source code in zamba/models/megadetector_lite_yolox.py def detect_video ( self , frames : np . ndarray , pbar : bool = False ): pbar = tqdm if pbar else lambda x : x detections = [] for frame in pbar ( frames ): detections . append ( self . detect_image ( frame )) return detections filter_frames ( self , frames : ndarray , detections : List [ Tuple [ float , float , float , float ]]) -> ndarray \u00b6 Filter video frames using megadetector lite. Which frames are returned depends on the fill_mode and how many frames are above the confidence threshold. If more than n_frames are above the threshold, the top n_frames are returned. Otherwise add to those over threshold based on fill_mode. If none of these conditions are met, returns all frames above the threshold. Parameters: Name Type Description Default frames np.ndarray Array of video frames to filter with dimensions (frames, height, width, channels) required detections list of tuples List of detection results for each frame. Each element is a tuple of the list of bounding boxes [array(x1, y1, x2, y2)] and the detection probabilities, both as float required Returns: Type Description np.ndarray An array of video frames of length n_frames or shorter Source code in zamba/models/megadetector_lite_yolox.py def filter_frames ( self , frames : np . ndarray , detections : List [ Tuple [ float , float , float , float ]] ) -> np . ndarray : \"\"\"Filter video frames using megadetector lite. Which frames are returned depends on the fill_mode and how many frames are above the confidence threshold. If more than n_frames are above the threshold, the top n_frames are returned. Otherwise add to those over threshold based on fill_mode. If none of these conditions are met, returns all frames above the threshold. Args: frames (np.ndarray): Array of video frames to filter with dimensions (frames, height, width, channels) detections (list of tuples): List of detection results for each frame. Each element is a tuple of the list of bounding boxes [array(x1, y1, x2, y2)] and the detection probabilities, both as float Returns: np.ndarray: An array of video frames of length n_frames or shorter \"\"\" frame_scores = pd . Series ( [( np . max ( score ) if ( len ( score ) > 0 ) else 0 ) for _ , score in detections ] ) . sort_values ( ascending = False ) # reduce to one score per frame selected_indices = frame_scores . loc [ frame_scores > self . config . confidence ] . index if self . config . n_frames is None : # no minimum n_frames provided, just select all the frames with scores > threshold pass elif len ( selected_indices ) >= self . config . n_frames : # num. frames with scores > threshold is greater than the requested number of frames selected_indices = ( frame_scores [ selected_indices ] . sort_values ( ascending = False ) . iloc [: self . config . n_frames ] . index ) elif len ( selected_indices ) < self . config . n_frames : # num. frames with scores > threshold is less than the requested number of frames # repeat frames that are above threshold to get to n_frames rng = np . random . RandomState ( self . config . seed ) if self . config . fill_mode == \"repeat\" : repeated_indices = rng . choice ( selected_indices , self . config . n_frames - len ( selected_indices ), replace = True , ) selected_indices = np . concatenate (( selected_indices , repeated_indices )) # take frames in sorted order up to n_frames, even if score is zero elif self . config . fill_mode == \"score_sorted\" : selected_indices = ( frame_scores . sort_values ( ascending = False ) . iloc [: self . config . n_frames ] . index ) # sample up to n_frames, prefer points closer to frames with detection elif self . config . fill_mode == \"weighted_euclidean\" : sample_from = frame_scores . loc [ ~ frame_scores . index . isin ( selected_indices )] . index # take one over euclidean distance to all points with detection weights = [ 1 / np . linalg . norm ( selected_indices - sample ) for sample in sample_from ] # normalize weights weights /= np . sum ( weights ) sampled = rng . choice ( sample_from , self . config . n_frames - len ( selected_indices ), replace = False , p = weights , ) selected_indices = np . concatenate (( selected_indices , sampled )) # sample up to n_frames, weight by predicted probability - only if some frames have nonzero prob elif ( self . config . fill_mode == \"weighted_prob\" ) and ( len ( selected_indices ) > 0 ): sample_from = frame_scores . loc [ ~ frame_scores . index . isin ( selected_indices )] . index weights = frame_scores [ sample_from ] / np . sum ( frame_scores [ sample_from ]) sampled = rng . choice ( sample_from , self . config . n_frames - len ( selected_indices ), replace = False , p = weights , ) selected_indices = np . concatenate (( selected_indices , sampled )) # sort the selected images back into their original order if self . config . sort_by_time : selected_indices = sorted ( selected_indices ) return frames [ selected_indices ] scale_and_pad_array ( image_array : ndarray , output_width : int , output_height : int ) -> ndarray staticmethod \u00b6 Source code in zamba/models/megadetector_lite_yolox.py @staticmethod def scale_and_pad_array ( image_array : np . ndarray , output_width : int , output_height : int ) -> np . ndarray : return np . array ( ImageOps . pad ( Image . fromarray ( image_array ), ( output_width , output_height ), method = Image . BICUBIC , color = None , centering = ( 0 , 0 ), ) ) MegadetectorLiteYoloXConfig ( BaseModel ) pydantic-model \u00b6 Configuration for a MegadetectorLiteYoloX frame selection model Attributes: Name Type Description confidence float Only consider object detections with this confidence or greater nms_threshold float Non-maximum suppression is a method for filtering many bounding boxes around the same object to a single bounding box. This is a constant that determines how much to suppress similar bounding boxes. image_width int Scale image to this width before sending to object detection model. image_height int Scale image to this height before sending to object detection model. device str Where to run the object detection model, \"cpu\" or \"cuda\". n_frames int Max number of frames to return. If None returns all frames above the threshold. Defaults to None. fill_mode str Mode for upsampling if the number of frames above the threshold is less than n_frames. Defaults to \"repeat\". sort_by_time bool Whether to sort the selected frames by time (original order) before returning. If False, returns frames sorted by score (descending). Defaults to True. seed int Random state for random number generator. Defaults to 55. confidence : float pydantic-field \u00b6 device : str pydantic-field \u00b6 fill_mode : FillModeEnum pydantic-field \u00b6 image_height : int pydantic-field \u00b6 image_width : int pydantic-field \u00b6 n_frames : int pydantic-field \u00b6 nms_threshold : float pydantic-field \u00b6 seed : int pydantic-field \u00b6 sort_by_time : bool pydantic-field \u00b6 Config \u00b6","title":"zamba.models.megadetector_lite_yolox"},{"location":"api-reference/models-megadetector_lite_yolox/#zambamodelsmegadetector_lite_yolox","text":"","title":"zamba.models.megadetector_lite_yolox"},{"location":"api-reference/models-megadetector_lite_yolox/#zamba.models.megadetector_lite_yolox-classes","text":"","title":"Classes"},{"location":"api-reference/models-model_manager/","text":"zamba.models.model_manager \u00b6 Classes \u00b6 ModelManager \u00b6 Mediates loading, configuration, and logic of model calls. Parameters: Name Type Description Default config ModelConfig Instantiated ModelConfig. required __init__ ( self , config : ModelConfig ) special \u00b6 Source code in zamba/models/model_manager.py def __init__ ( self , config : ModelConfig ): self . config = config from_yaml ( config ) classmethod \u00b6 Source code in zamba/models/model_manager.py @classmethod def from_yaml ( cls , config ): if not isinstance ( config , ModelConfig ): config = ModelConfig . parse_file ( config ) return cls ( config ) predict ( self ) \u00b6 Source code in zamba/models/model_manager.py def predict ( self ): predict_model ( predict_config = self . config . predict_config , video_loader_config = self . config . video_loader_config , ) train ( self ) \u00b6 Source code in zamba/models/model_manager.py def train ( self ): train_model ( train_config = self . config . train_config , video_loader_config = self . config . video_loader_config , ) Functions \u00b6 instantiate_model ( checkpoint : Union [ os . PathLike , str ], weight_download_region : RegionEnum , scheduler_config : Optional [ zamba . models . config . SchedulerConfig ], model_cache_dir : Optional [ os . PathLike ], labels : Optional [ pandas . core . frame . DataFrame ], from_scratch : bool = False , model_name : Optional [ zamba . models . config . ModelEnum ] = None , predict_all_zamba_species : bool = True ) -> ZambaVideoClassificationLightningModule \u00b6 Instantiates the model from a checkpoint and detects whether the model head should be replaced. The model head is replaced if labels contain species that are not on the model or predict_all_zamba_species=False. Supports model instantiation for the following cases: - train from scratch (from_scratch=True) - finetune with new species (from_scratch=False, labels contains different species than model) - finetune with a subset of zamba species and output only the species in the labels file (predict_all_zamba_species=False) - finetune with a subset of zamba species but output all zamba species (predict_all_zamba_species=True) - predict using pretrained model (labels=None) Parameters: Name Type Description Default checkpoint path or str Either the path to a checkpoint on disk or the name of a checkpoint file in the S3 bucket, i.e., one that is discoverable by download_weights . required weight_download_region RegionEnum Server region for downloading weights. required scheduler_config SchedulerConfig SchedulerConfig to use for training or finetuning. Only used if labels is not None. required model_cache_dir path Directory in which to store pretrained model weights. required labels pd.DataFrame Dataframe where filepath is the index and columns are one hot encoded species. required from_scratch bool Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Defaults to False. Only used if labels is not None. False model_name ModelEnum Model name used to look up default hparams used for that model. Only relevant if training from scratch. None predict_all_zamba_species(bool) Whether the species outputted by the model should be all zamba species. If you want the model classes to only be the species in your labels file, set to False. Defaults to True. Only used if labels is not None. required Returns: Type Description ZambaVideoClassificationLightningModule Instantiated model Source code in zamba/models/model_manager.py def instantiate_model ( checkpoint : Union [ os . PathLike , str ], weight_download_region : RegionEnum , scheduler_config : Optional [ SchedulerConfig ], model_cache_dir : Optional [ os . PathLike ], labels : Optional [ pd . DataFrame ], from_scratch : bool = False , model_name : Optional [ ModelEnum ] = None , predict_all_zamba_species : bool = True , ) -> ZambaVideoClassificationLightningModule : \"\"\"Instantiates the model from a checkpoint and detects whether the model head should be replaced. The model head is replaced if labels contain species that are not on the model or predict_all_zamba_species=False. Supports model instantiation for the following cases: - train from scratch (from_scratch=True) - finetune with new species (from_scratch=False, labels contains different species than model) - finetune with a subset of zamba species and output only the species in the labels file (predict_all_zamba_species=False) - finetune with a subset of zamba species but output all zamba species (predict_all_zamba_species=True) - predict using pretrained model (labels=None) Args: checkpoint (path or str): Either the path to a checkpoint on disk or the name of a checkpoint file in the S3 bucket, i.e., one that is discoverable by `download_weights`. weight_download_region (RegionEnum): Server region for downloading weights. scheduler_config (SchedulerConfig, optional): SchedulerConfig to use for training or finetuning. Only used if labels is not None. model_cache_dir (path, optional): Directory in which to store pretrained model weights. labels (pd.DataFrame, optional): Dataframe where filepath is the index and columns are one hot encoded species. from_scratch (bool): Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Defaults to False. Only used if labels is not None. model_name (ModelEnum, optional): Model name used to look up default hparams used for that model. Only relevant if training from scratch. predict_all_zamba_species(bool): Whether the species outputted by the model should be all zamba species. If you want the model classes to only be the species in your labels file, set to False. Defaults to True. Only used if labels is not None. Returns: ZambaVideoClassificationLightningModule: Instantiated model \"\"\" if from_scratch : # get hparams from official model with ( MODELS_DIRECTORY / f \" { model_name } /hparams.yaml\" ) . open () as f : hparams = yaml . safe_load ( f ) else : # download if neither local checkpoint nor cached checkpoint exist if not checkpoint . exists () and not ( model_cache_dir / checkpoint ) . exists (): logger . info ( \"Downloading weights for model.\" ) checkpoint = download_weights ( filename = str ( checkpoint ), weight_region = weight_download_region , destination_dir = model_cache_dir , ) hparams = torch . load ( checkpoint , map_location = torch . device ( \"cpu\" ))[ \"hyper_parameters\" ] model_class = available_models [ hparams [ \"model_class\" ]] logger . info ( f \"Instantiating model: { model_class . __name__ } \" ) if labels is None : # predict; load from checkpoint uses associated hparams logger . info ( \"Loading from checkpoint.\" ) return model_class . load_from_checkpoint ( checkpoint_path = checkpoint ) # get species from labels file species = labels . filter ( regex = r \"^species_\" ) . columns . tolist () species = [ s . split ( \"species_\" , 1 )[ 1 ] for s in species ] # check if species in label file are a subset of pretrained model species is_subset = set ( species ) . issubset ( set ( hparams [ \"species\" ])) # train from scratch if from_scratch : logger . info ( \"Training from scratch.\" ) # default would use scheduler used for pretrained model if scheduler_config != \"default\" : hparams . update ( scheduler_config . dict ()) hparams . update ({ \"species\" : species }) model = model_class ( ** hparams ) # replace the head elif not predict_all_zamba_species or not is_subset : if not predict_all_zamba_species : logger . info ( \"Limiting only to species in labels file. Replacing model head and finetuning.\" ) else : logger . info ( \"Provided species do not fully overlap with Zamba species. Replacing model head and finetuning.\" ) # update in case we want to finetune with different scheduler if scheduler_config != \"default\" : hparams . update ( scheduler_config . dict ()) hparams . update ({ \"species\" : species }) model = model_class ( finetune_from = checkpoint , ** hparams ) # resume training; add additional species columns to labels file if needed elif is_subset : logger . info ( \"Provided species fully overlap with Zamba species. Resuming training from latest checkpoint.\" ) # update in case we want to resume with different scheduler if scheduler_config != \"default\" : hparams . update ( scheduler_config . dict ()) model = model_class . load_from_checkpoint ( checkpoint_path = checkpoint , ** hparams ) # add in remaining columns for species that are not present for c in set ( hparams [ \"species\" ]) . difference ( set ( species )): # labels are still OHE at this point labels [ f \"species_ { c } \" ] = 0 # sort columns so columns on dataloader are the same as columns on model labels . sort_index ( axis = 1 , inplace = True ) logger . info ( f \"Using learning rate scheduler: { model . hparams [ 'scheduler' ] } \" ) logger . info ( f \"Using scheduler params: { model . hparams [ 'scheduler_params' ] } \" ) return model predict_model ( predict_config : PredictConfig , video_loader_config : VideoLoaderConfig = None ) \u00b6 Predicts from a model and writes out predictions to a csv. Parameters: Name Type Description Default predict_config PredictConfig Pydantic config for performing inference. required video_loader_config VideoLoaderConfig Pydantic config for preprocessing videos. If None, will use default for model specified in PredictConfig. None Source code in zamba/models/model_manager.py def predict_model ( predict_config : PredictConfig , video_loader_config : VideoLoaderConfig = None , ): \"\"\"Predicts from a model and writes out predictions to a csv. Args: predict_config (PredictConfig): Pydantic config for performing inference. video_loader_config (VideoLoaderConfig, optional): Pydantic config for preprocessing videos. If None, will use default for model specified in PredictConfig. \"\"\" # get default VLC for model if not specified if video_loader_config is None : video_loader_config = ModelConfig ( predict_config = predict_config , video_loader_config = video_loader_config ) . video_loader_config # set up model model = instantiate_model ( checkpoint = predict_config . checkpoint , weight_download_region = predict_config . weight_download_region , model_cache_dir = predict_config . model_cache_dir , scheduler_config = None , labels = None , ) data_module = ZambaDataModule ( video_loader_config = video_loader_config , transform = MODEL_MAPPING [ model . __class__ . __name__ ][ \"transform\" ], predict_metadata = predict_config . filepaths , batch_size = predict_config . batch_size , num_workers = predict_config . num_workers , ) validate_species ( model , data_module ) if video_loader_config . cache_dir is None : logger . info ( \"No cache dir is specified. Videos will not be cached.\" ) else : logger . info ( f \"Videos will be cached to { video_loader_config . cache_dir } .\" ) trainer = pl . Trainer ( gpus = predict_config . gpus , logger = False , fast_dev_run = predict_config . dry_run ) configuration = { \"model_class\" : model . model_class , \"species\" : model . species , \"predict_config\" : json . loads ( predict_config . json ( exclude = { \"filepaths\" })), \"inference_start_time\" : datetime . utcnow () . isoformat (), \"video_loader_config\" : json . loads ( video_loader_config . json ()), } if predict_config . save is not False : config_path = predict_config . save_dir / \"predict_configuration.yaml\" logger . info ( f \"Writing out full configuration to { config_path } .\" ) with config_path . open ( \"w\" ) as fp : yaml . dump ( configuration , fp ) dataloader = data_module . predict_dataloader () logger . info ( \"Starting prediction...\" ) probas = trainer . predict ( model = model , dataloaders = dataloader ) df = pd . DataFrame ( np . vstack ( probas ), columns = model . species , index = dataloader . dataset . original_indices ) # change output format if specified if predict_config . proba_threshold is not None : df = ( df > predict_config . proba_threshold ) . astype ( int ) elif predict_config . output_class_names : df = df . idxmax ( axis = 1 ) else : # round to a useful number of places df = df . round ( 5 ) if predict_config . save is not False : preds_path = predict_config . save_dir / \"zamba_predictions.csv\" logger . info ( f \"Saving out predictions to { preds_path } .\" ) with preds_path . open ( \"w\" ) as fp : df . to_csv ( fp , index = True ) return df train_model ( train_config : TrainConfig , video_loader_config : Optional [ zamba . data . video . VideoLoaderConfig ] = None ) \u00b6 Trains a model. Parameters: Name Type Description Default train_config TrainConfig Pydantic config for training. required video_loader_config VideoLoaderConfig Pydantic config for preprocessing videos. If None, will use default for model specified in TrainConfig. None Source code in zamba/models/model_manager.py def train_model ( train_config : TrainConfig , video_loader_config : Optional [ VideoLoaderConfig ] = None , ): \"\"\"Trains a model. Args: train_config (TrainConfig): Pydantic config for training. video_loader_config (VideoLoaderConfig, optional): Pydantic config for preprocessing videos. If None, will use default for model specified in TrainConfig. \"\"\" # get default VLC for model if not specified if video_loader_config is None : video_loader_config = ModelConfig ( train_config = train_config , video_loader_config = video_loader_config ) . video_loader_config # set up model model = instantiate_model ( checkpoint = train_config . checkpoint , scheduler_config = train_config . scheduler_config , weight_download_region = train_config . weight_download_region , model_cache_dir = train_config . model_cache_dir , labels = train_config . labels , from_scratch = train_config . from_scratch , model_name = train_config . model_name , predict_all_zamba_species = train_config . predict_all_zamba_species , ) data_module = ZambaDataModule ( video_loader_config = video_loader_config , transform = MODEL_MAPPING [ model . __class__ . __name__ ][ \"transform\" ], train_metadata = train_config . labels , batch_size = train_config . batch_size , num_workers = train_config . num_workers , ) validate_species ( model , data_module ) train_config . save_dir . mkdir ( parents = True , exist_ok = True ) # add folder version_n that auto increments if we are not overwriting tensorboard_version = train_config . save_dir . name if train_config . overwrite else None tensorboard_save_dir = ( train_config . save_dir . parent if train_config . overwrite else train_config . save_dir ) tensorboard_logger = TensorBoardLogger ( save_dir = tensorboard_save_dir , name = None , version = tensorboard_version , default_hp_metric = False , ) logging_and_save_dir = ( tensorboard_logger . log_dir if not train_config . overwrite else train_config . save_dir ) model_checkpoint = ModelCheckpoint ( dirpath = logging_and_save_dir , filename = train_config . model_name , monitor = train_config . early_stopping_config . monitor , mode = train_config . early_stopping_config . mode , ) callbacks = [ model_checkpoint ] if train_config . early_stopping_config is not None : callbacks . append ( EarlyStopping ( ** train_config . early_stopping_config . dict ())) if train_config . backbone_finetune_config is not None : callbacks . append ( BackboneFinetuning ( ** train_config . backbone_finetune_config . dict ())) trainer = pl . Trainer ( gpus = train_config . gpus , max_epochs = train_config . max_epochs , auto_lr_find = train_config . auto_lr_find , logger = tensorboard_logger , callbacks = callbacks , fast_dev_run = train_config . dry_run , accelerator = \"ddp\" if data_module . multiprocessing_context is not None else None , plugins = DDPPlugin ( find_unused_parameters = False ) if data_module . multiprocessing_context is not None else None , ) if video_loader_config . cache_dir is None : logger . info ( \"No cache dir is specified. Videos will not be cached.\" ) else : logger . info ( f \"Videos will be cached to { video_loader_config . cache_dir } .\" ) if train_config . auto_lr_find : logger . info ( \"Finding best learning rate.\" ) trainer . tune ( model , data_module ) try : git_hash = git . Repo ( search_parent_directories = True ) . head . object . hexsha except git . exc . InvalidGitRepositoryError : git_hash = None configuration = { \"git_hash\" : git_hash , \"model_class\" : model . model_class , \"species\" : model . species , \"starting_learning_rate\" : model . lr , \"train_config\" : json . loads ( train_config . json ( exclude = { \"labels\" })), \"training_start_time\" : datetime . utcnow () . isoformat (), \"video_loader_config\" : json . loads ( video_loader_config . json ()), } if not train_config . dry_run : config_path = Path ( logging_and_save_dir ) / \"train_configuration.yaml\" config_path . parent . mkdir ( exist_ok = True , parents = True ) logger . info ( f \"Writing out full configuration to { config_path } .\" ) with config_path . open ( \"w\" ) as fp : yaml . dump ( configuration , fp ) logger . info ( \"Starting training...\" ) trainer . fit ( model , data_module ) if not train_config . dry_run : if trainer . datamodule . test_dataloader () is not None : logger . info ( \"Calculating metrics on holdout set.\" ) test_metrics = trainer . test ( dataloaders = trainer . datamodule . test_dataloader ())[ 0 ] with ( Path ( logging_and_save_dir ) / \"test_metrics.json\" ) . open ( \"w\" ) as fp : json . dump ( test_metrics , fp , indent = 2 ) if trainer . datamodule . val_dataloader () is not None : logger . info ( \"Calculating metrics on validation set.\" ) val_metrics = trainer . validate ( dataloaders = trainer . datamodule . val_dataloader ())[ 0 ] with ( Path ( logging_and_save_dir ) / \"val_metrics.json\" ) . open ( \"w\" ) as fp : json . dump ( val_metrics , fp , indent = 2 ) return trainer validate_species ( model : ZambaVideoClassificationLightningModule , data_module : ZambaDataModule ) \u00b6 Source code in zamba/models/model_manager.py def validate_species ( model : ZambaVideoClassificationLightningModule , data_module : ZambaDataModule ): conflicts = [] for dataloader_name , dataloader in zip ( ( \"Train\" , \"Val\" , \"Test\" ), ( data_module . train_dataloader (), data_module . val_dataloader (), data_module . test_dataloader (), ), ): if ( dataloader is not None ) and ( dataloader . dataset . species != model . species ): conflicts . append ( f \"\"\" { dataloader_name } dataset includes: \\n { \", \" . join ( dataloader . dataset . species ) } \\n \"\"\" ) if len ( conflicts ) > 0 : conflicts . append ( f \"\"\"Model predicts: \\n { \", \" . join ( model . species ) } \"\"\" ) conflict_msg = \" \\n\\n \" . join ( conflicts ) raise ValueError ( f \"\"\"Dataloader species and model species do not match. \\n\\n { conflict_msg } \"\"\" )","title":"zamba.models.model_manager"},{"location":"api-reference/models-model_manager/#zambamodelsmodel_manager","text":"","title":"zamba.models.model_manager"},{"location":"api-reference/models-model_manager/#zamba.models.model_manager-classes","text":"","title":"Classes"},{"location":"api-reference/models-model_manager/#zamba.models.model_manager-functions","text":"","title":"Functions"},{"location":"api-reference/models-slowfast_models/","text":"zamba.models.slowfast_models \u00b6 Classes \u00b6 SlowFast ( ZambaVideoClassificationLightningModule ) \u00b6 Pretrained SlowFast model for fine-tuning with the following architecture: Input -> SlowFast Base (including trainable Backbone) -> Res Basic Head -> Output Attributes: Name Type Description backbone torch.nn.Module When scheduling the backbone to train with the BackboneFinetune callback, this indicates the trainable part of the base. base torch.nn.Module The entire model prior to the head. head torch.nn.Module The trainable head. _backbone_output_dim int Dimensionality of the backbone output (and head input). Attributes \u00b6 CHECKPOINT_HYPER_PARAMS_KEY inherited \u00b6 CHECKPOINT_HYPER_PARAMS_NAME inherited \u00b6 CHECKPOINT_HYPER_PARAMS_TYPE inherited \u00b6 T_destination inherited \u00b6 automatic_optimization : bool inherited property writable \u00b6 If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch : int inherited property readonly \u00b6 The current epoch in the Trainer. If no Trainer is attached, this propery is 0. datamodule : Any inherited property writable \u00b6 device : Union [ str , torch . device ] inherited property readonly \u00b6 dtype : Union [ str , torch . dtype ] inherited property writable \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. example_input_array : Any inherited property writable \u00b6 The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank : int inherited property readonly \u00b6 The index of the current process across all nodes and devices. global_step : int inherited property readonly \u00b6 Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams : Union [ pytorch_lightning . utilities . parsing . AttributeDict , dict , argparse . Namespace ] inherited property readonly \u00b6 hparams_initial : AttributeDict inherited property readonly \u00b6 loaded_optimizer_states_dict : dict inherited property writable \u00b6 local_rank : int inherited property readonly \u00b6 The index of the current process within a single node. logger inherited property readonly \u00b6 Reference to the logger object in the Trainer. model_size : float inherited property readonly \u00b6 The model's size in megabytes. The computation includes everything in the :meth: ~torch.nn.Module.state_dict , i.e., by default the parameteters and buffers. on_gpu inherited property readonly \u00b6 Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps : int inherited property writable \u00b6 Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the :meth: training_step receives an additional argument hiddens and is expected to return a hidden state. Methods \u00b6 __init__ ( self , backbone_mode : str = 'train' , post_backbone_dropout : Optional [ float ] = None , output_with_global_average : bool = True , head_dropout_rate : Optional [ float ] = None , head_hidden_layer_sizes : Optional [ Tuple [ int ]] = None , finetune_from : Union [ str , os . PathLike ] = None , ** kwargs ) special \u00b6 Initializes the SlowFast model. Parameters: Name Type Description Default backbone_mode str If \"eval\", treat the backbone as a feature extractor and set to evaluation mode in all forward passes. 'train' post_backbone_dropout float Dropout that operates on the output of the backbone + pool (before the fully-connected layer in the head). None output_with_global_average bool If True, apply an adaptive average pooling operation after the fully-connected layer in the head. True head_dropout_rate float Optional dropout rate applied after backbone and between projection layers in the head. None head_hidden_layer_sizes tuple of int If not None, the size of hidden layers in the head multilayer perceptron. None finetune_from pathlike or str If not None, load an existing model from the path and resume training from an existing model. None Source code in zamba/models/slowfast_models.py def __init__ ( self , backbone_mode : str = \"train\" , post_backbone_dropout : Optional [ float ] = None , output_with_global_average : bool = True , head_dropout_rate : Optional [ float ] = None , head_hidden_layer_sizes : Optional [ Tuple [ int ]] = None , finetune_from : Optional [ Union [ os . PathLike , str ]] = None , ** kwargs , ): \"\"\"Initializes the SlowFast model. Args: backbone_mode (str): If \"eval\", treat the backbone as a feature extractor and set to evaluation mode in all forward passes. post_backbone_dropout (float, optional): Dropout that operates on the output of the backbone + pool (before the fully-connected layer in the head). output_with_global_average (bool): If True, apply an adaptive average pooling operation after the fully-connected layer in the head. head_dropout_rate (float, optional): Optional dropout rate applied after backbone and between projection layers in the head. head_hidden_layer_sizes (tuple of int): If not None, the size of hidden layers in the head multilayer perceptron. finetune_from (pathlike or str, optional): If not None, load an existing model from the path and resume training from an existing model. \"\"\" super () . __init__ ( ** kwargs ) if finetune_from is None : self . initialize_from_torchub () else : model = self . load_from_checkpoint ( finetune_from ) self . _backbone_output_dim = model . head . proj . in_features self . backbone = model . backbone self . base = model . base for param in self . base . parameters (): param . requires_grad = False head = ResNetBasicHead ( proj = build_multilayer_perceptron ( self . _backbone_output_dim , head_hidden_layer_sizes , self . num_classes , activation = torch . nn . ReLU , dropout = head_dropout_rate , output_activation = None , ), activation = None , pool = None , dropout = None if post_backbone_dropout is None else torch . nn . Dropout ( post_backbone_dropout ), output_pool = torch . nn . AdaptiveAvgPool3d ( 1 ), ) self . backbone_mode = backbone_mode self . head = head self . save_hyperparameters ( \"backbone_mode\" , \"head_dropout_rate\" , \"head_hidden_layer_sizes\" , \"output_with_global_average\" , \"post_backbone_dropout\" , ) add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/models/slowfast_models.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7f45559664f0 >> ) -> None inherited \u00b6 Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Parameters: Name Type Description Default queue <bound method BaseContext.SimpleQueue of <multiprocessing.context.DefaultContext object at 0x7f45559664f0>> the instance of the queue to append the data. required Source code in zamba/models/slowfast_models.py def add_to_queue ( self , queue : torch . multiprocessing . SimpleQueue ) -> None : \"\"\" Appends the :attr:`trainer.callback_metrics` dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. \"\"\" callback_metrics : dict = apply_to_collection ( self . trainer . callback_metrics , torch . Tensor , lambda x : x . cpu () . numpy () ) # send as numpy to avoid issues with memory sharing queue . put ( callback_metrics ) aggregate_step_outputs ( outputs : Dict [ str , numpy . ndarray ]) -> Tuple [ numpy . ndarray , numpy . ndarray , numpy . ndarray ] inherited \u00b6 Source code in zamba/models/slowfast_models.py @staticmethod def aggregate_step_outputs ( outputs : Dict [ str , np . ndarray ] ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: y_true = np . vstack ([ output [ \"y_true\" ] for output in outputs ]) y_pred = np . vstack ([ output [ \"y_pred\" ] for output in outputs ]) y_proba = np . vstack ([ output [ \"y_proba\" ] for output in outputs ]) return y_true , y_pred , y_proba all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) inherited \u00b6 Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Parameters: Name Type Description Default data Union[torch.Tensor, Dict, List, Tuple] int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. required group Optional[Any] the process group to gather results from. Defaults to all processes (world) None sync_grads bool flag that allows users to synchronize gradients for the all_gather operation False Returns: Type Description A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. Source code in zamba/models/slowfast_models.py def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ): r \"\"\" Allows users to call ``self.all_gather()`` from the LightningModule, thus making the ``all_gather`` operation accelerator agnostic. ``all_gather`` is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. \"\"\" group = group if group is not None else torch . distributed . group . WORLD all_gather = self . trainer . accelerator . all_gather data = convert_to_tensors ( data , device = self . device ) return apply_to_collection ( data , torch . Tensor , all_gather , group = group , sync_grads = sync_grads ) apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/models/slowfast_models.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self backward ( self , loss : Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None inherited \u00b6 Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss Tensor The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). required optimizer Optional[torch.optim.optimizer.Optimizer] Current optimizer being used. None if using manual optimization. required optimizer_idx Optional[int] Index of the current optimizer being used. None if using manual optimization. required Example:: def backward(self, loss, optimizer, optimizer_idx): loss.backward() Source code in zamba/models/slowfast_models.py def backward ( self , loss : Tensor , optimizer : Optional [ Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None : \"\"\" Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own implementation if you need to. Args: loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). optimizer: Current optimizer being used. ``None`` if using manual optimization. optimizer_idx: Index of the current optimizer being used. ``None`` if using manual optimization. Example:: def backward(self, loss, optimizer, optimizer_idx): loss.backward() \"\"\" loss . backward ( * args , ** kwargs ) bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/models/slowfast_models.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/models/slowfast_models.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module compute_and_log_metrics ( self , y_true : ndarray , y_pred : ndarray , y_proba : ndarray , subset : str ) inherited \u00b6 Source code in zamba/models/slowfast_models.py def compute_and_log_metrics ( self , y_true : np . ndarray , y_pred : np . ndarray , y_proba : np . ndarray , subset : str ): self . log ( f \" { subset } _macro_f1\" , f1_score ( y_true , y_pred , average = \"macro\" , zero_division = 0 )) # if only two classes, skip top_k accuracy since not enough classes if self . num_classes > 2 : for k in DEFAULT_TOP_K : if k < self . num_classes : self . log ( f \" { subset } _top_ { k } _accuracy\" , top_k_accuracy_score ( y_true . argmax ( axis = 1 ), # top k accuracy only supports single label case y_proba , labels = np . arange ( y_proba . shape [ 1 ]), k = k , ), ) else : self . log ( f \" { subset } _accuracy\" , accuracy_score ( y_true , y_pred )) for metric_name , label , metric in compute_species_specific_metrics ( y_true , y_pred , self . species ): self . log ( f \"species/ { subset } _ { metric_name } / { label } \" , metric ) configure_callbacks ( self ) inherited \u00b6 Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Returns: Type Description A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] !!! note Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here. Source code in zamba/models/slowfast_models.py def configure_callbacks ( self ): \"\"\" Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth:`~pytorch_lightning.callbacks.base.Callback.on_init_start` will never be invoked on the new callbacks returned here. \"\"\" return [] configure_optimizers ( self ) inherited \u00b6 Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. Source code in zamba/models/slowfast_models.py def configure_optimizers ( self ): \"\"\" Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. \"\"\" optim = self . _get_optimizer () if self . scheduler is None : return optim else : return { \"optimizer\" : optim , \"lr_scheduler\" : self . scheduler ( optim , ** ({} if self . scheduler_params is None else self . scheduler_params ) ), } configure_sharded_model ( self ) -> None inherited \u00b6 Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. The accelerator manages whether to call this hook at every given stage. For sharded plugins where model parallelism is required, the hook is usually on called once to initialize the sharded parameters, and not called again in the same process. By default for accelerators/plugins that do not use model sharding techniques, this hook is called during each fit/val/test/predict stages. Source code in zamba/models/slowfast_models.py def configure_sharded_model ( self ) -> None : \"\"\" Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. The accelerator manages whether to call this hook at every given stage. For sharded plugins where model parallelism is required, the hook is usually on called once to initialize the sharded parameters, and not called again in the same process. By default for accelerators/plugins that do not use model sharding techniques, this hook is called during each fit/val/test/predict stages. \"\"\" cpu ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves all model parameters and buffers to the CPU. Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def cpu ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" self . __update_properties ( device = torch . device ( \"cpu\" )) return super () . cpu () cuda ( self , device : Union [ torch . device , int ] = None ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device Union[torch.device, int] if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def cuda ( self , device : Optional [ Union [ torch . device , int ]] = None ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self \"\"\" if device is None or isinstance ( device , int ): device = torch . device ( \"cuda\" , index = device ) self . __update_properties ( device = device ) return super () . cuda ( device = device ) double ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def double ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . double ) return super () . double () eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/models/slowfast_models.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def float ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . float ) return super () . float () forward ( self , x , * args , ** kwargs ) \u00b6 Same as :meth: torch.nn.Module.forward() . Parameters: Name Type Description Default *args Whatever you decide to pass into the forward method. () **kwargs Keyword arguments are also possible. {} Returns: Type Description Your model's output Source code in zamba/models/slowfast_models.py def forward ( self , x , * args , ** kwargs ): if self . backbone_mode == \"eval\" : self . base . eval () x = self . base ( x ) return self . head ( x ) freeze ( self ) -> None inherited \u00b6 Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() Source code in zamba/models/slowfast_models.py def freeze ( self ) -> None : r \"\"\" Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() \"\"\" for param in self . parameters (): param . requires_grad = False self . eval () get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/models/slowfast_models.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/models/slowfast_models.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7f45559664f0 >> ) -> None inherited \u00b6 Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Parameters: Name Type Description Default queue <bound method BaseContext.SimpleQueue of <multiprocessing.context.DefaultContext object at 0x7f45559664f0>> the instance of the queue from where to get the data. required Source code in zamba/models/slowfast_models.py def get_from_queue ( self , queue : torch . multiprocessing . SimpleQueue ) -> None : \"\"\" Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we cast back the data to ``torch.Tensor``. Args: queue: the instance of the queue from where to get the data. \"\"\" # NOTE: `add_to_queue` needs to be called before callback_metrics : dict = queue . get () self . trainer . callback_metrics . update ( apply_to_collection ( callback_metrics , np . ndarray , lambda x : torch . tensor ( x )) ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/models/slowfast_models.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] inherited \u00b6 Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. .. code-block:: Epoch 1: 4%|\u258e | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10] Here is an example how to override the defaults: .. code-block:: python def get_progress_bar_dict(self): # don't show the version number items = super().get_progress_bar_dict() items.pop(\"v_num\", None) return items Returns: Type Description Dict[str, Union[int, str]] Dictionary with the items to be displayed in the progress bar. Source code in zamba/models/slowfast_models.py def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]: r \"\"\" Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. .. code-block:: Epoch 1: 4%|\u258e | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10] Here is an example how to override the defaults: .. code-block:: python def get_progress_bar_dict(self): # don't show the version number items = super().get_progress_bar_dict() items.pop(\"v_num\", None) return items Return: Dictionary with the items to be displayed in the progress bar. \"\"\" # call .item() only once but store elements without graphs running_train_loss = self . trainer . fit_loop . running_loss . mean () avg_training_loss = None if running_train_loss is not None : avg_training_loss = running_train_loss . cpu () . item () elif self . automatic_optimization : avg_training_loss = float ( \"NaN\" ) tqdm_dict = {} if avg_training_loss is not None : tqdm_dict [ \"loss\" ] = f \" { avg_training_loss : .3g } \" module_tbptt_enabled = self . truncated_bptt_steps > 0 trainer_tbptt_enabled = self . trainer . truncated_bptt_steps is not None and self . trainer . truncated_bptt_steps > 0 if module_tbptt_enabled or trainer_tbptt_enabled : tqdm_dict [ \"split_idx\" ] = self . trainer . fit_loop . split_idx if self . trainer . logger is not None and self . trainer . logger . version is not None : version = self . trainer . logger . version # show last 4 places of long version strings version = version [ - 4 :] if isinstance ( version , str ) else version tqdm_dict [ \"v_num\" ] = version return tqdm_dict get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/models/slowfast_models.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ] inherited \u00b6 Compute each parameter's gradient's norm and their overall norm. .. deprecated:: v1.3 Will be removed in v1.5.0. Use :func: pytorch_lightning.utilities.grads.grad_norm instead. Source code in zamba/models/slowfast_models.py def grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ]: \"\"\"Compute each parameter's gradient's norm and their overall norm. .. deprecated:: v1.3 Will be removed in v1.5.0. Use :func:`pytorch_lightning.utilities.grads.grad_norm` instead. \"\"\" rank_zero_deprecation ( \"LightningModule.grad_norm is deprecated in v1.3 and will be removed in v1.5.\" \" Use grad_norm from pytorch_lightning.utilities.grads instead.\" ) return new_grad_norm ( self , norm_type ) half ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def half ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . half ) return super () . half () initialize_from_torchub ( self ) \u00b6 Loads SlowFast model from torchhub and prepares ZambaVideoClassificationLightningModule by removing the head and setting the backbone and base. Source code in zamba/models/slowfast_models.py def initialize_from_torchub ( self ): \"\"\"Loads SlowFast model from torchhub and prepares ZambaVideoClassificationLightningModule by removing the head and setting the backbone and base.\"\"\" # workaround for pytorch bug torch . hub . _validate_not_a_forked_repo = lambda a , b , c : True base = torch . hub . load ( \"facebookresearch/pytorchvideo:0.1.3\" , model = \"slowfast_r50\" , pretrained = True ) self . _backbone_output_dim = base . blocks [ - 1 ] . proj . in_features base . blocks = base . blocks [: - 1 ] # Remove the pre-trained head # self.backbone attribute lets `BackboneFinetune` freeze and unfreeze that module self . backbone = base . blocks [ - 2 :] self . base = base load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/models/slowfast_models.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None inherited \u00b6 Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name str key to log required value Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number, Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number]]] value to log. Can be a float , Tensor , Metric , or a dictionary of the former. required prog_bar bool if True logs to the progress bar False logger bool if True logs to the logger True on_step Optional[bool] if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch Optional[bool] if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx Union[str, Callable] reduction function over step values for end of epoch. :meth: torch.mean by default. 'default' enable_graph bool if True, will not auto detach the graph False sync_dist bool if True, reduces the metric across GPUs/TPUs False sync_dist_group Optional[Any] the ddp group to sync across None add_dataloader_idx bool if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values True batch_size Optional[int] Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute Optional[str] To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only Optional[bool] Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None Source code in zamba/models/slowfast_models.py def log ( self , name : str , value : _METRIC_COLLECTION , prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = \"default\" , # TODO: change to 'mean' when `sync_dist_op` is removed in 1.6 tbptt_reduce_fx : Optional = None , # noqa: Remove in 1.6 tbptt_pad_token : Optional = None , # noqa: Remove in 1.6 enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , # noqa: Remove in 1.6 sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None , ) -> None : \"\"\" Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: ``*`` also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step*\", \"F\", \"T\", \"F\", \"T\" \"validation_step_end*\", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Args: name: key to log value: value to log. Can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former. prog_bar: if True logs to the progress bar logger: if True logs to the logger on_step: if True logs at this step. None auto-logs at the training_step but not validation/test_step on_epoch: if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default. enable_graph: if True, will not auto detach the graph sync_dist: if True, reduces the metric across GPUs/TPUs sync_dist_group: the ddp group to sync across add_dataloader_idx: if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values batch_size: Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. metric_attribute: To restore the metric state, Lightning requires the reference of the :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute. rank_zero_only: Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. \"\"\" if tbptt_reduce_fx is not None : rank_zero_deprecation ( \"`self.log(tbptt_reduce_fx=...)` is no longer supported. The flag will be removed in v1.6.\" \" Please, open a discussion explaining your use-case in\" \" `https://github.com/PyTorchLightning/pytorch-lightning/discussions`\" ) if tbptt_pad_token is not None : rank_zero_deprecation ( \"`self.log(tbptt_pad_token=...)` is no longer supported. The flag will be removed in v1.6.\" \" Please, open a discussion explaining your use-case in\" \" `https://github.com/PyTorchLightning/pytorch-lightning/discussions`\" ) if sync_dist_op is not None : rank_zero_deprecation ( f \"`self.log(sync_dist_op=' { sync_dist_op } ')` is deprecated and will be removed in v.1.6.\" f \" Use `self.log(reduce_fx= { sync_dist_op } )` instead.\" ) if reduce_fx == \"default\" : reduce_fx = sync_dist_op elif reduce_fx == \"default\" : reduce_fx = \"mean\" # check for invalid values apply_to_collection ( value , dict , self . __check_not_nested , name ) apply_to_collection ( value , object , self . __check_allowed , name , value , wrong_dtype = ( numbers . Number , Metric , Tensor , dict ) ) # set the default depending on the fx_name on_step = self . __auto_choose_log_on_step ( on_step ) on_epoch = self . __auto_choose_log_on_epoch ( on_epoch ) results = self . trainer . _results assert results is not None assert self . _current_fx_name is not None FxValidator . check_logging ( self . _current_fx_name , on_step = on_step , on_epoch = on_epoch ) # make sure user doesn't introduce logic for multi-dataloaders if \"/dataloader_idx_\" in name : raise MisconfigurationException ( f \"You called `self.log` with the key ` { name } `\" \" but it should not contain information about `dataloader_idx`\" ) value = apply_to_collection ( value , numbers . Number , self . __to_tensor ) if self . trainer . logger_connector . should_reset_tensors ( self . _current_fx_name ): # if we started a new epoch (running it's first batch) the hook name has changed # reset any tensors for the new hook name results . reset ( metrics = False , fx = self . _current_fx_name ) if metric_attribute is None and isinstance ( value , Metric ): if self . _metric_attributes is None : # compute once self . _metric_attributes = { id ( module ): name for name , module in self . named_modules () if isinstance ( module , Metric ) } if not self . _metric_attributes : raise MisconfigurationException ( \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\" \" You can fix this by setting an attribute for the metric in your `LightningModule`.\" ) # try to find the passed metric in the LightningModule metric_attribute = self . _metric_attributes . get ( id ( value ), None ) if metric_attribute is None : raise MisconfigurationException ( \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\" f \" You can fix this by calling `self.log( { name } , ..., metric_attribute=name)` where `name` is one\" f \" of { list ( self . _metric_attributes . values ()) } \" ) results . log ( self . _current_fx_name , name , value , prog_bar = prog_bar , logger = logger , on_step = on_step , on_epoch = on_epoch , reduce_fx = reduce_fx , enable_graph = enable_graph , dataloader_idx = ( self . _current_dataloader_idx if add_dataloader_idx else None ), batch_size = batch_size , sync_dist = sync_dist and distributed_available (), sync_dist_fn = self . trainer . training_type_plugin . reduce or sync_ddp , sync_dist_group = sync_dist_group , metric_attribute = metric_attribute , rank_zero_only = rank_zero_only , ) self . trainer . logger_connector . _current_fx = self . _current_fx_name log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True ) -> None inherited \u00b6 Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number, Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number]]]] key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. required prog_bar bool if True logs to the progress base False logger bool if True logs to the logger True on_step Optional[bool] if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch Optional[bool] if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx Union[str, Callable] reduction function over step values for end of epoch. :meth: torch.mean by default. 'default' enable_graph bool if True, will not auto detach the graph False sync_dist bool if True, reduces the metric across GPUs/TPUs False sync_dist_group Optional[Any] the ddp group sync across None add_dataloader_idx bool if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values True Source code in zamba/models/slowfast_models.py def log_dict ( self , dictionary : Mapping [ str , _METRIC_COLLECTION ], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = \"default\" , # TODO: change to 'mean' when `sync_dist_op` is removed in 1.6 tbptt_reduce_fx : Optional [ Any ] = None , # noqa: Remove in 1.6 tbptt_pad_token : Optional [ Any ] = None , # noqa: Remove in 1.6 enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , # noqa: Remove in 1.6 sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , ) -> None : \"\"\" Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Args: dictionary: key value pairs. The values can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former. prog_bar: if True logs to the progress base logger: if True logs to the logger on_step: if True logs at this step. None auto-logs for training_step but not validation/test_step on_epoch: if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default. enable_graph: if True, will not auto detach the graph sync_dist: if True, reduces the metric across GPUs/TPUs sync_dist_group: the ddp group sync across add_dataloader_idx: if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values \"\"\" for k , v in dictionary . items (): self . log ( name = k , value = v , prog_bar = prog_bar , logger = logger , on_step = on_step , on_epoch = on_epoch , reduce_fx = reduce_fx , enable_graph = enable_graph , sync_dist = sync_dist , sync_dist_group = sync_dist_group , sync_dist_op = sync_dist_op , tbptt_pad_token = tbptt_pad_token , tbptt_reduce_fx = tbptt_reduce_fx , add_dataloader_idx = add_dataloader_idx , ) log_grad_norm ( self , grad_norm_dict : Dict [ str , torch . Tensor ]) -> None inherited \u00b6 Override this method to change the default behaviour of log_grad_norm . Parameters: Name Type Description Default grad_norm_dict Dict[str, torch.Tensor] Dictionary containing current grad norm metrics required Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) Source code in zamba/models/slowfast_models.py def log_grad_norm ( self , grad_norm_dict : Dict [ str , torch . Tensor ]) -> None : \"\"\"Override this method to change the default behaviour of ``log_grad_norm``. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) \"\"\" self . log_dict ( grad_norm_dict , on_step = True , on_epoch = True , prog_bar = True , logger = True ) lr_schedulers ( self ) -> Union [ Any , List [ Any ]] inherited \u00b6 Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no schedulers were returned in meth: configure_optimizers . Source code in zamba/models/slowfast_models.py def lr_schedulers ( self ) -> Optional [ Union [ Any , List [ Any ]]]: \"\"\" Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no schedulers were returned in :meth:`configure_optimizers`. \"\"\" if not self . trainer . lr_schedulers : return None # ignore other keys \"interval\", \"frequency\", etc. lr_schedulers = [ s [ \"scheduler\" ] for s in self . trainer . lr_schedulers ] # single scheduler if len ( lr_schedulers ) == 1 : return lr_schedulers [ 0 ] # multiple schedulers return lr_schedulers manual_backward ( self , loss : Tensor , * args , ** kwargs ) -> None inherited \u00b6 Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss Tensor The tensor on which to compute gradients. Must have a graph attached. required *args Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward () **kwargs Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward {} Source code in zamba/models/slowfast_models.py def manual_backward ( self , loss : Tensor , * args , ** kwargs ) -> None : \"\"\" Call this directly from your :meth:`training_step` when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref:`manual optimization<common/optimizers:Manual optimization>` for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Args: loss: The tensor on which to compute gradients. Must have a graph attached. *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward` **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward` \"\"\" # make sure we're using manual opt self . _verify_is_manual_optimization ( \"manual_backward\" ) # backward self . trainer . fit_loop . epoch_loop . batch_loop . backward ( loss , None , None , * args , ** kwargs ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/models/slowfast_models.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/models/slowfast_models.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/models/slowfast_models.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/models/slowfast_models.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/models/slowfast_models.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem on_after_backward ( self ) -> None inherited \u00b6 Called after loss.backward() and before optimizers are stepped. !!! note If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients. Source code in zamba/models/slowfast_models.py def on_after_backward ( self ) -> None : \"\"\" Called after ``loss.backward()`` and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the ``on_before_optimizer_step`` if you need the unscaled gradients. \"\"\" on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any inherited \u00b6 Override to alter or apply batch augmentations to your batch after it is transferred to the device. !!! note To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be altered or augmented. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch See Also: - :meth: on_before_batch_transfer - :meth: transfer_batch_to_device Source code in zamba/models/slowfast_models.py def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any : \"\"\" Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be altered or augmented. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`on_before_batch_transfer` - :meth:`transfer_batch_to_device` \"\"\" return batch on_before_backward ( self , loss : Tensor ) -> None inherited \u00b6 Called before loss.backward() . Parameters: Name Type Description Default loss Tensor Loss divided by number of batches for gradient accumulation and scaled if using native AMP. required Source code in zamba/models/slowfast_models.py def on_before_backward ( self , loss : torch . Tensor ) -> None : \"\"\" Called before ``loss.backward()``. Args: loss: Loss divided by number of batches for gradient accumulation and scaled if using native AMP. \"\"\" pass on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any inherited \u00b6 Override to alter or apply batch augmentations to your batch before it is transferred to the device. !!! note To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be altered or augmented. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch See Also: - :meth: on_after_batch_transfer - :meth: transfer_batch_to_device Source code in zamba/models/slowfast_models.py def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any : \"\"\" Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be altered or augmented. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`on_after_batch_transfer` - :meth:`transfer_batch_to_device` \"\"\" return batch on_before_optimizer_step ( self , optimizer : Optimizer , optimizer_idx : int ) -> None inherited \u00b6 Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. Parameters: Name Type Description Default optimizer Optimizer Current optimizer being used. required optimizer_idx int Index of the current optimizer being used. required Example:: def on_before_optimizer_step(self, optimizer, optimizer_idx): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) Source code in zamba/models/slowfast_models.py def on_before_optimizer_step ( self , optimizer : Optimizer , optimizer_idx : int ) -> None : \"\"\" Called before ``optimizer.step()``. The hook is only called if gradients do not need to be accumulated. See: :paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`. If using native AMP, the loss will be unscaled before calling this hook. See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__ for more information on the scaling of gradients. Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. Example:: def on_before_optimizer_step(self, optimizer, optimizer_idx): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) \"\"\" on_before_zero_grad ( self , optimizer : Optimizer ) -> None inherited \u00b6 Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer Optimizer The optimizer for which grads should be zeroed. required Source code in zamba/models/slowfast_models.py def on_before_zero_grad ( self , optimizer : Optimizer ) -> None : \"\"\" Called after ``training_step()`` and before ``optimizer.zero_grad()``. Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Args: optimizer: The optimizer for which grads should be zeroed. \"\"\" on_epoch_end ( self ) -> None inherited \u00b6 Called when either of train/val/test epoch ends. Source code in zamba/models/slowfast_models.py def on_epoch_end ( self ) -> None : \"\"\" Called when either of train/val/test epoch ends. \"\"\" on_epoch_start ( self ) -> None inherited \u00b6 Called when either of train/val/test epoch begins. Source code in zamba/models/slowfast_models.py def on_epoch_start ( self ) -> None : \"\"\" Called when either of train/val/test epoch begins. \"\"\" on_fit_end ( self ) -> None inherited \u00b6 Called at the very end of fit. If on DDP it is called on every process Source code in zamba/models/slowfast_models.py def on_fit_end ( self ) -> None : \"\"\" Called at the very end of fit. If on DDP it is called on every process \"\"\" on_fit_start ( self ) -> None inherited \u00b6 Called at the very beginning of fit. If on DDP it is called on every process Source code in zamba/models/slowfast_models.py def on_fit_start ( self ) -> None : \"\"\" Called at the very beginning of fit. If on DDP it is called on every process \"\"\" on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary with variables from the checkpoint. required Source code in zamba/models/slowfast_models.py def on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\" on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. required Source code in zamba/models/slowfast_models.py def on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\" on_load_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Do something with the checkpoint. Gives model a chance to load something before state_dict is restored. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary with variables from the checkpoint. required Source code in zamba/models/slowfast_models.py def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Do something with the checkpoint. Gives model a chance to load something before ``state_dict`` is restored. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\" on_post_move_to_device ( self ) -> None inherited \u00b6 Called in the parameter_validation decorator after :meth: ~pytorch_lightning.core.LightningModule.to is called. This is a good place to tie weights between modules after moving them to a device. Can be used when training models with weight sharing properties on TPU. Addresses the handling of shared weights on TPU: https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks Example:: def on_post_move_to_device(self): self.decoder.weight = self.encoder.weight Source code in zamba/models/slowfast_models.py def on_post_move_to_device ( self ) -> None : \"\"\" Called in the ``parameter_validation`` decorator after :meth:`~pytorch_lightning.core.LightningModule.to` is called. This is a good place to tie weights between modules after moving them to a device. Can be used when training models with weight sharing properties on TPU. Addresses the handling of shared weights on TPU: https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks Example:: def on_post_move_to_device(self): self.decoder.weight = self.encoder.weight \"\"\" on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the predict loop after the batch. Parameters: Name Type Description Default outputs Optional[Any] The outputs of predict_step_end(test_step(x)) required batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/slowfast_models.py def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the predict loop after the batch. Args: outputs: The outputs of predict_step_end(test_step(x)) batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/slowfast_models.py def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the predict loop before anything happens for that batch. Args: batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_predict_dataloader ( self ) -> None inherited \u00b6 Called before requesting the predict dataloader. .. deprecated:: v1.5 :meth: on_predict_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: predict_dataloader() directly. Source code in zamba/models/slowfast_models.py def on_predict_dataloader ( self ) -> None : \"\"\"Called before requesting the predict dataloader. .. deprecated:: v1.5 :meth:`on_predict_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`predict_dataloader()` directly. \"\"\" on_predict_end ( self ) -> None inherited \u00b6 Called at the end of predicting. Source code in zamba/models/slowfast_models.py def on_predict_end ( self ) -> None : \"\"\" Called at the end of predicting. \"\"\" on_predict_epoch_end ( self , results : List [ Any ]) -> None inherited \u00b6 Called at the end of predicting. Source code in zamba/models/slowfast_models.py def on_predict_epoch_end ( self , results : List [ Any ]) -> None : \"\"\" Called at the end of predicting. \"\"\" on_predict_epoch_start ( self ) -> None inherited \u00b6 Called at the beginning of predicting. Source code in zamba/models/slowfast_models.py def on_predict_epoch_start ( self ) -> None : \"\"\" Called at the beginning of predicting. \"\"\" on_predict_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the predict loop Source code in zamba/models/slowfast_models.py def on_predict_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the predict loop \"\"\" self . trainer . model . eval () on_predict_start ( self ) -> None inherited \u00b6 Called at the beginning of predicting. Source code in zamba/models/slowfast_models.py def on_predict_start ( self ) -> None : \"\"\" Called at the beginning of predicting. \"\"\" on_pretrain_routine_end ( self ) -> None inherited \u00b6 Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start Source code in zamba/models/slowfast_models.py def on_pretrain_routine_end ( self ) -> None : \"\"\" Called at the end of the pretrain routine (between fit and train start). - fit - pretrain_routine start - pretrain_routine end - training_start \"\"\" on_pretrain_routine_start ( self ) -> None inherited \u00b6 Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start Source code in zamba/models/slowfast_models.py def on_pretrain_routine_start ( self ) -> None : \"\"\" Called at the beginning of the pretrain routine (between fit and train start). - fit - pretrain_routine start - pretrain_routine end - training_start \"\"\" on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Give the model a chance to add something to the checkpoint. state_dict is already there. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. required Source code in zamba/models/slowfast_models.py def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Give the model a chance to add something to the checkpoint. ``state_dict`` is already there. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\" on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the test loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of test_step_end(test_step(x)) required batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/slowfast_models.py def on_test_batch_end ( self , outputs : Optional [ STEP_OUTPUT ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the test loop after the batch. Args: outputs: The outputs of test_step_end(test_step(x)) batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/slowfast_models.py def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the test loop before anything happens for that batch. Args: batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_test_dataloader ( self ) -> None inherited \u00b6 Called before requesting the test dataloader. .. deprecated:: v1.5 :meth: on_test_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: test_dataloader() directly. Source code in zamba/models/slowfast_models.py def on_test_dataloader ( self ) -> None : \"\"\"Called before requesting the test dataloader. .. deprecated:: v1.5 :meth:`on_test_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`test_dataloader()` directly. \"\"\" on_test_end ( self ) -> None inherited \u00b6 Called at the end of testing. Source code in zamba/models/slowfast_models.py def on_test_end ( self ) -> None : \"\"\" Called at the end of testing. \"\"\" on_test_epoch_end ( self ) -> None inherited \u00b6 Called in the test loop at the very end of the epoch. Source code in zamba/models/slowfast_models.py def on_test_epoch_end ( self ) -> None : \"\"\" Called in the test loop at the very end of the epoch. \"\"\" on_test_epoch_start ( self ) -> None inherited \u00b6 Called in the test loop at the very beginning of the epoch. Source code in zamba/models/slowfast_models.py def on_test_epoch_start ( self ) -> None : \"\"\" Called in the test loop at the very beginning of the epoch. \"\"\" on_test_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the test loop Source code in zamba/models/slowfast_models.py def on_test_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the test loop \"\"\" self . trainer . model . eval () on_test_model_train ( self ) -> None inherited \u00b6 Sets the model to train during the test loop Source code in zamba/models/slowfast_models.py def on_test_model_train ( self ) -> None : \"\"\" Sets the model to train during the test loop \"\"\" self . trainer . model . train () on_test_start ( self ) -> None inherited \u00b6 Called at the beginning of testing. Source code in zamba/models/slowfast_models.py def on_test_start ( self ) -> None : \"\"\" Called at the beginning of testing. \"\"\" on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the training loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of training_step_end(training_step(x)) required batch Any The batched data as it is returned by the training DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/slowfast_models.py def on_train_batch_end ( self , outputs : STEP_OUTPUT , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the training loop after the batch. Args: outputs: The outputs of training_step_end(training_step(x)) batch: The batched data as it is returned by the training DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_train_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the training DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/slowfast_models.py def on_train_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_train_dataloader ( self ) -> None inherited \u00b6 Called before requesting the train dataloader. .. deprecated:: v1.5 :meth: on_train_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: train_dataloader() directly. Source code in zamba/models/slowfast_models.py def on_train_dataloader ( self ) -> None : \"\"\"Called before requesting the train dataloader. .. deprecated:: v1.5 :meth:`on_train_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`train_dataloader()` directly. \"\"\" on_train_end ( self ) -> None inherited \u00b6 Called at the end of training before logger experiment is closed. Source code in zamba/models/slowfast_models.py def on_train_end ( self ) -> None : \"\"\" Called at the end of training before logger experiment is closed. \"\"\" on_train_epoch_end ( self , unused : Optional = None ) -> None inherited \u00b6 Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook Source code in zamba/models/slowfast_models.py def on_train_epoch_end ( self , unused : Optional = None ) -> None : \"\"\" Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: 1. Implement `training_epoch_end` in the LightningModule OR 2. Cache data across steps on the attribute(s) of the `LightningModule` and access them in this hook \"\"\" on_train_epoch_start ( self ) -> None inherited \u00b6 Called in the training loop at the very beginning of the epoch. Source code in zamba/models/slowfast_models.py def on_train_epoch_start ( self ) -> None : \"\"\" Called in the training loop at the very beginning of the epoch. \"\"\" on_train_start ( self ) inherited \u00b6 Called at the beginning of training after sanity check. Source code in zamba/models/slowfast_models.py def on_train_start ( self ): metrics = { \"val_macro_f1\" : {}} if self . num_classes > 2 : metrics . update ( { f \"val_top_ { k } _accuracy\" : {} for k in DEFAULT_TOP_K if k < self . num_classes } ) else : metrics . update ({ \"val_accuracy\" : {}}) # write hparams to hparams.yaml file, log metrics to tb hparams tab self . logger . log_hyperparams ( self . hparams , metrics ) on_val_dataloader ( self ) -> None inherited \u00b6 Called before requesting the val dataloader. .. deprecated:: v1.5 :meth: on_val_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: val_dataloader() directly. Source code in zamba/models/slowfast_models.py def on_val_dataloader ( self ) -> None : \"\"\"Called before requesting the val dataloader. .. deprecated:: v1.5 :meth:`on_val_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`val_dataloader()` directly. \"\"\" on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the validation loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of validation_step_end(validation_step(x)) required batch Any The batched data as it is returned by the validation DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/slowfast_models.py def on_validation_batch_end ( self , outputs : Optional [ STEP_OUTPUT ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the validation loop after the batch. Args: outputs: The outputs of validation_step_end(validation_step(x)) batch: The batched data as it is returned by the validation DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the validation DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/models/slowfast_models.py def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the validation loop before anything happens for that batch. Args: batch: The batched data as it is returned by the validation DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_validation_end ( self ) -> None inherited \u00b6 Called at the end of validation. Source code in zamba/models/slowfast_models.py def on_validation_end ( self ) -> None : \"\"\" Called at the end of validation. \"\"\" on_validation_epoch_end ( self ) -> None inherited \u00b6 Called in the validation loop at the very end of the epoch. Source code in zamba/models/slowfast_models.py def on_validation_epoch_end ( self ) -> None : \"\"\" Called in the validation loop at the very end of the epoch. \"\"\" on_validation_epoch_start ( self ) -> None inherited \u00b6 Called in the validation loop at the very beginning of the epoch. Source code in zamba/models/slowfast_models.py def on_validation_epoch_start ( self ) -> None : \"\"\" Called in the validation loop at the very beginning of the epoch. \"\"\" on_validation_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the val loop Source code in zamba/models/slowfast_models.py def on_validation_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the val loop \"\"\" self . trainer . model . eval () on_validation_model_train ( self ) -> None inherited \u00b6 Sets the model to train during the val loop Source code in zamba/models/slowfast_models.py def on_validation_model_train ( self ) -> None : \"\"\" Sets the model to train during the val loop \"\"\" self . trainer . model . train () on_validation_start ( self ) -> None inherited \u00b6 Called at the beginning of validation. Source code in zamba/models/slowfast_models.py def on_validation_start ( self ) -> None : \"\"\" Called at the beginning of validation. \"\"\" optimizer_step ( self , epoch : int = None , batch_idx : int = None , optimizer : Optimizer = None , optimizer_idx : int = None , optimizer_closure : Optional [ Callable ] = None , on_tpu : bool = None , using_native_amp : bool = None , using_lbfgs : bool = None ) -> None inherited \u00b6 Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . !!! warning If you are overriding this method, make sure that you pass the optimizer_closure parameter to optimizer.step() function as shown in the examples. This ensures that training_step() , optimizer.zero_grad() , backward() are called within the training loop. Parameters: Name Type Description Default epoch int Current epoch None batch_idx int Index of current batch None optimizer Optimizer A PyTorch optimizer None optimizer_idx int If you used multiple optimizers, this indexes into that list. None optimizer_closure Optional[Callable] Closure for all optimizers None on_tpu bool True if TPU backward is required None using_native_amp bool True if using native amp None using_lbfgs bool True if the matching optimizer is :class: torch.optim.LBFGS None Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) Source code in zamba/models/slowfast_models.py def optimizer_step ( self , epoch : int = None , batch_idx : int = None , optimizer : Optimizer = None , optimizer_idx : int = None , optimizer_closure : Optional [ Callable ] = None , on_tpu : bool = None , using_native_amp : bool = None , using_lbfgs : bool = None , ) -> None : r \"\"\" Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer. By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer. This method (and ``zero_grad()``) won't be called during the accumulation phase when ``Trainer(accumulate_grad_batches != 1)``. Warning: If you are overriding this method, make sure that you pass the ``optimizer_closure`` parameter to ``optimizer.step()`` function as shown in the examples. This ensures that ``training_step()``, ``optimizer.zero_grad()``, ``backward()`` are called within the training loop. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers on_tpu: ``True`` if TPU backward is required using_native_amp: ``True`` if using native amp using_lbfgs: True if the matching optimizer is :class:`torch.optim.LBFGS` Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) \"\"\" optimizer . step ( closure = optimizer_closure ) optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ) inherited \u00b6 Override this method to change the default behaviour of optimizer.zero_grad() . Parameters: Name Type Description Default epoch int Current epoch required batch_idx int Index of current batch required optimizer Optimizer A PyTorch optimizer required optimizer_idx int If you used multiple optimizers this indexes into that list. required Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example. Source code in zamba/models/slowfast_models.py def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ): \"\"\"Override this method to change the default behaviour of ``optimizer.zero_grad()``. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example. \"\"\" optimizer . zero_grad () optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] inherited \u00b6 Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer bool If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. True Returns: Type Description Union[torch.optim.optimizer.Optimizer, pytorch_lightning.core.optimizer.LightningOptimizer, List[torch.optim.optimizer.Optimizer], List[pytorch_lightning.core.optimizer.LightningOptimizer]] A single optimizer, or a list of optimizers in case multiple ones are present. Source code in zamba/models/slowfast_models.py def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ Optimizer , LightningOptimizer , List [ Optimizer ], List [ LightningOptimizer ]]: \"\"\" Returns the optimizer(s) that are being used during training. Useful for manual optimization. Args: use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a :class:`~pytorch_lightning.core.optimizer.LightningOptimizer` for automatic handling of precision and profiling. Returns: A single optimizer, or a list of optimizers in case multiple ones are present. \"\"\" if use_pl_optimizer : opts = list ( self . trainer . lightning_optimizers . values ()) else : opts = self . trainer . optimizers # single optimizer if isinstance ( opts , list ) and len ( opts ) == 1 and isinstance ( opts [ 0 ], ( Optimizer , LightningOptimizer )): return opts [ 0 ] # multiple opts return opts parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/models/slowfast_models.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. !!! note In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. Source code in zamba/models/slowfast_models.py def predict_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth:`prepare_data`. - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth:`predict` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" predict_step ( self , batch , batch_idx , dataloader_idx : Optional [ int ] = None ) inherited \u00b6 Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(accelerator=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Parameters: Name Type Description Default batch Current batch required batch_idx Index of current batch required dataloader_idx Optional[int] Index of the current dataloader None Returns: Type Description Predicted output Source code in zamba/models/slowfast_models.py def predict_step ( self , batch , batch_idx , dataloader_idx : Optional [ int ] = None ): x , y = batch y_hat = self ( x ) pred = torch . sigmoid ( y_hat ) . cpu () . numpy () return pred prepare_data ( self ) -> None inherited \u00b6 Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() Source code in zamba/models/slowfast_models.py def prepare_data ( self ) -> None : \"\"\" Use this to download and prepare data. .. warning:: DO NOT set state to the model (use `setup` instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): 1. Once per node. This is the default and is only called on LOCAL_RANK=0. 2. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() \"\"\" print ( self , * args , ** kwargs ) -> None inherited \u00b6 Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args The thing to print. The same as for Python's built-in print function. () **kwargs The same as for Python's built-in print function. {} Example:: def forward(self, x): self.print(x, 'in forward') Source code in zamba/models/slowfast_models.py def print ( self , * args , ** kwargs ) -> None : r \"\"\" Prints only from process 0. Use this in any distributed mode to log only once. Args: *args: The thing to print. The same as for Python's built-in print function. **kwargs: The same as for Python's built-in print function. Example:: def forward(self, x): self.print(x, 'in forward') \"\"\" if self . trainer . is_global_zero : progress_bar = self . trainer . progress_bar_callback if progress_bar is not None and progress_bar . is_enabled : progress_bar . print ( * args , ** kwargs ) else : print ( * args , ** kwargs ) register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/models/slowfast_models.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/models/slowfast_models.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/models/slowfast_models.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/models/slowfast_models.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/models/slowfast_models.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/models/slowfast_models.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self save_hyperparameters ( self , * args , * , ignore : Union [ Sequence [ str ], str ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None inherited \u00b6 Save arguments to hparams attribute. Parameters: Name Type Description Default args single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ () ignore Union[Sequence[str], str] an argument name or a list of argument names from class __init__ to be ignored None frame Optional[frame] a frame object. Default is None None logger bool Whether to send the hyperparameters to the logger. Default: True True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 Source code in zamba/models/slowfast_models.py def save_hyperparameters ( self , * args , ignore : Optional [ Union [ Sequence [ str ], str ]] = None , frame : Optional [ types . FrameType ] = None , logger : bool = True , ) -> None : \"\"\"Save arguments to ``hparams`` attribute. Args: args: single object of `dict`, `NameSpace` or `OmegaConf` or string names or arguments from class ``__init__`` ignore: an argument name or a list of argument names from class ``__init__`` to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 \"\"\" self . _log_hyperparams = logger # the frame needs to be created in this file. if not frame : frame = inspect . currentframe () . f_back save_hyperparameters ( self , * args , ignore = ignore , frame = frame ) set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/models/slowfast_models.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) setup ( self , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) Source code in zamba/models/slowfast_models.py def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Args: stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'`` Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) \"\"\" share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/models/slowfast_models.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/models/slowfast_models.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . core . memory . ModelSummary ] inherited \u00b6 Summarize this LightningModule. Parameters: Name Type Description Default mode Optional[str] Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of max_depth and will be removed in v1.6. 'top' max_depth Optional[int] The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. None Returns: Type Description Optional[pytorch_lightning.core.memory.ModelSummary] The model summary object Source code in zamba/models/slowfast_models.py def summarize ( self , mode : Optional [ str ] = \"top\" , max_depth : Optional [ int ] = None ) -> Optional [ ModelSummary ]: \"\"\" Summarize this LightningModule. Args: mode: Can be either ``'top'`` (summarize only direct submodules) or ``'full'`` (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object \"\"\" model_summary = None # temporary mapping from mode to max_depth if max_depth is None : if mode in ModelSummary . MODES : max_depth = ModelSummary . MODES [ mode ] rank_zero_deprecation ( f \"Argument `mode` in `LightningModule.summarize` is deprecated in v1.4\" f \" and will be removed in v1.6. Use `max_depth= { max_depth } ` to replicate `mode= { mode } ` behavior.\" ) model_summary = ModelSummary ( self , max_depth = max_depth ) elif mode is not None : raise MisconfigurationException ( f \"`mode` can be None, { ', ' . join ( ModelSummary . MODES ) } , got { mode } \" ) else : model_summary = ModelSummary ( self , max_depth = max_depth ) log . info ( \" \\n \" + str ( model_summary )) return model_summary tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list inherited \u00b6 When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Parameters: Name Type Description Default batch Tensor Current batch required split_size int The size of the split required Returns: Type Description List of batch splits. Each split will be passed to meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits !!! note Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . Source code in zamba/models/slowfast_models.py def tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list : r \"\"\" When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start` if :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` > 0. Each returned batch split is passed separately to :meth:`training_step`. \"\"\" time_dims = [ len ( x [ 0 ]) for x in batch if isinstance ( x , ( torch . Tensor , collections . Sequence ))] assert len ( time_dims ) >= 1 , \"Unable to determine batch time dimension\" assert all ( x == time_dims [ 0 ] for x in time_dims ), \"Batch time dimension length is ambiguous\" splits = [] for t in range ( 0 , time_dims [ 0 ], split_size ): batch_split = [] for i , x in enumerate ( batch ): if isinstance ( x , torch . Tensor ): split_x = x [:, t : t + split_size ] elif isinstance ( x , collections . Sequence ): split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )): split_x [ batch_idx ] = x [ batch_idx ][ t : t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits teardown ( self , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Source code in zamba/models/slowfast_models.py def teardown ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Called at the end of fit (train + validate), validate, test, predict, or tune. Args: stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'`` \"\"\" test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a postive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] !!! note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. !!! note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in zamba/models/slowfast_models.py def test_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a postive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`setup` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note: If you don't need a test dataset and a :meth:`test_step`, you don't need to implement this method. Note: In the case where you return multiple test dataloaders, the :meth:`test_step` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" test_epoch_end ( self , outputs : List [ Dict [ str , numpy . ndarray ]]) inherited \u00b6 Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs List[Dict[str, numpy.ndarray]] List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader required Returns: Type Description None !!! note If you didn't define a :meth: test_step , this won't be called. Examples: With a single dataloader: .. code-block:: python def test_epoch_end(self, outputs): # do something with the outputs of all test batches all_test_preds = test_step_outputs.predictions some_result = calc_all_results(all_test_preds) self.log(some_result) With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each test step for that dataloader. .. code-block:: python def test_epoch_end(self, outputs): final_value = 0 for dataloader_outputs in outputs: for test_step_out in dataloader_outputs: # do something final_value += test_step_out self.log(\"final_metric\", final_value) Source code in zamba/models/slowfast_models.py def test_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"test\" ) test_step ( self , batch , batch_idx ) inherited \u00b6 Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch. required dataloader_idx int The index of the dataloader that produced this batch (only if multiple test dataloaders used). required Returns: Type Description Any of. Any object or value None - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... !!! note If you don't need to test you don't need to implement this method. !!! note When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. Source code in zamba/models/slowfast_models.py def test_step ( self , batch , batch_idx ): return self . validation_step ( batch , batch_idx ) test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in :meth: test_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/models/slowfast_models.py def test_step_end ( self , * args , ** kwargs ) -> Optional [ STEP_OUTPUT ]: \"\"\" Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`test_step` for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" to ( self , * args : Any , ** kwargs : Any ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. !!! note This method modifies the module in-place. Parameters: Name Type Description Default device the desired device of the parameters and buffers in this module required dtype the desired floating point type of the floating point parameters and buffers in this module required tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 Source code in zamba/models/slowfast_models.py def to ( self , * args : Any , ** kwargs : Any ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def __init__(self, weight: torch.Tensor): ... super().__init__() ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 \"\"\" # there is diff nb vars in PT 1.5 out = torch . _C . _nn . _parse_to ( * args , ** kwargs ) self . __update_properties ( device = out [ 0 ], dtype = out [ 1 ]) return super () . to ( * args , ** kwargs ) to_disk ( self , path : PathLike ) inherited \u00b6 Source code in zamba/models/slowfast_models.py def to_disk ( self , path : os . PathLike ): checkpoint = { \"state_dict\" : self . state_dict (), \"hyper_parameters\" : self . hparams , } torch . save ( checkpoint , path ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) inherited \u00b6 Saves the model in ONNX format. Parameters: Name Type Description Default file_path Union[str, pathlib.Path] The path of the file the onnx model should be saved to. required input_sample Optional[Any] An input for tracing. Default: None (Use self.example_input_array) None **kwargs Will be passed to torch.onnx.export function. {} Examples: >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) >>> with tempfile . NamedTemporaryFile ( suffix = '.onnx' , delete = False ) as tmpfile : ... model = SimpleModel () ... input_sample = torch . randn (( 1 , 64 )) ... model . to_onnx ( tmpfile . name , input_sample , export_params = True ) ... os . path . isfile ( tmpfile . name ) True Source code in zamba/models/slowfast_models.py @torch . no_grad () def to_onnx ( self , file_path : Union [ str , Path ], input_sample : Optional [ Any ] = None , ** kwargs ): \"\"\" Saves the model in ONNX format. Args: file_path: The path of the file the onnx model should be saved to. input_sample: An input for tracing. Default: None (Use self.example_input_array) **kwargs: Will be passed to torch.onnx.export function. Example: >>> class SimpleModel(LightningModule): ... def __init__(self): ... super().__init__() ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) >>> with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmpfile: ... model = SimpleModel() ... input_sample = torch.randn((1, 64)) ... model.to_onnx(tmpfile.name, input_sample, export_params=True) ... os.path.isfile(tmpfile.name) True \"\"\" mode = self . training if input_sample is None : if self . example_input_array is None : raise ValueError ( \"Could not export to ONNX since neither `input_sample` nor\" \" `model.example_input_array` attribute is set.\" ) input_sample = self . example_input_array input_sample = self . _apply_batch_transfer_handler ( input_sample ) if \"example_outputs\" not in kwargs : self . eval () if isinstance ( input_sample , Tuple ): kwargs [ \"example_outputs\" ] = self ( * input_sample ) else : kwargs [ \"example_outputs\" ] = self ( input_sample ) torch . onnx . export ( self , input_sample , file_path , ** kwargs ) self . train ( mode ) to_torchscript ( self , file_path : Union [ str , pathlib . Path ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] inherited \u00b6 By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Parameters: Name Type Description Default file_path Union[str, pathlib.Path] Path where to save the torchscript. Default: None (no file saved). None method Optional[str] Whether to use TorchScript's script or trace method. Default: 'script' 'script' example_inputs Optional[Any] An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) None **kwargs Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. {} !!! note - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Examples: >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) ... >>> model = SimpleModel () >>> torch . jit . save ( model . to_torchscript (), \"model.pt\" ) # doctest: +SKIP >>> os . path . isfile ( \"model.pt\" ) # doctest: +SKIP >>> torch . jit . save ( model . to_torchscript ( file_path = \"model_trace.pt\" , method = 'trace' , # doctest: +SKIP ... example_inputs = torch . randn ( 1 , 64 ))) # doctest: +SKIP >>> os . path . isfile ( \"model_trace.pt\" ) # doctest: +SKIP True Returns: Type Description Union[torch._C.ScriptModule, Dict[str, torch._C.ScriptModule]] This LightningModule as a torchscript, regardless of whether file_path is defined or not. Source code in zamba/models/slowfast_models.py @torch . no_grad () def to_torchscript ( self , file_path : Optional [ Union [ str , Path ]] = None , method : Optional [ str ] = \"script\" , example_inputs : Optional [ Any ] = None , ** kwargs , ) -> Union [ ScriptModule , Dict [ str , ScriptModule ]]: \"\"\" By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing, please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr:`example_input_array`) **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or :func:`torch.jit.trace` function. Note: - Requires the implementation of the :meth:`~pytorch_lightning.core.lightning.LightningModule.forward` method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod:`torch.jit` documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def __init__(self): ... super().__init__() ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether `file_path` is defined or not. \"\"\" mode = self . training if method == \"script\" : torchscript_module = torch . jit . script ( self . eval (), ** kwargs ) elif method == \"trace\" : # if no example inputs are provided, try to see if model has example_input_array set if example_inputs is None : if self . example_input_array is None : raise ValueError ( \"Choosing method=`trace` requires either `example_inputs`\" \" or `model.example_input_array` to be defined.\" ) example_inputs = self . example_input_array # automatically send example inputs to the right device and use trace example_inputs = self . _apply_batch_transfer_handler ( example_inputs ) torchscript_module = torch . jit . trace ( func = self . eval (), example_inputs = example_inputs , ** kwargs ) else : raise ValueError ( f \"The 'method' parameter only supports 'script' or 'trace', but value given was: { method } \" ) self . train ( mode ) if file_path is not None : fs = get_filesystem ( file_path ) with fs . open ( file_path , \"wb\" ) as f : torch . jit . save ( torchscript_module , f ) return torchscript_module toggle_optimizer ( self , optimizer : Optimizer , optimizer_idx : int ) inherited \u00b6 Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Override for your own behavior. Parameters: Name Type Description Default optimizer Optimizer Current optimizer used in the training loop required optimizer_idx int Current optimizer idx in the training loop required !!! note Only called when using multiple optimizers Source code in zamba/models/slowfast_models.py def toggle_optimizer ( self , optimizer : Optimizer , optimizer_idx : int ): \"\"\" Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset. Override for your own behavior. Args: optimizer: Current optimizer used in the training loop optimizer_idx: Current optimizer idx in the training loop Note: Only called when using multiple optimizers \"\"\" # Iterate over all optimizer parameters to preserve their `requires_grad` information # in case these are pre-defined during `configure_optimizers` param_requires_grad_state = {} for opt in self . optimizers ( use_pl_optimizer = False ): for group in opt . param_groups : for param in group [ \"params\" ]: # If a param already appear in param_requires_grad_state, continue if param in param_requires_grad_state : continue param_requires_grad_state [ param ] = param . requires_grad param . requires_grad = False # Then iterate over the current optimizer's parameters and set its `requires_grad` # properties accordingly for group in optimizer . param_groups : for param in group [ \"params\" ]: param . requires_grad = param_requires_grad_state [ param ] self . _param_requires_grad_state = param_requires_grad_state train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] inherited \u00b6 Implement one or more PyTorch DataLoaders for training. Returns: Type Description A collection of class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in zamba/models/slowfast_models.py def train_dataloader ( self ) -> TRAIN_DATALOADERS : \"\"\" Implement one or more PyTorch DataLoaders for training. Return: A collection of :class:`torch.utils.data.DataLoader` specifying training samples. In the case of multiple dataloaders, please see this :ref:`page <multiple-training-dataloaders>`. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`setup` - :meth:`train_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} \"\"\" rank_zero_warn ( \"`train_dataloader` must be implemented to be used with the Lightning Trainer\" ) training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]]) -> None inherited \u00b6 Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs List[Union[torch.Tensor, Dict[str, Any]]] List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. required Returns: Type Description None None !!! note If this method is not overridden, this won't be called. Example:: def training_epoch_end(self, training_step_outputs): # do something with all training_step outputs return result With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, training_step_outputs): for out in training_step_outputs: ... Source code in zamba/models/slowfast_models.py def training_epoch_end ( self , outputs : EPOCH_OUTPUT ) -> None : \"\"\" Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth:`training_step`. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth:`training_step`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If this method is not overridden, this won't be called. Example:: def training_epoch_end(self, training_step_outputs): # do something with all training_step outputs return result With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, training_step_outputs): for out in training_step_outputs: ... \"\"\" training_step ( self , batch , batch_idx ) inherited \u00b6 Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int Integer displaying index of this batch required optimizer_idx int When using multiple optimizers, this argument will also be present. required hiddens( class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. required Returns: Type Description Any of. - class: ~torch.Tensor - The loss tensor - dict - A dictionary. Can include any keys, but must include the key 'loss' - None - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU or TPU, or using DeepSpeed . In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step ... out, hiddens = self.lstm(data, hiddens) ... return {\"loss\": loss, \"hiddens\": hiddens} !!! note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. Source code in zamba/models/slowfast_models.py def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"train_loss\" , loss . detach ()) return loss training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in training_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} def training_step_end(self, training_step_outputs): gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/models/slowfast_models.py def training_step_end ( self , * args , ** kwargs ) -> STEP_OUTPUT : \"\"\" Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in `training_step` for each batch part. Return: Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} def training_step_end(self, training_step_outputs): gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" transfer_batch_to_device ( self , batch : Any , device : device , dataloader_idx : int ) -> Any inherited \u00b6 Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). !!! note This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be transferred to a new device. required device device The target device as defined in PyTorch. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) !!! else batch = super().transfer_batch_to_device(data, device) return batch See Also: - :meth: move_data_to_device - :meth: apply_to_collection Source code in zamba/models/slowfast_models.py def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any : \"\"\" Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: - :class:`torch.Tensor` or anything that implements `.to(...)` - :class:`list` - :class:`dict` - :class:`tuple` - :class:`torchtext.data.batch.Batch` For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`move_data_to_device` - :meth:`apply_to_collection` \"\"\" return move_data_to_device ( batch , device ) type ( self , dst_type : Union [ str , torch . dtype ]) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def type ( self , dst_type : Union [ str , torch . dtype ]) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\"\" self . __update_properties ( dtype = dst_type ) return super () . type ( dst_type = dst_type ) unfreeze ( self ) -> None inherited \u00b6 Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() Source code in zamba/models/slowfast_models.py def unfreeze ( self ) -> None : \"\"\" Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() \"\"\" for param in self . parameters (): param . requires_grad = True self . train () untoggle_optimizer ( self , optimizer_idx : int ) inherited \u00b6 Resets the state of required gradients that were toggled with :meth: toggle_optimizer . Override for your own behavior. Parameters: Name Type Description Default optimizer_idx int Current optimizer idx in the training loop required !!! note Only called when using multiple optimizers Source code in zamba/models/slowfast_models.py def untoggle_optimizer ( self , optimizer_idx : int ): \"\"\" Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`. Override for your own behavior. Args: optimizer_idx: Current optimizer idx in the training loop Note: Only called when using multiple optimizers \"\"\" for opt_idx , opt in enumerate ( self . optimizers ( use_pl_optimizer = False )): if optimizer_idx != opt_idx : for group in opt . param_groups : for param in group [ \"params\" ]: if param in self . _param_requires_grad_state : param . requires_grad = self . _param_requires_grad_state [ param ] # save memory self . _param_requires_grad_state = {} val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] !!! note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. !!! note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in zamba/models/slowfast_models.py def val_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a positive integer. It's recommended that all data downloads and preparation happen in :meth:`prepare_data`. - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note: If you don't need a validation dataset and a :meth:`validation_step`, you don't need to implement this method. Note: In the case where you return multiple validation dataloaders, the :meth:`validation_step` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" validation_epoch_end ( self , outputs : List [ Dict [ str , numpy . ndarray ]]) inherited \u00b6 Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Parameters: Name Type Description Default outputs List[dict] list of output dictionaries from each validation step containing y_pred and y_true. required Source code in zamba/models/slowfast_models.py def validation_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Args: outputs (List[dict]): list of output dictionaries from each validation step containing y_pred and y_true. \"\"\" y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"val\" ) validation_step ( self , batch , batch_idx ) inherited \u00b6 Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch required dataloader_idx int The index of the dataloader that produced this batch (only if multiple val dataloaders used) required Returns: Type Description Any object or value None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... !!! note If you don't need to validate you don't need to implement this method. !!! note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. Source code in zamba/models/slowfast_models.py def validation_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"val_loss\" , loss . detach ()) y_proba = torch . sigmoid ( y_hat . cpu ()) . numpy () return { \"y_true\" : y . cpu () . numpy () . astype ( int ), \"y_pred\" : y_proba . round () . astype ( int ), \"y_proba\" : y_proba , } validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in :meth: validation_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/models/slowfast_models.py def validation_step_end ( self , * args , ** kwargs ) -> Optional [ STEP_OUTPUT ]: \"\"\" Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`validation_step` for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" write_prediction ( self , name : str , value : Union [ torch . Tensor , List [ torch . Tensor ]], filename : str = 'predictions.pt' ) inherited \u00b6 Write predictions to disk using torch.save Example:: self.write_prediction('pred', torch.tensor(...), filename='my_predictions.pt') Parameters: Name Type Description Default name str a string indicating the name to save the predictions under required value Union[torch.Tensor, List[torch.Tensor]] the predictions, either a single :class: ~torch.Tensor or a list of them required filename str name of the file to save the predictions to 'predictions.pt' !!! note when running in distributed mode, calling write_prediction will create a file for each device with respective names: filename_rank_0.pt , filename_rank_1.pt , ... .. deprecated::v1.3 Will be removed in v1.5.0. Source code in zamba/models/slowfast_models.py def write_prediction ( self , name : str , value : Union [ torch . Tensor , List [ torch . Tensor ]], filename : str = \"predictions.pt\" ): \"\"\" Write predictions to disk using ``torch.save`` Example:: self.write_prediction('pred', torch.tensor(...), filename='my_predictions.pt') Args: name: a string indicating the name to save the predictions under value: the predictions, either a single :class:`~torch.Tensor` or a list of them filename: name of the file to save the predictions to Note: when running in distributed mode, calling ``write_prediction`` will create a file for each device with respective names: ``filename_rank_0.pt``, ``filename_rank_1.pt``, ... .. deprecated::v1.3 Will be removed in v1.5.0. \"\"\" rank_zero_deprecation ( \"LightningModule method `write_prediction` was deprecated in v1.3 and will be removed in v1.5.\" ) self . trainer . _evaluation_loop . predictions . _add_prediction ( name , value , filename ) write_prediction_dict ( self , predictions_dict : Dict [ str , Any ], filename : str = 'predictions.pt' ) inherited \u00b6 Write a dictonary of predictions to disk at once using torch.save Example:: pred_dict = {'pred1': torch.tensor(...), 'pred2': torch.tensor(...)} self.write_prediction_dict(pred_dict) Parameters: Name Type Description Default predictions_dict Dict[str, Any] dict containing predictions, where each prediction should either be single :class: ~torch.Tensor or a list of them required !!! note when running in distributed mode, calling write_prediction_dict will create a file for each device with respective names: filename_rank_0.pt , filename_rank_1.pt , ... .. deprecated::v1.3 Will be removed in v1.5.0. Source code in zamba/models/slowfast_models.py def write_prediction_dict ( self , predictions_dict : Dict [ str , Any ], filename : str = \"predictions.pt\" ): \"\"\" Write a dictonary of predictions to disk at once using ``torch.save`` Example:: pred_dict = {'pred1': torch.tensor(...), 'pred2': torch.tensor(...)} self.write_prediction_dict(pred_dict) Args: predictions_dict: dict containing predictions, where each prediction should either be single :class:`~torch.Tensor` or a list of them Note: when running in distributed mode, calling ``write_prediction_dict`` will create a file for each device with respective names: ``filename_rank_0.pt``, ``filename_rank_1.pt``, ... .. deprecated::v1.3 Will be removed in v1.5.0. \"\"\" rank_zero_deprecation ( \"LightningModule method `write_prediction_dict` was deprecated in v1.3 and will be removed in v1.5.\" ) for k , v in predictions_dict . items (): self . write_prediction ( k , v , filename ) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/models/slowfast_models.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/models/slowfast_models.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ ()","title":"zamba.models.slowfast_models"},{"location":"api-reference/models-slowfast_models/#zambamodelsslowfast_models","text":"","title":"zamba.models.slowfast_models"},{"location":"api-reference/models-slowfast_models/#zamba.models.slowfast_models-classes","text":"","title":"Classes"},{"location":"api-reference/models-utils/","text":"zamba.models.utils \u00b6 S3_BUCKET \u00b6 Classes \u00b6 RegionEnum ( str , Enum ) \u00b6 An enumeration. asia \u00b6 eu \u00b6 us \u00b6 download_weights ( filename : str , destination_dir : Union [ os . PathLike , str ], weight_region : RegionEnum = < RegionEnum . us : 'us' > ) -> Path \u00b6 Source code in zamba/models/utils.py def download_weights ( filename : str , destination_dir : Union [ os . PathLike , str ], weight_region : RegionEnum = RegionEnum ( \"us\" ), ) -> Path : # get s3 bucket based on region if weight_region != \"us\" : region_bucket = f \" { S3_BUCKET } - { weight_region } \" else : region_bucket = S3_BUCKET s3p = S3Path ( f \" { region_bucket } /zamba_official_models/ { filename } \" , client = S3Client ( local_cache_dir = destination_dir , no_sign_request = True ), ) return s3p . fspath get_model_checkpoint_filename ( model_name ) \u00b6 Source code in zamba/models/utils.py def get_model_checkpoint_filename ( model_name ): if isinstance ( model_name , Enum ): model_name = model_name . value config_file = MODELS_DIRECTORY / model_name / \"config.yaml\" with config_file . open () as f : config_dict = yaml . safe_load ( f ) return Path ( config_dict [ \"public_checkpoint\" ])","title":"zamba.models.utils"},{"location":"api-reference/models-utils/#zambamodelsutils","text":"","title":"zamba.models.utils"},{"location":"api-reference/models-utils/#zamba.models.utils-classes","text":"","title":"Classes"},{"location":"api-reference/models-yolox_models/","text":"zamba.models.yolox_models \u00b6 Modules \u00b6 yolox_base \u00b6 Classes \u00b6 YoloXBase ( Exp ) \u00b6 Modified from https://github.com/Megvii-BaseDetection/YOLOX/blob/main/yolox/exp/yolox_base.py __init__ ( self ) inherited special \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def __init__ ( self ): super () . __init__ () # ---------------- model config ---------------- # self . num_classes = 80 self . depth = 1.00 self . width = 1.00 # ---------------- dataloader config ---------------- # # set worker to 4 for shorter dataloader init time self . data_num_workers = 4 self . input_size = ( 640 , 640 ) # (height, width) # Actual multiscale ranges: [640-5*32, 640+5*32]. # To disable multiscale training, set the # self.multiscale_range to 0. self . multiscale_range = 5 # You can uncomment this line to specify a multiscale range # self.random_size = (14, 26) self . data_dir = None self . train_ann = \"instances_train2017.json\" self . val_ann = \"instances_val2017.json\" # --------------- transform config ----------------- # self . mosaic_prob = 1.0 self . mixup_prob = 1.0 self . hsv_prob = 1.0 self . flip_prob = 0.5 self . degrees = 10.0 self . translate = 0.1 self . mosaic_scale = ( 0.1 , 2 ) self . mixup_scale = ( 0.5 , 1.5 ) self . shear = 2.0 self . perspective = 0.0 self . enable_mixup = True # -------------- training config --------------------- # self . warmup_epochs = 5 self . max_epoch = 300 self . warmup_lr = 0 self . basic_lr_per_img = 0.01 / 64.0 self . scheduler = \"yoloxwarmcos\" self . no_aug_epochs = 15 self . min_lr_ratio = 0.05 self . ema = True self . weight_decay = 5e-4 self . momentum = 0.9 self . print_interval = 10 self . eval_interval = 10 self . exp_name = os . path . split ( os . path . realpath ( __file__ ))[ 1 ] . split ( \".\" )[ 0 ] # ----------------- testing config ------------------ # self . test_size = ( 640 , 640 ) self . test_conf = 0.01 self . nmsthre = 0.65 eval ( self , model , evaluator , is_distributed , half = False ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def eval ( self , model , evaluator , is_distributed , half = False ): return evaluator . evaluate ( model , is_distributed , half ) get_data_loader ( self , batch_size , is_distributed , no_aug = False , cache_img = False ) \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def get_data_loader ( self , batch_size , is_distributed , no_aug = False , cache_img = False ): from yolox.data import ( COCODataset , TrainTransform , YoloBatchSampler , DataLoader , InfiniteSampler , MosaicDetection , worker_init_reset_seed , ) from yolox.utils import ( wait_for_the_master , get_local_rank , ) local_rank = get_local_rank () with wait_for_the_master ( local_rank ): dataset = COCODataset ( data_dir = self . data_dir , name = \"data\" , json_file = self . train_ann , img_size = self . input_size , preproc = TrainTransform ( max_labels = 50 ), cache = cache_img , ) dataset = MosaicDetection ( dataset , mosaic = not no_aug , img_size = self . input_size , preproc = TrainTransform ( max_labels = 120 ), degrees = self . degrees , translate = self . translate , mosaic_scale = self . mosaic_scale , mixup_scale = self . mixup_scale , shear = self . shear , perspective = self . perspective , enable_mixup = self . enable_mixup , mosaic_prob = self . mosaic_prob , mixup_prob = self . mixup_prob , ) self . dataset = dataset if is_distributed : batch_size = batch_size // dist . get_world_size () sampler = InfiniteSampler ( len ( self . dataset ), seed = self . seed if self . seed else 0 ) batch_sampler = YoloBatchSampler ( sampler = sampler , batch_size = batch_size , drop_last = False , mosaic = not no_aug , ) dataloader_kwargs = { \"num_workers\" : self . data_num_workers , \"pin_memory\" : True } dataloader_kwargs [ \"batch_sampler\" ] = batch_sampler # Make sure each process has different random seed, especially for 'fork' method. # Check https://github.com/pytorch/pytorch/issues/63311 for more details. dataloader_kwargs [ \"worker_init_fn\" ] = worker_init_reset_seed train_loader = DataLoader ( self . dataset , ** dataloader_kwargs ) return train_loader get_eval_loader ( self , batch_size , is_distributed , testdev = False , legacy = False ) \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def get_eval_loader ( self , batch_size , is_distributed , testdev = False , legacy = False ): from yolox.data import COCODataset , ValTransform valdataset = COCODataset ( data_dir = self . data_dir , name = \"data\" , json_file = self . val_ann , img_size = self . test_size , preproc = ValTransform ( legacy = legacy ), ) if is_distributed : batch_size = batch_size // dist . get_world_size () sampler = torch . utils . data . distributed . DistributedSampler ( valdataset , shuffle = False ) else : sampler = torch . utils . data . SequentialSampler ( valdataset ) dataloader_kwargs = { \"num_workers\" : self . data_num_workers , \"pin_memory\" : True , \"sampler\" : sampler , } dataloader_kwargs [ \"batch_size\" ] = batch_size val_loader = torch . utils . data . DataLoader ( valdataset , ** dataloader_kwargs ) return val_loader get_evaluator ( self , batch_size , is_distributed , testdev = False , legacy = False ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def get_evaluator ( self , batch_size , is_distributed , testdev = False , legacy = False ): from yolox.evaluators import COCOEvaluator val_loader = self . get_eval_loader ( batch_size , is_distributed , testdev , legacy ) evaluator = COCOEvaluator ( dataloader = val_loader , img_size = self . test_size , confthre = self . test_conf , nmsthre = self . nmsthre , num_classes = self . num_classes , testdev = testdev , ) return evaluator get_lr_scheduler ( self , lr , iters_per_epoch ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def get_lr_scheduler ( self , lr , iters_per_epoch ): from yolox.utils import LRScheduler scheduler = LRScheduler ( self . scheduler , lr , iters_per_epoch , self . max_epoch , warmup_epochs = self . warmup_epochs , warmup_lr_start = self . warmup_lr , no_aug_epochs = self . no_aug_epochs , min_lr_ratio = self . min_lr_ratio , ) return scheduler get_model ( self ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def get_model ( self ): from yolox.models import YOLOX , YOLOPAFPN , YOLOXHead def init_yolo ( M ): for m in M . modules (): if isinstance ( m , nn . BatchNorm2d ): m . eps = 1e-3 m . momentum = 0.03 if getattr ( self , \"model\" , None ) is None : in_channels = [ 256 , 512 , 1024 ] backbone = YOLOPAFPN ( self . depth , self . width , in_channels = in_channels ) head = YOLOXHead ( self . num_classes , self . width , in_channels = in_channels ) self . model = YOLOX ( backbone , head ) self . model . apply ( init_yolo ) self . model . head . initialize_biases ( 1e-2 ) return self . model get_optimizer ( self , batch_size ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def get_optimizer ( self , batch_size ): if \"optimizer\" not in self . __dict__ : if self . warmup_epochs > 0 : lr = self . warmup_lr else : lr = self . basic_lr_per_img * batch_size pg0 , pg1 , pg2 = [], [], [] # optimizer parameter groups for k , v in self . model . named_modules (): if hasattr ( v , \"bias\" ) and isinstance ( v . bias , nn . Parameter ): pg2 . append ( v . bias ) # biases if isinstance ( v , nn . BatchNorm2d ) or \"bn\" in k : pg0 . append ( v . weight ) # no decay elif hasattr ( v , \"weight\" ) and isinstance ( v . weight , nn . Parameter ): pg1 . append ( v . weight ) # apply decay optimizer = torch . optim . SGD ( pg0 , lr = lr , momentum = self . momentum , nesterov = True ) optimizer . add_param_group ( { \"params\" : pg1 , \"weight_decay\" : self . weight_decay } ) # add pg1 with weight_decay optimizer . add_param_group ({ \"params\" : pg2 }) self . optimizer = optimizer return self . optimizer merge ( self , cfg_list ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def merge ( self , cfg_list ): assert len ( cfg_list ) % 2 == 0 for k , v in zip ( cfg_list [ 0 :: 2 ], cfg_list [ 1 :: 2 ]): # only update value with same key if hasattr ( self , k ): src_value = getattr ( self , k ) src_type = type ( src_value ) if src_value is not None and src_type != type ( v ): try : v = src_type ( v ) except Exception : v = ast . literal_eval ( v ) setattr ( self , k , v ) preprocess ( self , inputs , targets , tsize ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def preprocess ( self , inputs , targets , tsize ): scale_y = tsize [ 0 ] / self . input_size [ 0 ] scale_x = tsize [ 1 ] / self . input_size [ 1 ] if scale_x != 1 or scale_y != 1 : inputs = nn . functional . interpolate ( inputs , size = tsize , mode = \"bilinear\" , align_corners = False ) targets [ ... , 1 :: 2 ] = targets [ ... , 1 :: 2 ] * scale_x targets [ ... , 2 :: 2 ] = targets [ ... , 2 :: 2 ] * scale_y return inputs , targets random_resize ( self , data_loader , epoch , rank , is_distributed ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_base.py def random_resize ( self , data_loader , epoch , rank , is_distributed ): tensor = torch . LongTensor ( 2 ) . cuda () if rank == 0 : size_factor = self . input_size [ 1 ] * 1.0 / self . input_size [ 0 ] if not hasattr ( self , 'random_size' ): min_size = int ( self . input_size [ 0 ] / 32 ) - self . multiscale_range max_size = int ( self . input_size [ 0 ] / 32 ) + self . multiscale_range self . random_size = ( min_size , max_size ) size = random . randint ( * self . random_size ) size = ( int ( 32 * size ), 32 * int ( size * size_factor )) tensor [ 0 ] = size [ 0 ] tensor [ 1 ] = size [ 1 ] if is_distributed : dist . barrier () dist . broadcast ( tensor , 0 ) input_size = ( tensor [ 0 ] . item (), tensor [ 1 ] . item ()) return input_size yolox_nano \u00b6 Classes \u00b6 YoloXNano ( YoloXBase ) \u00b6 Copied from https://github.com/Megvii-BaseDetection/YOLOX/blob/main/exps/default/nano.py __init__ ( self , num_classes : int ) special \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def __init__ ( self , num_classes : int ): super () . __init__ () self . depth = 0.33 self . width = 0.25 self . input_size = ( 416 , 416 ) self . random_size = ( 10 , 20 ) self . mosaic_scale = ( 0.5 , 1.5 ) self . test_size = ( 416 , 416 ) self . mosaic_prob = 0.5 self . enable_mixup = False self . num_classes = num_classes self . exp_name = Path ( __file__ ) . stem eval ( self , model , evaluator , is_distributed , half = False ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def eval ( self , model , evaluator , is_distributed , half = False ): return evaluator . evaluate ( model , is_distributed , half ) get_data_loader ( self , batch_size , is_distributed , no_aug = False , cache_img = False ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def get_data_loader ( self , batch_size , is_distributed , no_aug = False , cache_img = False ): from yolox.data import ( COCODataset , TrainTransform , YoloBatchSampler , DataLoader , InfiniteSampler , MosaicDetection , worker_init_reset_seed , ) from yolox.utils import ( wait_for_the_master , get_local_rank , ) local_rank = get_local_rank () with wait_for_the_master ( local_rank ): dataset = COCODataset ( data_dir = self . data_dir , name = \"data\" , json_file = self . train_ann , img_size = self . input_size , preproc = TrainTransform ( max_labels = 50 ), cache = cache_img , ) dataset = MosaicDetection ( dataset , mosaic = not no_aug , img_size = self . input_size , preproc = TrainTransform ( max_labels = 120 ), degrees = self . degrees , translate = self . translate , mosaic_scale = self . mosaic_scale , mixup_scale = self . mixup_scale , shear = self . shear , perspective = self . perspective , enable_mixup = self . enable_mixup , mosaic_prob = self . mosaic_prob , mixup_prob = self . mixup_prob , ) self . dataset = dataset if is_distributed : batch_size = batch_size // dist . get_world_size () sampler = InfiniteSampler ( len ( self . dataset ), seed = self . seed if self . seed else 0 ) batch_sampler = YoloBatchSampler ( sampler = sampler , batch_size = batch_size , drop_last = False , mosaic = not no_aug , ) dataloader_kwargs = { \"num_workers\" : self . data_num_workers , \"pin_memory\" : True } dataloader_kwargs [ \"batch_sampler\" ] = batch_sampler # Make sure each process has different random seed, especially for 'fork' method. # Check https://github.com/pytorch/pytorch/issues/63311 for more details. dataloader_kwargs [ \"worker_init_fn\" ] = worker_init_reset_seed train_loader = DataLoader ( self . dataset , ** dataloader_kwargs ) return train_loader get_eval_loader ( self , batch_size , is_distributed , testdev = False , legacy = False ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def get_eval_loader ( self , batch_size , is_distributed , testdev = False , legacy = False ): from yolox.data import COCODataset , ValTransform valdataset = COCODataset ( data_dir = self . data_dir , name = \"data\" , json_file = self . val_ann , img_size = self . test_size , preproc = ValTransform ( legacy = legacy ), ) if is_distributed : batch_size = batch_size // dist . get_world_size () sampler = torch . utils . data . distributed . DistributedSampler ( valdataset , shuffle = False ) else : sampler = torch . utils . data . SequentialSampler ( valdataset ) dataloader_kwargs = { \"num_workers\" : self . data_num_workers , \"pin_memory\" : True , \"sampler\" : sampler , } dataloader_kwargs [ \"batch_size\" ] = batch_size val_loader = torch . utils . data . DataLoader ( valdataset , ** dataloader_kwargs ) return val_loader get_evaluator ( self , batch_size , is_distributed , testdev = False , legacy = False ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def get_evaluator ( self , batch_size , is_distributed , testdev = False , legacy = False ): from yolox.evaluators import COCOEvaluator val_loader = self . get_eval_loader ( batch_size , is_distributed , testdev , legacy ) evaluator = COCOEvaluator ( dataloader = val_loader , img_size = self . test_size , confthre = self . test_conf , nmsthre = self . nmsthre , num_classes = self . num_classes , testdev = testdev , ) return evaluator get_lr_scheduler ( self , lr , iters_per_epoch ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def get_lr_scheduler ( self , lr , iters_per_epoch ): from yolox.utils import LRScheduler scheduler = LRScheduler ( self . scheduler , lr , iters_per_epoch , self . max_epoch , warmup_epochs = self . warmup_epochs , warmup_lr_start = self . warmup_lr , no_aug_epochs = self . no_aug_epochs , min_lr_ratio = self . min_lr_ratio , ) return scheduler get_model ( self , sublinear = False ) \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def get_model ( self , sublinear = False ): def init_yolo ( M ): for m in M . modules (): if isinstance ( m , torch . nn . BatchNorm2d ): m . eps = 1e-3 m . momentum = 0.03 if \"model\" not in self . __dict__ : from yolox.models import YOLOX , YOLOPAFPN , YOLOXHead in_channels = [ 256 , 512 , 1024 ] # NANO model use depthwise = True, which is main difference. backbone = YOLOPAFPN ( self . depth , self . width , in_channels = in_channels , depthwise = True ) head = YOLOXHead ( self . num_classes , self . width , in_channels = in_channels , depthwise = True ) self . model = YOLOX ( backbone , head ) self . model . apply ( init_yolo ) self . model . head . initialize_biases ( 1e-2 ) return self . model get_optimizer ( self , batch_size ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def get_optimizer ( self , batch_size ): if \"optimizer\" not in self . __dict__ : if self . warmup_epochs > 0 : lr = self . warmup_lr else : lr = self . basic_lr_per_img * batch_size pg0 , pg1 , pg2 = [], [], [] # optimizer parameter groups for k , v in self . model . named_modules (): if hasattr ( v , \"bias\" ) and isinstance ( v . bias , nn . Parameter ): pg2 . append ( v . bias ) # biases if isinstance ( v , nn . BatchNorm2d ) or \"bn\" in k : pg0 . append ( v . weight ) # no decay elif hasattr ( v , \"weight\" ) and isinstance ( v . weight , nn . Parameter ): pg1 . append ( v . weight ) # apply decay optimizer = torch . optim . SGD ( pg0 , lr = lr , momentum = self . momentum , nesterov = True ) optimizer . add_param_group ( { \"params\" : pg1 , \"weight_decay\" : self . weight_decay } ) # add pg1 with weight_decay optimizer . add_param_group ({ \"params\" : pg2 }) self . optimizer = optimizer return self . optimizer merge ( self , cfg_list ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def merge ( self , cfg_list ): assert len ( cfg_list ) % 2 == 0 for k , v in zip ( cfg_list [ 0 :: 2 ], cfg_list [ 1 :: 2 ]): # only update value with same key if hasattr ( self , k ): src_value = getattr ( self , k ) src_type = type ( src_value ) if src_value is not None and src_type != type ( v ): try : v = src_type ( v ) except Exception : v = ast . literal_eval ( v ) setattr ( self , k , v ) preprocess ( self , inputs , targets , tsize ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def preprocess ( self , inputs , targets , tsize ): scale_y = tsize [ 0 ] / self . input_size [ 0 ] scale_x = tsize [ 1 ] / self . input_size [ 1 ] if scale_x != 1 or scale_y != 1 : inputs = nn . functional . interpolate ( inputs , size = tsize , mode = \"bilinear\" , align_corners = False ) targets [ ... , 1 :: 2 ] = targets [ ... , 1 :: 2 ] * scale_x targets [ ... , 2 :: 2 ] = targets [ ... , 2 :: 2 ] * scale_y return inputs , targets random_resize ( self , data_loader , epoch , rank , is_distributed ) inherited \u00b6 Source code in zamba/models/yolox_models/yolox_nano.py def random_resize ( self , data_loader , epoch , rank , is_distributed ): tensor = torch . LongTensor ( 2 ) . cuda () if rank == 0 : size_factor = self . input_size [ 1 ] * 1.0 / self . input_size [ 0 ] if not hasattr ( self , 'random_size' ): min_size = int ( self . input_size [ 0 ] / 32 ) - self . multiscale_range max_size = int ( self . input_size [ 0 ] / 32 ) + self . multiscale_range self . random_size = ( min_size , max_size ) size = random . randint ( * self . random_size ) size = ( int ( 32 * size ), 32 * int ( size * size_factor )) tensor [ 0 ] = size [ 0 ] tensor [ 1 ] = size [ 1 ] if is_distributed : dist . barrier () dist . broadcast ( tensor , 0 ) input_size = ( tensor [ 0 ] . item (), tensor [ 1 ] . item ()) return input_size","title":"zamba.models.yolox_models"},{"location":"api-reference/models-yolox_models/#zambamodelsyolox_models","text":"","title":"zamba.models.yolox_models"},{"location":"api-reference/models-yolox_models/#zamba.models.yolox_models-modules","text":"","title":"Modules"},{"location":"api-reference/pytorch-dataloaders/","text":"zamba.pytorch.dataloaders \u00b6 FfmpegZambaVideoDataset ( VisionDataset ) \u00b6 functions : Dict [ str , Callable ] inherited \u00b6 __init__ ( self , annotations : DataFrame , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ zamba . data . video . VideoLoaderConfig ] = None ) special \u00b6 Source code in zamba/pytorch/dataloaders.py def __init__ ( self , annotations : pd . DataFrame , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ VideoLoaderConfig ] = None , ): self . original_indices = annotations . index self . video_paths = annotations . index . tolist () self . species = [ s . split ( \"species_\" , 1 )[ 1 ] for s in annotations . columns ] self . targets = annotations self . transform = transform # get environment variable for cache if it exists if video_loader_config is None : video_loader_config = VideoLoaderConfig () self . video_loader_config = video_loader_config super () . __init__ ( root = None , transform = transform ) extra_repr ( self ) -> str inherited \u00b6 Source code in zamba/pytorch/dataloaders.py def extra_repr ( self ) -> str : return \"\" Functions \u00b6 get_datasets ( train_metadata : Optional [ pandas . core . frame . DataFrame ] = None , predict_metadata : Optional [ pandas . core . frame . DataFrame ] = None , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ zamba . data . video . VideoLoaderConfig ] = None ) -> Tuple [ Union [ FfmpegZambaVideoDataset , NoneType ], Union [ FfmpegZambaVideoDataset , NoneType ], Union [ FfmpegZambaVideoDataset , NoneType ], Union [ FfmpegZambaVideoDataset ]] \u00b6 Gets training and/or prediction datasets. Parameters: Name Type Description Default train_metadata pathlike Path to a CSV or DataFrame with columns: - filepath: path to a video, relative to video_dir - label:, label of the species that appears in the video - split (optional): If provided, \"train\", \"val\", or \"holdout\" indicating which dataset split the video will be included in. If not provided, and a \"site\" column exists, generate a site-specific split. Otherwise, generate a random split using split_proportions . - site (optional): If no \"split\" column, generate a site-specific split using the values in this column. None predict_metadata pathlike Path to a CSV or DataFrame with a \"filepath\" column. None Returns: Type Description Tuple[Union[FfmpegZambaVideoDataset, NoneType], Union[FfmpegZambaVideoDataset, NoneType], Union[FfmpegZambaVideoDataset, NoneType], Union[FfmpegZambaVideoDataset]] A tuple of (train_dataset, val_dataset, test_dataset, predict_dataset) where each dataset can be None if not specified. Source code in zamba/pytorch/dataloaders.py def get_datasets ( train_metadata : Optional [ pd . DataFrame ] = None , predict_metadata : Optional [ pd . DataFrame ] = None , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ VideoLoaderConfig ] = None , ) -> Tuple [ Optional [ \"FfmpegZambaVideoDataset\" ], Optional [ \"FfmpegZambaVideoDataset\" ], Optional [ \"FfmpegZambaVideoDataset\" ], Optional [ \"FfmpegZambaVideoDataset\" ], ]: \"\"\"Gets training and/or prediction datasets. Args: train_metadata (pathlike, optional): Path to a CSV or DataFrame with columns: - filepath: path to a video, relative to `video_dir` - label:, label of the species that appears in the video - split (optional): If provided, \"train\", \"val\", or \"holdout\" indicating which dataset split the video will be included in. If not provided, and a \"site\" column exists, generate a site-specific split. Otherwise, generate a random split using `split_proportions`. - site (optional): If no \"split\" column, generate a site-specific split using the values in this column. predict_metadata (pathlike, optional): Path to a CSV or DataFrame with a \"filepath\" column. transform (torchvision.transforms.transforms.Compose, optional) video_loader_config (VideoLoaderConfig, optional) Returns: A tuple of (train_dataset, val_dataset, test_dataset, predict_dataset) where each dataset can be None if not specified. \"\"\" if predict_metadata is not None : # enable filtering the same way on all datasets predict_metadata [ \"species_\" ] = 0 def subset_metadata_or_none ( metadata : Optional [ pd . DataFrame ] = None , subset : Optional [ str ] = None ) -> Optional [ pd . DataFrame ]: if metadata is None : return None else : metadata_subset = metadata . loc [ metadata . split == subset ] if subset else metadata if len ( metadata_subset ) > 0 : return FfmpegZambaVideoDataset ( annotations = metadata_subset . set_index ( \"filepath\" ) . filter ( regex = \"species\" ), transform = transform , video_loader_config = video_loader_config , ) else : return None train_dataset = subset_metadata_or_none ( train_metadata , \"train\" ) val_dataset = subset_metadata_or_none ( train_metadata , \"val\" ) test_dataset = subset_metadata_or_none ( train_metadata , \"holdout\" ) predict_dataset = subset_metadata_or_none ( predict_metadata ) return train_dataset , val_dataset , test_dataset , predict_dataset","title":"zamba.pytorch.dataloaders"},{"location":"api-reference/pytorch-dataloaders/#zambapytorchdataloaders","text":"","title":"zamba.pytorch.dataloaders"},{"location":"api-reference/pytorch-dataloaders/#zamba.pytorch.dataloaders-functions","text":"","title":"Functions"},{"location":"api-reference/pytorch-finetuning/","text":"zamba.pytorch.finetuning \u00b6 Classes \u00b6 BackboneFinetuning ( BackboneFinetuning ) \u00b6 Derived from PTL's built-in BackboneFinetuning , but during the backbone freeze phase, choose whether to freeze batch norm layers, even if train_bn is True (i.e., even if we train them during the backbone unfreeze phase). Finetune a backbone model based on a learning rate user-defined scheduling. When the backbone learning rate reaches the current model learning rate and should_align is set to True, it will align with it for the rest of the training. Parameters: Name Type Description Default unfreeze_backbone_at_epoch Epoch at which the backbone will be unfreezed. required lambda_func Scheduling function for increasing backbone learning rate. required backbone_initial_ratio_lr Used to scale down the backbone learning rate compared to rest of model required backbone_initial_lr Optional, Inital learning rate for the backbone. By default, we will use current_learning / backbone_initial_ratio_lr required should_align Wheter to align with current learning rate when backbone learning reaches it. required initial_denom_lr When unfreezing the backbone, the intial learning rate will current_learning_rate / initial_denom_lr. required train_bn Wheter to make Batch Normalization trainable. required verbose Display current learning rate for model and backbone required round Precision for displaying learning rate required Example:: >>> from pytorch_lightning import Trainer >>> from pytorch_lightning.callbacks import BackboneFinetuning >>> multiplicative = lambda epoch: 1.5 >>> backbone_finetuning = BackboneFinetuning(200, multiplicative) >>> trainer = Trainer(callbacks=[backbone_finetuning]) Methods \u00b6 __init__ ( self , * args , * , multiplier : Optional [ float ] = 1 , pre_train_bn : bool = False , ** kwargs ) special \u00b6 Source code in zamba/pytorch/finetuning.py def __init__ ( self , * args , multiplier : Optional [ float ] = 1 , pre_train_bn : bool = False , ** kwargs ): if multiplier is not None : kwargs [ \"lambda_func\" ] = multiplier_factory ( multiplier ) super () . __init__ ( * args , ** kwargs ) # choose whether to train batch norm layers prior to finetuning phase self . pre_train_bn = pre_train_bn filter_on_optimizer ( optimizer : Optimizer , params : Iterable ) -> List inherited \u00b6 This function is used to exclude any parameter which already exists in this optimizer Parameters: Name Type Description Default optimizer Optimizer Optimizer used for parameter exclusion required params Iterable Iterable of parameters used to check against the provided optimizer required Returns: Type Description List List of parameters not contained in this optimizer param groups Source code in zamba/pytorch/finetuning.py @staticmethod def filter_on_optimizer ( optimizer : Optimizer , params : Iterable ) -> List : \"\"\" This function is used to exclude any parameter which already exists in this optimizer Args: optimizer: Optimizer used for parameter exclusion params: Iterable of parameters used to check against the provided optimizer Returns: List of parameters not contained in this optimizer param groups \"\"\" out_params = [] removed_params = [] for param in params : if not any ( torch . equal ( p , param ) for group in optimizer . param_groups for p in group [ \"params\" ]): out_params . append ( param ) else : removed_params . append ( param ) if removed_params : rank_zero_warn ( \"The provided params to be freezed already exist within another group of this optimizer.\" \" Those parameters will be skipped. \\n \" \"HINT: Did you init your optimizer in `configure_optimizer` as such: \\n \" f \" { type ( optimizer ) } (filter(lambda p: p.requires_grad, self.parameters()), ...) \" , UserWarning , ) return out_params filter_params ( modules : Union [ torch . nn . modules . module . Module , Iterable [ Union [ torch . nn . modules . module . Module , Iterable ]]], train_bn : bool = True , requires_grad : bool = True ) -> Generator inherited \u00b6 Yields the requires_grad parameters of a given module or list of modules. Parameters: Name Type Description Default modules Union[torch.nn.modules.module.Module, Iterable[Union[torch.nn.modules.module.Module, Iterable]]] A given module or an iterable of modules required train_bn bool Whether to train BatchNorm module True requires_grad bool Whether to create a generator for trainable or non-trainable parameters. True Returns: Type Description Generator Generator Source code in zamba/pytorch/finetuning.py @staticmethod def filter_params ( modules : Union [ Module , Iterable [ Union [ Module , Iterable ]]], train_bn : bool = True , requires_grad : bool = True ) -> Generator : \"\"\"Yields the `requires_grad` parameters of a given module or list of modules. Args: modules: A given module or an iterable of modules train_bn: Whether to train BatchNorm module requires_grad: Whether to create a generator for trainable or non-trainable parameters. Returns: Generator \"\"\" modules = BaseFinetuning . flatten_modules ( modules ) for mod in modules : if isinstance ( mod , _BatchNorm ) and not train_bn : continue # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it for param in mod . parameters ( recurse = False ): if param . requires_grad == requires_grad : yield param finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int ) inherited \u00b6 Called when the epoch begins. Source code in zamba/pytorch/finetuning.py def finetune_function ( self , pl_module : \"pl.LightningModule\" , epoch : int , optimizer : Optimizer , opt_idx : int ): \"\"\"Called when the epoch begins.\"\"\" if epoch == self . unfreeze_backbone_at_epoch : current_lr = optimizer . param_groups [ 0 ][ \"lr\" ] initial_backbone_lr = ( self . backbone_initial_lr if self . backbone_initial_lr is not None else current_lr * self . backbone_initial_ratio_lr ) self . previous_backbone_lr = initial_backbone_lr self . unfreeze_and_add_param_group ( pl_module . backbone , optimizer , initial_backbone_lr , train_bn = self . train_bn , initial_denom_lr = self . initial_denom_lr , ) if self . verbose : log . info ( f \"Current lr: { round ( current_lr , self . round ) } , \" f \"Backbone lr: { round ( initial_backbone_lr , self . round ) } \" ) elif epoch > self . unfreeze_backbone_at_epoch : current_lr = optimizer . param_groups [ 0 ][ \"lr\" ] next_current_backbone_lr = self . lambda_func ( epoch + 1 ) * self . previous_backbone_lr next_current_backbone_lr = ( current_lr if ( self . should_align and next_current_backbone_lr > current_lr ) else next_current_backbone_lr ) optimizer . param_groups [ - 1 ][ \"lr\" ] = next_current_backbone_lr self . previous_backbone_lr = next_current_backbone_lr if self . verbose : log . info ( f \"Current lr: { round ( current_lr , self . round ) } , \" f \"Backbone lr: { round ( next_current_backbone_lr , self . round ) } \" ) flatten_modules ( modules : Union [ torch . nn . modules . module . Module , Iterable [ Union [ torch . nn . modules . module . Module , Iterable ]]]) -> List [ torch . nn . modules . module . Module ] inherited \u00b6 This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules with no children) and parent modules that have parameters directly themselves. Parameters: Name Type Description Default modules Union[torch.nn.modules.module.Module, Iterable[Union[torch.nn.modules.module.Module, Iterable]]] A given module or an iterable of modules required Returns: Type Description List[torch.nn.modules.module.Module] List of modules Source code in zamba/pytorch/finetuning.py @staticmethod def flatten_modules ( modules : Union [ Module , Iterable [ Union [ Module , Iterable ]]]) -> List [ Module ]: \"\"\" This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules with no children) and parent modules that have parameters directly themselves. Args: modules: A given module or an iterable of modules Returns: List of modules \"\"\" if isinstance ( modules , ModuleDict ): modules = modules . values () if isinstance ( modules , Iterable ): _modules = [] for m in modules : _modules . extend ( BaseFinetuning . flatten_modules ( m )) else : _modules = modules . modules () # Capture all leaf modules as well as parent modules that have parameters directly themsleves return [ m for m in _modules if not list ( m . children ()) or m . _parameters ] freeze ( modules : Union [ torch . nn . modules . module . Module , Iterable [ Union [ torch . nn . modules . module . Module , Iterable ]]], train_bn : bool = True ) -> None inherited \u00b6 Freezes the parameters of the provided modules Parameters: Name Type Description Default modules Union[torch.nn.modules.module.Module, Iterable[Union[torch.nn.modules.module.Module, Iterable]]] A given module or an iterable of modules required train_bn bool If True, leave the BatchNorm layers in training mode True Returns: Type Description None None Source code in zamba/pytorch/finetuning.py @staticmethod def freeze ( modules : Union [ Module , Iterable [ Union [ Module , Iterable ]]], train_bn : bool = True ) -> None : \"\"\" Freezes the parameters of the provided modules Args: modules: A given module or an iterable of modules train_bn: If True, leave the BatchNorm layers in training mode Returns: None \"\"\" modules = BaseFinetuning . flatten_modules ( modules ) for mod in modules : if isinstance ( mod , _BatchNorm ) and train_bn : BaseFinetuning . make_trainable ( mod ) else : # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it for param in mod . parameters ( recurse = False ): param . requires_grad = False freeze_before_training ( self , pl_module : pl . LightningModule ) \u00b6 Override to add your freeze logic Source code in zamba/pytorch/finetuning.py def freeze_before_training ( self , pl_module : \"pl.LightningModule\" ): self . freeze ( pl_module . backbone , train_bn = self . pre_train_bn ) make_trainable ( modules : Union [ torch . nn . modules . module . Module , Iterable [ Union [ torch . nn . modules . module . Module , Iterable ]]]) -> None inherited \u00b6 Unfreezes the parameters of the provided modules Parameters: Name Type Description Default modules Union[torch.nn.modules.module.Module, Iterable[Union[torch.nn.modules.module.Module, Iterable]]] A given module or an iterable of modules required Source code in zamba/pytorch/finetuning.py @staticmethod def make_trainable ( modules : Union [ Module , Iterable [ Union [ Module , Iterable ]]]) -> None : \"\"\" Unfreezes the parameters of the provided modules Args: modules: A given module or an iterable of modules \"\"\" modules = BaseFinetuning . flatten_modules ( modules ) for module in modules : # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it for param in module . parameters ( recurse = False ): param . requires_grad = True on_after_backward ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called after loss.backward() and before optimizers are stepped. Source code in zamba/pytorch/finetuning.py def on_after_backward ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\"\"\" pass on_batch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the training batch ends. Source code in zamba/pytorch/finetuning.py def on_batch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the training batch ends.\"\"\" pass on_batch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the training batch begins. Source code in zamba/pytorch/finetuning.py def on_batch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the training batch begins.\"\"\" pass on_before_accelerator_backend_setup ( self , trainer , pl_module ) inherited \u00b6 Called before accelerator is being setup Source code in zamba/pytorch/finetuning.py def on_before_accelerator_backend_setup ( self , trainer , pl_module ): self . freeze_before_training ( pl_module ) on_before_backward ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , loss : Tensor ) -> None inherited \u00b6 Called before loss.backward() . Source code in zamba/pytorch/finetuning.py def on_before_backward ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , loss : torch . Tensor ) -> None : \"\"\"Called before ``loss.backward()``.\"\"\" pass on_before_optimizer_step ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , optimizer : Optimizer , opt_idx : int ) -> None inherited \u00b6 Called before optimizer.step() . Source code in zamba/pytorch/finetuning.py def on_before_optimizer_step ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , optimizer : Optimizer , opt_idx : int ) -> None : \"\"\"Called before ``optimizer.step()``.\"\"\" pass on_before_zero_grad ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , optimizer : Optimizer ) -> None inherited \u00b6 Called after optimizer.step() and before optimizer.zero_grad() . Source code in zamba/pytorch/finetuning.py def on_before_zero_grad ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , optimizer : Optimizer ) -> None : \"\"\"Called after ``optimizer.step()`` and before ``optimizer.zero_grad()``.\"\"\" pass on_configure_sharded_model ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called before configure sharded model Source code in zamba/pytorch/finetuning.py def on_configure_sharded_model ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called before configure sharded model\"\"\" on_epoch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when either of train/val/test epoch ends. Source code in zamba/pytorch/finetuning.py def on_epoch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when either of train/val/test epoch ends.\"\"\" pass on_epoch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when either of train/val/test epoch begins. Source code in zamba/pytorch/finetuning.py def on_epoch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when either of train/val/test epoch begins.\"\"\" pass on_fit_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when fit ends Source code in zamba/pytorch/finetuning.py def on_fit_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when fit ends\"\"\" pass on_fit_start ( self , trainer , pl_module ) inherited \u00b6 Source code in zamba/pytorch/finetuning.py def on_fit_start ( self , trainer , pl_module ): \"\"\" Raises: MisconfigurationException: If LightningModule has no nn.Module `backbone` attribute. \"\"\" if hasattr ( pl_module , \"backbone\" ) and isinstance ( pl_module . backbone , Module ): return super () . on_fit_start ( trainer , pl_module ) raise MisconfigurationException ( \"The LightningModule should have a nn.Module `backbone` attribute\" ) on_init_end ( self , trainer : pl . Trainer ) -> None inherited \u00b6 Called when the trainer initialization ends, model has not yet been set. Source code in zamba/pytorch/finetuning.py def on_init_end ( self , trainer : \"pl.Trainer\" ) -> None : \"\"\"Called when the trainer initialization ends, model has not yet been set.\"\"\" pass on_init_start ( self , trainer : pl . Trainer ) -> None inherited \u00b6 Called when the trainer initialization begins, model has not yet been set. Source code in zamba/pytorch/finetuning.py def on_init_start ( self , trainer : \"pl.Trainer\" ) -> None : \"\"\"Called when the trainer initialization begins, model has not yet been set.\"\"\" pass on_keyboard_interrupt ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the training is interrupted by KeyboardInterrupt . Source code in zamba/pytorch/finetuning.py def on_keyboard_interrupt ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the training is interrupted by ``KeyboardInterrupt``.\"\"\" pass on_load_checkpoint ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , callback_state : Dict [ int , List [ Dict [ str , Any ]]]) -> None inherited \u00b6 Called when loading a model checkpoint, use to reload state. Parameters: Name Type Description Default trainer pl.Trainer the current :class: ~pytorch_lightning.trainer.Trainer instance. required pl_module pl.LightningModule the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. required callback_state Dict[int, List[Dict[str, Any]]] the callback state returned by on_save_checkpoint . required !!! note The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state . Source code in zamba/pytorch/finetuning.py def on_load_checkpoint ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , callback_state : Dict [ int , List [ Dict [ str , Any ]]] ) -> None : self . previous_backbone_lr = callback_state [ \"previous_backbone_lr\" ] super () . on_load_checkpoint ( trainer , pl_module , callback_state [ \"internal_optimizer_metadata\" ]) on_predict_batch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called when the predict batch ends. Source code in zamba/pytorch/finetuning.py def on_predict_batch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int , ) -> None : \"\"\"Called when the predict batch ends.\"\"\" pass on_predict_batch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called when the predict batch begins. Source code in zamba/pytorch/finetuning.py def on_predict_batch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\"Called when the predict batch begins.\"\"\" pass on_predict_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when predict ends. Source code in zamba/pytorch/finetuning.py def on_predict_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when predict ends.\"\"\" pass on_predict_epoch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , outputs : List [ Any ]) -> None inherited \u00b6 Called when the predict epoch ends. Source code in zamba/pytorch/finetuning.py def on_predict_epoch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , outputs : List [ Any ]) -> None : \"\"\"Called when the predict epoch ends.\"\"\" pass on_predict_epoch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the predict epoch begins. Source code in zamba/pytorch/finetuning.py def on_predict_epoch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the predict epoch begins.\"\"\" pass on_predict_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the predict begins. Source code in zamba/pytorch/finetuning.py def on_predict_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the predict begins.\"\"\" pass on_pretrain_routine_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the pretrain routine ends. Source code in zamba/pytorch/finetuning.py def on_pretrain_routine_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the pretrain routine ends.\"\"\" pass on_pretrain_routine_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the pretrain routine begins. Source code in zamba/pytorch/finetuning.py def on_pretrain_routine_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the pretrain routine begins.\"\"\" pass on_sanity_check_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the validation sanity check ends. Source code in zamba/pytorch/finetuning.py def on_sanity_check_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the validation sanity check ends.\"\"\" pass on_sanity_check_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the validation sanity check starts. Source code in zamba/pytorch/finetuning.py def on_sanity_check_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the validation sanity check starts.\"\"\" pass on_save_checkpoint ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , checkpoint : Dict [ str , Any ]) -> Dict [ int , Any ] inherited \u00b6 Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer pl.Trainer the current :class: ~pytorch_lightning.trainer.Trainer instance. required pl_module pl.LightningModule the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. required checkpoint Dict[str, Any] the checkpoint dictionary that will be saved. required Returns: Type Description Dict[int, Any] The callback state. Source code in zamba/pytorch/finetuning.py def on_save_checkpoint ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , checkpoint : Dict [ str , Any ] ) -> Dict [ int , Any ]: return { \"internal_optimizer_metadata\" : self . _internal_optimizer_metadata , \"previous_backbone_lr\" : self . previous_backbone_lr , } on_test_batch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called when the test batch ends. Source code in zamba/pytorch/finetuning.py def on_test_batch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , outputs : Optional [ STEP_OUTPUT ], batch : Any , batch_idx : int , dataloader_idx : int , ) -> None : \"\"\"Called when the test batch ends.\"\"\" pass on_test_batch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called when the test batch begins. Source code in zamba/pytorch/finetuning.py def on_test_batch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\"Called when the test batch begins.\"\"\" pass on_test_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the test ends. Source code in zamba/pytorch/finetuning.py def on_test_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the test ends.\"\"\" pass on_test_epoch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the test epoch ends. Source code in zamba/pytorch/finetuning.py def on_test_epoch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the test epoch ends.\"\"\" pass on_test_epoch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the test epoch begins. Source code in zamba/pytorch/finetuning.py def on_test_epoch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the test epoch begins.\"\"\" pass on_test_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the test begins. Source code in zamba/pytorch/finetuning.py def on_test_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the test begins.\"\"\" pass on_train_batch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called when the train batch ends. Source code in zamba/pytorch/finetuning.py def on_train_batch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , outputs : STEP_OUTPUT , batch : Any , batch_idx : int , dataloader_idx : int , ) -> None : \"\"\"Called when the train batch ends.\"\"\" pass on_train_batch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called when the train batch begins. Source code in zamba/pytorch/finetuning.py def on_train_batch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\"Called when the train batch begins.\"\"\" pass on_train_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the train ends. Source code in zamba/pytorch/finetuning.py def on_train_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the train ends.\"\"\" pass on_train_epoch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , unused : Optional = None ) -> None inherited \u00b6 Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook. Source code in zamba/pytorch/finetuning.py def on_train_epoch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , unused : Optional = None ) -> None : \"\"\"Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: 1. Implement `training_epoch_end` in the `LightningModule` and access outputs via the module OR 2. Cache data across train batch hooks inside the callback implementation to post-process in this hook. \"\"\" pass on_train_epoch_start ( self , trainer , pl_module ) inherited \u00b6 Called when the epoch begins. Source code in zamba/pytorch/finetuning.py def on_train_epoch_start ( self , trainer , pl_module ): \"\"\"Called when the epoch begins.\"\"\" for opt_idx , optimizer in trainer . fit_loop . epoch_loop . batch_loop . get_active_optimizers (): num_param_groups = len ( optimizer . param_groups ) self . finetune_function ( pl_module , trainer . current_epoch , optimizer , opt_idx ) current_param_groups = optimizer . param_groups self . _store ( pl_module , opt_idx , num_param_groups , current_param_groups ) on_train_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the train begins. Source code in zamba/pytorch/finetuning.py def on_train_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the train begins.\"\"\" pass on_validation_batch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called when the validation batch ends. Source code in zamba/pytorch/finetuning.py def on_validation_batch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , outputs : Optional [ STEP_OUTPUT ], batch : Any , batch_idx : int , dataloader_idx : int , ) -> None : \"\"\"Called when the validation batch ends.\"\"\" pass on_validation_batch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called when the validation batch begins. Source code in zamba/pytorch/finetuning.py def on_validation_batch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\"Called when the validation batch begins.\"\"\" pass on_validation_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the validation loop ends. Source code in zamba/pytorch/finetuning.py def on_validation_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the validation loop ends.\"\"\" pass on_validation_epoch_end ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the val epoch ends. Source code in zamba/pytorch/finetuning.py def on_validation_epoch_end ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the val epoch ends.\"\"\" pass on_validation_epoch_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the val epoch begins. Source code in zamba/pytorch/finetuning.py def on_validation_epoch_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the val epoch begins.\"\"\" pass on_validation_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ) -> None inherited \u00b6 Called when the validation loop begins. Source code in zamba/pytorch/finetuning.py def on_validation_start ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" ) -> None : \"\"\"Called when the validation loop begins.\"\"\" pass setup ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called when fit, validate, test, predict, or tune begins Source code in zamba/pytorch/finetuning.py def setup ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , stage : Optional [ str ] = None ) -> None : \"\"\"Called when fit, validate, test, predict, or tune begins\"\"\" pass teardown ( self , trainer : pl . Trainer , pl_module : pl . LightningModule , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called when fit, validate, test, predict, or tune ends Source code in zamba/pytorch/finetuning.py def teardown ( self , trainer : \"pl.Trainer\" , pl_module : \"pl.LightningModule\" , stage : Optional [ str ] = None ) -> None : \"\"\"Called when fit, validate, test, predict, or tune ends\"\"\" pass unfreeze_and_add_param_group ( modules : Union [ torch . nn . modules . module . Module , Iterable [ Union [ torch . nn . modules . module . Module , Iterable ]]], optimizer : Optimizer , lr : Optional [ float ] = None , initial_denom_lr : float = 10.0 , train_bn : bool = True ) -> None inherited \u00b6 Unfreezes a module and adds its parameters to an optimizer. Parameters: Name Type Description Default modules Union[torch.nn.modules.module.Module, Iterable[Union[torch.nn.modules.module.Module, Iterable]]] A module or iterable of modules to unfreeze. Their parameters will be added to an optimizer as a new param group. required optimizer Optimizer The provided optimizer will receive new parameters and will add them to add_param_group required lr Optional[float] Learning rate for the new param group. None initial_denom_lr float If no lr is provided, the learning from the first param group will be used and divided by initial_denom_lr. 10.0 train_bn bool Whether to train the BatchNormalization layers. True Returns: Type Description None None Source code in zamba/pytorch/finetuning.py @staticmethod def unfreeze_and_add_param_group ( modules : Union [ Module , Iterable [ Union [ Module , Iterable ]]], optimizer : Optimizer , lr : Optional [ float ] = None , initial_denom_lr : float = 10.0 , train_bn : bool = True , ) -> None : \"\"\" Unfreezes a module and adds its parameters to an optimizer. Args: modules: A module or iterable of modules to unfreeze. Their parameters will be added to an optimizer as a new param group. optimizer: The provided optimizer will receive new parameters and will add them to `add_param_group` lr: Learning rate for the new param group. initial_denom_lr: If no lr is provided, the learning from the first param group will be used and divided by initial_denom_lr. train_bn: Whether to train the BatchNormalization layers. Returns: None \"\"\" BaseFinetuning . make_trainable ( modules ) params_lr = optimizer . param_groups [ 0 ][ \"lr\" ] if lr is None else float ( lr ) denom_lr = initial_denom_lr if lr is None else 1.0 params = BaseFinetuning . filter_params ( modules , train_bn = train_bn , requires_grad = True ) params = BaseFinetuning . filter_on_optimizer ( optimizer , params ) if params : optimizer . add_param_group ({ \"params\" : params , \"lr\" : params_lr / denom_lr }) Functions \u00b6 multiplier_factory ( rate : float ) \u00b6 Returns a function that returns a constant value for use in computing a constant learning rate multiplier. Parameters: Name Type Description Default rate float Constant multiplier. required Source code in zamba/pytorch/finetuning.py def multiplier_factory ( rate : float ): \"\"\"Returns a function that returns a constant value for use in computing a constant learning rate multiplier. Args: rate (float): Constant multiplier. \"\"\" def multiplier ( * args , ** kwargs ): return rate return multiplier","title":"zamba.pytorch.finetuning"},{"location":"api-reference/pytorch-finetuning/#zambapytorchfinetuning","text":"","title":"zamba.pytorch.finetuning"},{"location":"api-reference/pytorch-finetuning/#zamba.pytorch.finetuning-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch-finetuning/#zamba.pytorch.finetuning-functions","text":"","title":"Functions"},{"location":"api-reference/pytorch-layers/","text":"zamba.pytorch.layers \u00b6 Classes \u00b6 TimeDistributed ( Module ) \u00b6 Applies module over tdim identically for each step, use low_mem to compute one at a time. NOTE: vendored (with minor adaptations) from fastai: https://github.com/fastai/fastai/blob/4b0785254fdece1a44859956b6e54eedb167a97e/fastai/layers.py#L510-L544 Updates: - super. init () in init - assign attributes in init - inherit from torch.nn.Module rather than fastai.Module Attributes \u00b6 T_destination inherited \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. Methods \u00b6 __init__ ( self , module , low_mem = False , tdim = 1 ) special \u00b6 Source code in zamba/pytorch/layers.py def __init__ ( self , module , low_mem = False , tdim = 1 ): super () . __init__ () self . low_mem = low_mem self . tdim = tdim self . module = module add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch/layers.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch/layers.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/layers.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/layers.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch/layers.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu ( self : ~ T ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/layers.py def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/layers.py def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/layers.py def double ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch/layers.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch/layers.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/layers.py def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) format_output ( self , out , bs , seq_len ) \u00b6 unstack from batchsize outputs Source code in zamba/pytorch/layers.py def format_output ( self , out , bs , seq_len ): \"unstack from batchsize outputs\" if isinstance ( out , tuple ): return tuple ( out_i . view ( bs , seq_len , * out_i . shape [ 1 :]) for out_i in out ) return out . view ( bs , seq_len , * out . shape [ 1 :]) forward ( self , * tensors , ** kwargs ) \u00b6 input x with shape:(bs,seq_len,channels,width,height) Source code in zamba/pytorch/layers.py def forward ( self , * tensors , ** kwargs ): \"input x with shape:(bs,seq_len,channels,width,height)\" if self . low_mem or self . tdim != 1 : return self . low_mem_forward ( * tensors , ** kwargs ) else : # only support tdim=1 inp_shape = tensors [ 0 ] . shape bs , seq_len = inp_shape [ 0 ], inp_shape [ 1 ] out = self . module ( * [ x . view ( bs * seq_len , * x . shape [ 2 :]) for x in tensors ], ** kwargs ) return self . format_output ( out , bs , seq_len ) get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch/layers.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch/layers.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch/layers.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch/layers.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod half ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/layers.py def half ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch/layers.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) low_mem_forward ( self , * tensors , ** kwargs ) \u00b6 input x with shape:(bs,seq_len,channels,width,height) Source code in zamba/pytorch/layers.py def low_mem_forward ( self , * tensors , ** kwargs ): \"input x with shape:(bs,seq_len,channels,width,height)\" seq_len = tensors [ 0 ] . shape [ self . tdim ] args_split = [ torch . unbind ( x , dim = self . tdim ) for x in tensors ] out = [] for i in range ( seq_len ): out . append ( self . module ( * [ args [ i ] for args in args_split ]), ** kwargs ) if isinstance ( out [ 0 ], tuple ): return _stack_tups ( out , stack_dim = self . tdim ) return torch . stack ( out , dim = self . tdim ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch/layers.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch/layers.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch/layers.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch/layers.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch/layers.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/layers.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/layers.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch/layers.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/layers.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/layers.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/layers.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch/layers.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/layers.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch/layers.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch/layers.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch/layers.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to ( self , * args , ** kwargs ) inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Source code in zamba/pytorch/layers.py def to ( self , * args , ** kwargs ): r \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) \"\"\" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not ( dtype . is_floating_point or dtype . is_complex ): raise TypeError ( 'nn.Module.to only accepts floating point or complex ' 'dtypes, but got desired dtype= {} ' . format ( dtype )) if dtype . is_complex : warnings . warn ( \"Complex modules are a new feature under active development whose design may change, \" \"and some modules might not work as expected when using complex tensors as parameters or buffers. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"if a complex module does not work as expected.\" ) def convert ( t ): if convert_to_format is not None and t . dim () in ( 4 , 5 ): return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking ) return self . _apply ( convert ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch/layers.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/layers.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self type ( self : ~ T , dst_type : Union [ torch . dtype , str ]) -> ~ T inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch/layers.py def type ( self : T , dst_type : Union [ dtype , str ]) -> T : r \"\"\"Casts all parameters and buffers to :attr:`dst_type`. .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self \"\"\" return self . _apply ( lambda t : t . type ( dst_type )) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/layers.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch/layers.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ ()","title":"zamba.pytorch.layers"},{"location":"api-reference/pytorch-layers/#zambapytorchlayers","text":"","title":"zamba.pytorch.layers"},{"location":"api-reference/pytorch-layers/#zamba.pytorch.layers-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch-transforms/","text":"zamba.pytorch.transforms \u00b6 imagenet_normalization_values \u00b6 Classes \u00b6 ConvertTCHWtoCTHW ( Module ) \u00b6 Convert tensor from (T, C, H, W) to (C, T, H, W) Attributes \u00b6 T_destination inherited \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. Methods \u00b6 __init__ ( self ) -> None inherited special \u00b6 Source code in zamba/pytorch/transforms.py def __init__ ( self ) -> None : \"\"\" Initializes internal Module state, shared by both nn.Module and ScriptModule. \"\"\" torch . _C . _log_api_usage_once ( \"python.nn_module\" ) self . training = True self . _parameters : Dict [ str , Optional [ Parameter ]] = OrderedDict () self . _buffers : Dict [ str , Optional [ Tensor ]] = OrderedDict () self . _non_persistent_buffers_set : Set [ str ] = set () self . _backward_hooks : Dict [ int , Callable ] = OrderedDict () self . _is_full_backward_hook = None self . _forward_hooks : Dict [ int , Callable ] = OrderedDict () self . _forward_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _state_dict_hooks : Dict [ int , Callable ] = OrderedDict () self . _load_state_dict_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _modules : Dict [ str , Optional [ 'Module' ]] = OrderedDict () add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch/transforms.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch/transforms.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch/transforms.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu ( self : ~ T ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def double ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch/transforms.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward ( self , vid : Tensor ) -> Tensor \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in zamba/pytorch/transforms.py def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 1 , 0 , 2 , 3 ) get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch/transforms.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch/transforms.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch/transforms.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch/transforms.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod half ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def half ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch/transforms.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch/transforms.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch/transforms.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch/transforms.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch/transforms.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch/transforms.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch/transforms.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch/transforms.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch/transforms.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch/transforms.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch/transforms.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to ( self , * args , ** kwargs ) inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Source code in zamba/pytorch/transforms.py def to ( self , * args , ** kwargs ): r \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) \"\"\" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not ( dtype . is_floating_point or dtype . is_complex ): raise TypeError ( 'nn.Module.to only accepts floating point or complex ' 'dtypes, but got desired dtype= {} ' . format ( dtype )) if dtype . is_complex : warnings . warn ( \"Complex modules are a new feature under active development whose design may change, \" \"and some modules might not work as expected when using complex tensors as parameters or buffers. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"if a complex module does not work as expected.\" ) def convert ( t ): if convert_to_format is not None and t . dim () in ( 4 , 5 ): return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking ) return self . _apply ( convert ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self type ( self : ~ T , dst_type : Union [ torch . dtype , str ]) -> ~ T inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def type ( self : T , dst_type : Union [ dtype , str ]) -> T : r \"\"\"Casts all parameters and buffers to :attr:`dst_type`. .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self \"\"\" return self . _apply ( lambda t : t . type ( dst_type )) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch/transforms.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ () ConvertTHWCtoCTHW ( Module ) \u00b6 Convert tensor from (0:T, 1:H, 2:W, 3:C) to (3:C, 0:T, 1:H, 2:W) Attributes \u00b6 T_destination inherited \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. Methods \u00b6 __init__ ( self ) -> None inherited special \u00b6 Source code in zamba/pytorch/transforms.py def __init__ ( self ) -> None : \"\"\" Initializes internal Module state, shared by both nn.Module and ScriptModule. \"\"\" torch . _C . _log_api_usage_once ( \"python.nn_module\" ) self . training = True self . _parameters : Dict [ str , Optional [ Parameter ]] = OrderedDict () self . _buffers : Dict [ str , Optional [ Tensor ]] = OrderedDict () self . _non_persistent_buffers_set : Set [ str ] = set () self . _backward_hooks : Dict [ int , Callable ] = OrderedDict () self . _is_full_backward_hook = None self . _forward_hooks : Dict [ int , Callable ] = OrderedDict () self . _forward_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _state_dict_hooks : Dict [ int , Callable ] = OrderedDict () self . _load_state_dict_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _modules : Dict [ str , Optional [ 'Module' ]] = OrderedDict () add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch/transforms.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch/transforms.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch/transforms.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu ( self : ~ T ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def double ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch/transforms.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward ( self , vid : Tensor ) -> Tensor \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in zamba/pytorch/transforms.py def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 3 , 0 , 1 , 2 ) get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch/transforms.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch/transforms.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch/transforms.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch/transforms.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod half ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def half ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch/transforms.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch/transforms.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch/transforms.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch/transforms.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch/transforms.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch/transforms.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch/transforms.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch/transforms.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch/transforms.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch/transforms.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch/transforms.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to ( self , * args , ** kwargs ) inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Source code in zamba/pytorch/transforms.py def to ( self , * args , ** kwargs ): r \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) \"\"\" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not ( dtype . is_floating_point or dtype . is_complex ): raise TypeError ( 'nn.Module.to only accepts floating point or complex ' 'dtypes, but got desired dtype= {} ' . format ( dtype )) if dtype . is_complex : warnings . warn ( \"Complex modules are a new feature under active development whose design may change, \" \"and some modules might not work as expected when using complex tensors as parameters or buffers. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"if a complex module does not work as expected.\" ) def convert ( t ): if convert_to_format is not None and t . dim () in ( 4 , 5 ): return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking ) return self . _apply ( convert ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self type ( self : ~ T , dst_type : Union [ torch . dtype , str ]) -> ~ T inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def type ( self : T , dst_type : Union [ dtype , str ]) -> T : r \"\"\"Casts all parameters and buffers to :attr:`dst_type`. .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self \"\"\" return self . _apply ( lambda t : t . type ( dst_type )) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch/transforms.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ () ConvertTHWCtoTCHW ( Module ) \u00b6 Convert tensor from (T, H, W, C) to (T, C, H, W) Attributes \u00b6 T_destination inherited \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. Methods \u00b6 __init__ ( self ) -> None inherited special \u00b6 Source code in zamba/pytorch/transforms.py def __init__ ( self ) -> None : \"\"\" Initializes internal Module state, shared by both nn.Module and ScriptModule. \"\"\" torch . _C . _log_api_usage_once ( \"python.nn_module\" ) self . training = True self . _parameters : Dict [ str , Optional [ Parameter ]] = OrderedDict () self . _buffers : Dict [ str , Optional [ Tensor ]] = OrderedDict () self . _non_persistent_buffers_set : Set [ str ] = set () self . _backward_hooks : Dict [ int , Callable ] = OrderedDict () self . _is_full_backward_hook = None self . _forward_hooks : Dict [ int , Callable ] = OrderedDict () self . _forward_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _state_dict_hooks : Dict [ int , Callable ] = OrderedDict () self . _load_state_dict_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _modules : Dict [ str , Optional [ 'Module' ]] = OrderedDict () add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch/transforms.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch/transforms.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch/transforms.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu ( self : ~ T ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def double ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch/transforms.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward ( self , vid : Tensor ) -> Tensor \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in zamba/pytorch/transforms.py def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 0 , 3 , 1 , 2 ) get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch/transforms.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch/transforms.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch/transforms.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch/transforms.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod half ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def half ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch/transforms.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch/transforms.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch/transforms.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch/transforms.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch/transforms.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch/transforms.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch/transforms.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch/transforms.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch/transforms.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch/transforms.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch/transforms.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to ( self , * args , ** kwargs ) inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Source code in zamba/pytorch/transforms.py def to ( self , * args , ** kwargs ): r \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) \"\"\" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not ( dtype . is_floating_point or dtype . is_complex ): raise TypeError ( 'nn.Module.to only accepts floating point or complex ' 'dtypes, but got desired dtype= {} ' . format ( dtype )) if dtype . is_complex : warnings . warn ( \"Complex modules are a new feature under active development whose design may change, \" \"and some modules might not work as expected when using complex tensors as parameters or buffers. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"if a complex module does not work as expected.\" ) def convert ( t ): if convert_to_format is not None and t . dim () in ( 4 , 5 ): return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking ) return self . _apply ( convert ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self type ( self : ~ T , dst_type : Union [ torch . dtype , str ]) -> ~ T inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def type ( self : T , dst_type : Union [ dtype , str ]) -> T : r \"\"\"Casts all parameters and buffers to :attr:`dst_type`. .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self \"\"\" return self . _apply ( lambda t : t . type ( dst_type )) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch/transforms.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ () PackSlowFastPathways ( Module ) \u00b6 Creates the slow and fast pathway inputs for the slowfast model. Attributes \u00b6 T_destination inherited \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. Methods \u00b6 __init__ ( self , alpha : int = 4 ) special \u00b6 Source code in zamba/pytorch/transforms.py def __init__ ( self , alpha : int = 4 ): super () . __init__ () self . alpha = alpha add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch/transforms.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch/transforms.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch/transforms.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu ( self : ~ T ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def double ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch/transforms.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward ( self , frames : Tensor ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in zamba/pytorch/transforms.py def forward ( self , frames : torch . Tensor ): fast_pathway = frames # Perform temporal sampling from the fast pathway. slow_pathway = torch . index_select ( frames , 1 , torch . linspace ( 0 , frames . shape [ 1 ] - 1 , frames . shape [ 1 ] // self . alpha ) . long (), ) frame_list = [ slow_pathway , fast_pathway ] return frame_list get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch/transforms.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch/transforms.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch/transforms.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch/transforms.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod half ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def half ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch/transforms.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch/transforms.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch/transforms.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch/transforms.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch/transforms.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch/transforms.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch/transforms.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch/transforms.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch/transforms.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch/transforms.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch/transforms.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to ( self , * args , ** kwargs ) inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Source code in zamba/pytorch/transforms.py def to ( self , * args , ** kwargs ): r \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) \"\"\" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not ( dtype . is_floating_point or dtype . is_complex ): raise TypeError ( 'nn.Module.to only accepts floating point or complex ' 'dtypes, but got desired dtype= {} ' . format ( dtype )) if dtype . is_complex : warnings . warn ( \"Complex modules are a new feature under active development whose design may change, \" \"and some modules might not work as expected when using complex tensors as parameters or buffers. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"if a complex module does not work as expected.\" ) def convert ( t ): if convert_to_format is not None and t . dim () in ( 4 , 5 ): return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking ) return self . _apply ( convert ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self type ( self : ~ T , dst_type : Union [ torch . dtype , str ]) -> ~ T inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def type ( self : T , dst_type : Union [ dtype , str ]) -> T : r \"\"\"Casts all parameters and buffers to :attr:`dst_type`. .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self \"\"\" return self . _apply ( lambda t : t . type ( dst_type )) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch/transforms.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ () PadDimensions ( Module ) \u00b6 Pads a tensor to ensure a fixed output dimension for a give axis. Attributes: Name Type Description dimension_sizes A tuple of int or None the same length as the number of dimensions in the input tensor. If int, pad that dimension to at least that size. If None, do not pad. Attributes \u00b6 T_destination inherited \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. Methods \u00b6 __init__ ( self , dimension_sizes : Tuple [ Optional [ int ]]) special \u00b6 Source code in zamba/pytorch/transforms.py def __init__ ( self , dimension_sizes : Tuple [ Optional [ int ]]): super () . __init__ () self . dimension_sizes = dimension_sizes add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch/transforms.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch/transforms.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch/transforms.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module compute_left_and_right_pad ( original_size : int , padded_size : int ) -> Tuple [ int , int ] staticmethod \u00b6 Computes left and right pad size. Parameters: Name Type Description Default original_size list, int The original tensor size required padded_size list, int The desired tensor size required Returns: Type Description Tuple[int] Pad size for right and left. For odd padding size, the right = left + 1 Source code in zamba/pytorch/transforms.py @staticmethod def compute_left_and_right_pad ( original_size : int , padded_size : int ) -> Tuple [ int , int ]: \"\"\"Computes left and right pad size. Args: original_size (list, int): The original tensor size padded_size (list, int): The desired tensor size Returns: Tuple[int]: Pad size for right and left. For odd padding size, the right = left + 1 \"\"\" if original_size >= padded_size : return 0 , 0 pad = padded_size - original_size quotient , remainder = divmod ( pad , 2 ) return quotient , quotient + remainder cpu ( self : ~ T ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def double ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch/transforms.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward ( self , vid : Tensor ) -> Tensor \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in zamba/pytorch/transforms.py def forward ( self , vid : torch . Tensor ) -> torch . Tensor : padding = tuple ( itertools . chain . from_iterable ( ( 0 , 0 ) if padded_size is None else self . compute_left_and_right_pad ( original_size , padded_size ) for original_size , padded_size in zip ( vid . shape , self . dimension_sizes ) ) ) return torch . nn . functional . pad ( vid , padding [:: - 1 ]) get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch/transforms.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch/transforms.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch/transforms.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch/transforms.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod half ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def half ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch/transforms.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch/transforms.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch/transforms.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch/transforms.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch/transforms.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch/transforms.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch/transforms.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch/transforms.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch/transforms.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch/transforms.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch/transforms.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to ( self , * args , ** kwargs ) inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Source code in zamba/pytorch/transforms.py def to ( self , * args , ** kwargs ): r \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) \"\"\" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not ( dtype . is_floating_point or dtype . is_complex ): raise TypeError ( 'nn.Module.to only accepts floating point or complex ' 'dtypes, but got desired dtype= {} ' . format ( dtype )) if dtype . is_complex : warnings . warn ( \"Complex modules are a new feature under active development whose design may change, \" \"and some modules might not work as expected when using complex tensors as parameters or buffers. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"if a complex module does not work as expected.\" ) def convert ( t ): if convert_to_format is not None and t . dim () in ( 4 , 5 ): return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking ) return self . _apply ( convert ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self type ( self : ~ T , dst_type : Union [ torch . dtype , str ]) -> ~ T inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def type ( self : T , dst_type : Union [ dtype , str ]) -> T : r \"\"\"Casts all parameters and buffers to :attr:`dst_type`. .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self \"\"\" return self . _apply ( lambda t : t . type ( dst_type )) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch/transforms.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ () Uint8ToFloat ( Module ) \u00b6 Attributes \u00b6 T_destination inherited \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. Methods \u00b6 __init__ ( self ) -> None inherited special \u00b6 Source code in zamba/pytorch/transforms.py def __init__ ( self ) -> None : \"\"\" Initializes internal Module state, shared by both nn.Module and ScriptModule. \"\"\" torch . _C . _log_api_usage_once ( \"python.nn_module\" ) self . training = True self . _parameters : Dict [ str , Optional [ Parameter ]] = OrderedDict () self . _buffers : Dict [ str , Optional [ Tensor ]] = OrderedDict () self . _non_persistent_buffers_set : Set [ str ] = set () self . _backward_hooks : Dict [ int , Callable ] = OrderedDict () self . _is_full_backward_hook = None self . _forward_hooks : Dict [ int , Callable ] = OrderedDict () self . _forward_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _state_dict_hooks : Dict [ int , Callable ] = OrderedDict () self . _load_state_dict_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _modules : Dict [ str , Optional [ 'Module' ]] = OrderedDict () add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch/transforms.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch/transforms.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch/transforms.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu ( self : ~ T ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def double ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch/transforms.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward ( self , tensor : Tensor ) -> Tensor \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in zamba/pytorch/transforms.py def forward ( self , tensor : torch . Tensor ) -> torch . Tensor : return tensor / 255.0 get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch/transforms.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch/transforms.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch/transforms.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch/transforms.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod half ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def half ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch/transforms.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch/transforms.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch/transforms.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch/transforms.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch/transforms.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch/transforms.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch/transforms.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch/transforms.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch/transforms.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch/transforms.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch/transforms.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to ( self , * args , ** kwargs ) inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Source code in zamba/pytorch/transforms.py def to ( self , * args , ** kwargs ): r \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) \"\"\" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not ( dtype . is_floating_point or dtype . is_complex ): raise TypeError ( 'nn.Module.to only accepts floating point or complex ' 'dtypes, but got desired dtype= {} ' . format ( dtype )) if dtype . is_complex : warnings . warn ( \"Complex modules are a new feature under active development whose design may change, \" \"and some modules might not work as expected when using complex tensors as parameters or buffers. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"if a complex module does not work as expected.\" ) def convert ( t ): if convert_to_format is not None and t . dim () in ( 4 , 5 ): return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking ) return self . _apply ( convert ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self type ( self : ~ T , dst_type : Union [ torch . dtype , str ]) -> ~ T inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def type ( self : T , dst_type : Union [ dtype , str ]) -> T : r \"\"\"Casts all parameters and buffers to :attr:`dst_type`. .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self \"\"\" return self . _apply ( lambda t : t . type ( dst_type )) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch/transforms.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ () VideotoImg ( Module ) \u00b6 Attributes \u00b6 T_destination inherited \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. Methods \u00b6 __init__ ( self ) -> None inherited special \u00b6 Source code in zamba/pytorch/transforms.py def __init__ ( self ) -> None : \"\"\" Initializes internal Module state, shared by both nn.Module and ScriptModule. \"\"\" torch . _C . _log_api_usage_once ( \"python.nn_module\" ) self . training = True self . _parameters : Dict [ str , Optional [ Parameter ]] = OrderedDict () self . _buffers : Dict [ str , Optional [ Tensor ]] = OrderedDict () self . _non_persistent_buffers_set : Set [ str ] = set () self . _backward_hooks : Dict [ int , Callable ] = OrderedDict () self . _is_full_backward_hook = None self . _forward_hooks : Dict [ int , Callable ] = OrderedDict () self . _forward_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _state_dict_hooks : Dict [ int , Callable ] = OrderedDict () self . _load_state_dict_pre_hooks : Dict [ int , Callable ] = OrderedDict () self . _modules : Dict [ str , Optional [ 'Module' ]] = OrderedDict () add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch/transforms.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch/transforms.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch/transforms.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu ( self : ~ T ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def double ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch/transforms.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward ( self , vid : Tensor ) -> Tensor \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in zamba/pytorch/transforms.py def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . squeeze ( 0 ) get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch/transforms.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch/transforms.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch/transforms.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch/transforms.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod half ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def half ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch/transforms.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch/transforms.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch/transforms.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch/transforms.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch/transforms.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch/transforms.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch/transforms.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch/transforms.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch/transforms.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch/transforms.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch/transforms.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch/transforms.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch/transforms.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to ( self , * args , ** kwargs ) inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Source code in zamba/pytorch/transforms.py def to ( self , * args , ** kwargs ): r \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) \"\"\" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not ( dtype . is_floating_point or dtype . is_complex ): raise TypeError ( 'nn.Module.to only accepts floating point or complex ' 'dtypes, but got desired dtype= {} ' . format ( dtype )) if dtype . is_complex : warnings . warn ( \"Complex modules are a new feature under active development whose design may change, \" \"and some modules might not work as expected when using complex tensors as parameters or buffers. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"if a complex module does not work as expected.\" ) def convert ( t ): if convert_to_format is not None and t . dim () in ( 4 , 5 ): return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () or t . is_complex () else None , non_blocking ) return self . _apply ( convert ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self type ( self : ~ T , dst_type : Union [ torch . dtype , str ]) -> ~ T inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def type ( self : T , dst_type : Union [ dtype , str ]) -> T : r \"\"\"Casts all parameters and buffers to :attr:`dst_type`. .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self \"\"\" return self . _apply ( lambda t : t . type ( dst_type )) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch/transforms.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch/transforms.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ () slowfast_transforms () \u00b6 Source code in zamba/pytorch/transforms.py def slowfast_transforms (): return transforms . Compose ( [ ConvertTHWCtoTCHW (), Uint8ToFloat (), Normalize ( mean = [ 0.45 , 0.45 , 0.45 ], std = [ 0.225 , 0.225 , 0.225 ]), ConvertTCHWtoCTHW (), PadDimensions (( None , 32 , None , None )), PackSlowFastPathways (), ] ) zamba_image_model_transforms ( single_frame = False , normalization_values = { 'mean' : [ 0.485 , 0.456 , 0.406 ], 'std' : [ 0.229 , 0.224 , 0.225 ]}, channels_first = False ) \u00b6 Source code in zamba/pytorch/transforms.py def zamba_image_model_transforms ( single_frame = False , normalization_values = imagenet_normalization_values , channels_first = False ): img_transforms = [ ConvertTHWCtoTCHW (), Uint8ToFloat (), transforms . Normalize ( ** imagenet_normalization_values ), ] if single_frame : img_transforms += [ VideotoImg ()] # squeeze dim if channels_first : img_transforms += [ ConvertTCHWtoCTHW ()] return transforms . Compose ( img_transforms )","title":"zamba.pytorch.transforms"},{"location":"api-reference/pytorch-transforms/#zambapytorchtransforms","text":"","title":"zamba.pytorch.transforms"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch-utils/","text":"zamba.pytorch.utils \u00b6 Functions \u00b6 build_multilayer_perceptron ( input_size : int , hidden_layer_sizes : Optional [ Tuple [ int ]], output_size : int , activation : Optional [ torch . nn . modules . module . Module ] = < class ' torch . nn . modules . activation . ReLU '>, dropout: Optional[float] = None, output_dropout: Optional[float] = None, output_activation: Optional[torch.nn.modules.module.Module] = None) -> Sequential \u00b6 Builds a multilayer perceptron. Parameters: Name Type Description Default input_size int Size of first input layer. required hidden_layer_sizes tuple of int If provided, size of hidden layers. required output_size int Size of the last output layer. required activation torch.nn.Module Activation layer between each pair of layers. <class 'torch.nn.modules.activation.ReLU'> dropout float If provided, insert dropout layers with the following dropout rate in between each pair of layers. None output_dropout float If provided, insert a dropout layer with the following dropout rate before the output. None output_activation torch.nn.Module Activation layer after the final layer. None Returns: Type Description Sequential torch.nn.Sequential Source code in zamba/pytorch/utils.py def build_multilayer_perceptron ( input_size : int , hidden_layer_sizes : Optional [ Tuple [ int ]], output_size : int , activation : Optional [ torch . nn . Module ] = torch . nn . ReLU , dropout : Optional [ float ] = None , output_dropout : Optional [ float ] = None , output_activation : Optional [ torch . nn . Module ] = None , ) -> torch . nn . Sequential : \"\"\"Builds a multilayer perceptron. Args: input_size (int): Size of first input layer. hidden_layer_sizes (tuple of int, optional): If provided, size of hidden layers. output_size (int): Size of the last output layer. activation (torch.nn.Module, optional): Activation layer between each pair of layers. dropout (float, optional): If provided, insert dropout layers with the following dropout rate in between each pair of layers. output_dropout (float, optional): If provided, insert a dropout layer with the following dropout rate before the output. output_activation (torch.nn.Module, optional): Activation layer after the final layer. Returns: torch.nn.Sequential \"\"\" if ( hidden_layer_sizes is None ) or len ( hidden_layer_sizes ) == 0 : return torch . nn . Linear ( input_size , output_size ) layers = [ torch . nn . Linear ( input_size , hidden_layer_sizes [ 0 ])] if activation is not None : layers . append ( activation ()) if ( dropout is not None ) and ( dropout > 0 ): layers . append ( torch . nn . Dropout ( dropout )) for in_size , out_size in zip ( hidden_layer_sizes [: - 1 ], hidden_layer_sizes [ 1 :]): layers . append ( torch . nn . Linear ( in_size , out_size )) if activation is not None : layers . append ( activation ()) if ( dropout is not None ) and ( dropout > 0 ): layers . append ( torch . nn . Dropout ( dropout )) layers . append ( torch . nn . Linear ( hidden_layer_sizes [ - 1 ], output_size )) if ( output_dropout is not None ) and ( output_dropout > 0 ): layers . append ( torch . nn . Dropout ( dropout )) if output_activation is not None : layers . append ( output_activation ()) return torch . nn . Sequential ( * layers )","title":"zamba.pytorch.utils"},{"location":"api-reference/pytorch-utils/#zambapytorchutils","text":"","title":"zamba.pytorch.utils"},{"location":"api-reference/pytorch-utils/#zamba.pytorch.utils-functions","text":"","title":"Functions"},{"location":"api-reference/pytorch_lightning-utils/","text":"zamba.pytorch_lightning.utils \u00b6 DEFAULT_TOP_K \u00b6 available_models \u00b6 default_transform \u00b6 Classes \u00b6 ZambaDataModule ( LightningDataModule ) \u00b6 Attributes \u00b6 dims inherited property writable \u00b6 A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.prepare_data() has been called or not. Returns: Type Description bool True if datamodule.prepare_data() has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. has_setup_fit : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.setup(stage='fit') has been called or not. Returns: Type Description bool True if datamodule.setup(stage='fit') has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. has_setup_predict : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.setup(stage='predict') has been called or not. Returns: Type Description bool True if datamodule.setup(stage='predict') has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. has_setup_test : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.setup(stage='test') has been called or not. Returns: Type Description bool True if datamodule.setup(stage='test') has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. has_setup_validate : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.setup(stage='validate') has been called or not. Returns: Type Description bool True if datamodule.setup(stage='validate') has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. has_teardown_fit : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. Returns: Type Description bool True if datamodule.teardown(stage='fit') has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. has_teardown_predict : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. Returns: Type Description bool True if datamodule.teardown(stage='predict') has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. has_teardown_test : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.teardown(stage='test') has been called or not. Returns: Type Description bool True if datamodule.teardown(stage='test') has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. has_teardown_validate : bool inherited property readonly \u00b6 Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. Returns: Type Description bool True if datamodule.teardown(stage='validate') has been called. False by default. .. deprecated:: v1.4 Will be removed in v1.6.0. hparams : Union [ pytorch_lightning . utilities . parsing . AttributeDict , dict , argparse . Namespace ] inherited property readonly \u00b6 hparams_initial : AttributeDict inherited property readonly \u00b6 name : str inherited \u00b6 test_transforms inherited property writable \u00b6 Optional transforms (or collection of transforms) you can apply to test dataset train_transforms inherited property writable \u00b6 Optional transforms (or collection of transforms) you can apply to train dataset val_transforms inherited property writable \u00b6 Optional transforms (or collection of transforms) you can apply to validation dataset Methods \u00b6 __init__ ( self , batch_size : int = 1 , num_workers : int = 1 , transform : Compose = Compose ( ConvertTHWCtoCTHW () ConvertImageDtype () ), video_loader_config : Optional [ zamba . data . video . VideoLoaderConfig ] = None , prefetch_factor : int = 2 , train_metadata : Optional [ pandas . core . frame . DataFrame ] = None , predict_metadata : Optional [ pandas . core . frame . DataFrame ] = None , multiprocessing_context : Optional [ str ] = 'forkserver' , * args , ** kwargs ) special \u00b6 Source code in zamba/pytorch_lightning/utils.py def __init__ ( self , batch_size : int = 1 , num_workers : int = max ( cpu_count () - 1 , 1 ), transform : transforms . Compose = default_transform , video_loader_config : Optional [ VideoLoaderConfig ] = None , prefetch_factor : int = 2 , train_metadata : Optional [ pd . DataFrame ] = None , predict_metadata : Optional [ pd . DataFrame ] = None , multiprocessing_context : Optional [ str ] = \"forkserver\" , * args , ** kwargs , ): self . batch_size = batch_size self . num_workers = num_workers # Number of parallel processes fetching data self . prefetch_factor = prefetch_factor self . video_loader_config = ( None if video_loader_config is None else video_loader_config . dict () ) self . train_metadata = train_metadata self . predict_metadata = predict_metadata ( self . train_dataset , self . val_dataset , self . test_dataset , self . predict_dataset , ) = get_datasets ( train_metadata = train_metadata , predict_metadata = predict_metadata , transform = transform , video_loader_config = video_loader_config , ) self . multiprocessing_context : BaseContext = ( None if ( multiprocessing_context is None ) or ( num_workers == 0 ) else multiprocessing_context ) super () . __init__ ( * args , ** kwargs ) on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any inherited \u00b6 Override to alter or apply batch augmentations to your batch after it is transferred to the device. !!! note To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be altered or augmented. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch See Also: - :meth: on_before_batch_transfer - :meth: transfer_batch_to_device Source code in zamba/pytorch_lightning/utils.py def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any : \"\"\" Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be altered or augmented. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`on_before_batch_transfer` - :meth:`transfer_batch_to_device` \"\"\" return batch on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any inherited \u00b6 Override to alter or apply batch augmentations to your batch before it is transferred to the device. !!! note To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be altered or augmented. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch See Also: - :meth: on_after_batch_transfer - :meth: transfer_batch_to_device Source code in zamba/pytorch_lightning/utils.py def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any : \"\"\" Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be altered or augmented. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`on_after_batch_transfer` - :meth:`transfer_batch_to_device` \"\"\" return batch on_load_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Parameters: Name Type Description Default checkpoint Dict[str, Any] Loaded checkpoint required Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] !!! note Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. Source code in zamba/pytorch_lightning/utils.py def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : r \"\"\" Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. \"\"\" on_predict_dataloader ( self ) -> None inherited \u00b6 Called before requesting the predict dataloader. .. deprecated:: v1.5 :meth: on_predict_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: predict_dataloader() directly. Source code in zamba/pytorch_lightning/utils.py def on_predict_dataloader ( self ) -> None : \"\"\"Called before requesting the predict dataloader. .. deprecated:: v1.5 :meth:`on_predict_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`predict_dataloader()` directly. \"\"\" on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Parameters: Name Type Description Default checkpoint Dict[str, Any] The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. required Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object !!! note Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. Source code in zamba/pytorch_lightning/utils.py def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : r \"\"\" Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. \"\"\" on_test_dataloader ( self ) -> None inherited \u00b6 Called before requesting the test dataloader. .. deprecated:: v1.5 :meth: on_test_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: test_dataloader() directly. Source code in zamba/pytorch_lightning/utils.py def on_test_dataloader ( self ) -> None : \"\"\"Called before requesting the test dataloader. .. deprecated:: v1.5 :meth:`on_test_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`test_dataloader()` directly. \"\"\" on_train_dataloader ( self ) -> None inherited \u00b6 Called before requesting the train dataloader. .. deprecated:: v1.5 :meth: on_train_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: train_dataloader() directly. Source code in zamba/pytorch_lightning/utils.py def on_train_dataloader ( self ) -> None : \"\"\"Called before requesting the train dataloader. .. deprecated:: v1.5 :meth:`on_train_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`train_dataloader()` directly. \"\"\" on_val_dataloader ( self ) -> None inherited \u00b6 Called before requesting the val dataloader. .. deprecated:: v1.5 :meth: on_val_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: val_dataloader() directly. Source code in zamba/pytorch_lightning/utils.py def on_val_dataloader ( self ) -> None : \"\"\"Called before requesting the val dataloader. .. deprecated:: v1.5 :meth:`on_val_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`val_dataloader()` directly. \"\"\" predict_dataloader ( self ) -> Optional [ torch . utils . data . dataloader . DataLoader ] \u00b6 Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. !!! note In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. Source code in zamba/pytorch_lightning/utils.py def predict_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . predict_dataset : return torch . utils . data . DataLoader ( self . predict_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = True , ) prepare_data ( self ) -> None inherited \u00b6 Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() Source code in zamba/pytorch_lightning/utils.py def prepare_data ( self ) -> None : \"\"\" Use this to download and prepare data. .. warning:: DO NOT set state to the model (use `setup` instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): 1. Once per node. This is the default and is only called on LOCAL_RANK=0. 2. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() \"\"\" save_hyperparameters ( self , * args , * , ignore : Union [ Sequence [ str ], str ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None inherited \u00b6 Save arguments to hparams attribute. Parameters: Name Type Description Default args single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ () ignore Union[Sequence[str], str] an argument name or a list of argument names from class __init__ to be ignored None frame Optional[frame] a frame object. Default is None None logger bool Whether to send the hyperparameters to the logger. Default: True True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 Source code in zamba/pytorch_lightning/utils.py def save_hyperparameters ( self , * args , ignore : Optional [ Union [ Sequence [ str ], str ]] = None , frame : Optional [ types . FrameType ] = None , logger : bool = True , ) -> None : \"\"\"Save arguments to ``hparams`` attribute. Args: args: single object of `dict`, `NameSpace` or `OmegaConf` or string names or arguments from class ``__init__`` ignore: an argument name or a list of argument names from class ``__init__`` to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 \"\"\" self . _log_hyperparams = logger # the frame needs to be created in this file. if not frame : frame = inspect . currentframe () . f_back save_hyperparameters ( self , * args , ignore = ignore , frame = frame ) setup ( self , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) Source code in zamba/pytorch_lightning/utils.py def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Args: stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'`` Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) \"\"\" size ( self , dim = None ) -> Union [ Tuple , int ] inherited \u00b6 Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. Source code in zamba/pytorch_lightning/utils.py def size ( self , dim = None ) -> Union [ Tuple , int ]: \"\"\" Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. \"\"\" if dim is not None : return self . dims [ dim ] return self . dims teardown ( self , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Source code in zamba/pytorch_lightning/utils.py def teardown ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Called at the end of fit (train + validate), validate, test, predict, or tune. Args: stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'`` \"\"\" test_dataloader ( self ) -> Optional [ torch . utils . data . dataloader . DataLoader ] \u00b6 Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a postive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] !!! note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. !!! note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in zamba/pytorch_lightning/utils.py def test_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . test_dataset : return torch . utils . data . DataLoader ( self . test_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) train_dataloader ( self ) -> Optional [ torch . utils . data . dataloader . DataLoader ] \u00b6 Implement one or more PyTorch DataLoaders for training. Returns: Type Description A collection of class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in zamba/pytorch_lightning/utils.py def train_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . train_dataset : return torch . utils . data . DataLoader ( self . train_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = True , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) transfer_batch_to_device ( self , batch : Any , device : device , dataloader_idx : int ) -> Any inherited \u00b6 Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). !!! note This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be transferred to a new device. required device device The target device as defined in PyTorch. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) !!! else batch = super().transfer_batch_to_device(data, device) return batch See Also: - :meth: move_data_to_device - :meth: apply_to_collection Source code in zamba/pytorch_lightning/utils.py def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any : \"\"\" Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: - :class:`torch.Tensor` or anything that implements `.to(...)` - :class:`list` - :class:`dict` - :class:`tuple` - :class:`torchtext.data.batch.Batch` For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`move_data_to_device` - :meth:`apply_to_collection` \"\"\" return move_data_to_device ( batch , device ) val_dataloader ( self ) -> Optional [ torch . utils . data . dataloader . DataLoader ] \u00b6 Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] !!! note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. !!! note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in zamba/pytorch_lightning/utils.py def val_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . val_dataset : return torch . utils . data . DataLoader ( self . val_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) ZambaVideoClassificationLightningModule ( LightningModule ) \u00b6 Attributes \u00b6 CHECKPOINT_HYPER_PARAMS_KEY inherited \u00b6 CHECKPOINT_HYPER_PARAMS_NAME inherited \u00b6 CHECKPOINT_HYPER_PARAMS_TYPE inherited \u00b6 T_destination inherited \u00b6 automatic_optimization : bool inherited property writable \u00b6 If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch : int inherited property readonly \u00b6 The current epoch in the Trainer. If no Trainer is attached, this propery is 0. datamodule : Any inherited property writable \u00b6 device : Union [ str , torch . device ] inherited property readonly \u00b6 dtype : Union [ str , torch . dtype ] inherited property writable \u00b6 dump_patches : bool inherited \u00b6 This allows better BC support for :meth: load_state_dict . In :meth: state_dict , the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module's _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. example_input_array : Any inherited property writable \u00b6 The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank : int inherited property readonly \u00b6 The index of the current process across all nodes and devices. global_step : int inherited property readonly \u00b6 Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams : Union [ pytorch_lightning . utilities . parsing . AttributeDict , dict , argparse . Namespace ] inherited property readonly \u00b6 hparams_initial : AttributeDict inherited property readonly \u00b6 loaded_optimizer_states_dict : dict inherited property writable \u00b6 local_rank : int inherited property readonly \u00b6 The index of the current process within a single node. logger inherited property readonly \u00b6 Reference to the logger object in the Trainer. model_size : float inherited property readonly \u00b6 The model's size in megabytes. The computation includes everything in the :meth: ~torch.nn.Module.state_dict , i.e., by default the parameteters and buffers. on_gpu inherited property readonly \u00b6 Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps : int inherited property writable \u00b6 Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the :meth: training_step receives an additional argument hiddens and is expected to return a hidden state. Methods \u00b6 __init__ ( self , species : List [ str ], lr : float = 0.001 , scheduler : Optional [ str ] = None , scheduler_params : Optional [ dict ] = None , ** kwargs ) special \u00b6 Source code in zamba/pytorch_lightning/utils.py def __init__ ( self , species : List [ str ], lr : float = 1e-3 , scheduler : Optional [ str ] = None , scheduler_params : Optional [ dict ] = None , ** kwargs , ): super () . __init__ () if ( scheduler is None ) and ( scheduler_params is not None ): warnings . warn ( \"scheduler_params provided without scheduler. scheduler_params will have no effect.\" ) self . lr = lr self . species = species self . num_classes = len ( species ) if scheduler is not None : self . scheduler = torch . optim . lr_scheduler . __dict__ [ scheduler ] else : self . scheduler = scheduler self . scheduler_params = scheduler_params self . model_class = type ( self ) . __name__ self . save_hyperparameters ( \"lr\" , \"scheduler\" , \"scheduler_params\" , \"species\" ) self . hparams [ \"model_class\" ] = self . model_class add_module ( self , name : str , module : Optional [ Module ]) -> None inherited \u00b6 Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name required module Module child module to be added to the module. required Source code in zamba/pytorch_lightning/utils.py def add_module ( self , name : str , module : Optional [ 'Module' ]) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \" {} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"module name should be a string. Got {} \" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\\" , got: {} \" . format ( name )) elif name == '' : raise KeyError ( \"module name can't be empty string \\\"\\\" \" ) self . _modules [ name ] = module add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7f45559664f0 >> ) -> None inherited \u00b6 Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Parameters: Name Type Description Default queue <bound method BaseContext.SimpleQueue of <multiprocessing.context.DefaultContext object at 0x7f45559664f0>> the instance of the queue to append the data. required Source code in zamba/pytorch_lightning/utils.py def add_to_queue ( self , queue : torch . multiprocessing . SimpleQueue ) -> None : \"\"\" Appends the :attr:`trainer.callback_metrics` dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. \"\"\" callback_metrics : dict = apply_to_collection ( self . trainer . callback_metrics , torch . Tensor , lambda x : x . cpu () . numpy () ) # send as numpy to avoid issues with memory sharing queue . put ( callback_metrics ) aggregate_step_outputs ( outputs : Dict [ str , numpy . ndarray ]) -> Tuple [ numpy . ndarray , numpy . ndarray , numpy . ndarray ] staticmethod \u00b6 Source code in zamba/pytorch_lightning/utils.py @staticmethod def aggregate_step_outputs ( outputs : Dict [ str , np . ndarray ] ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: y_true = np . vstack ([ output [ \"y_true\" ] for output in outputs ]) y_pred = np . vstack ([ output [ \"y_pred\" ] for output in outputs ]) y_proba = np . vstack ([ output [ \"y_proba\" ] for output in outputs ]) return y_true , y_pred , y_proba all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) inherited \u00b6 Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Parameters: Name Type Description Default data Union[torch.Tensor, Dict, List, Tuple] int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. required group Optional[Any] the process group to gather results from. Defaults to all processes (world) None sync_grads bool flag that allows users to synchronize gradients for the all_gather operation False Returns: Type Description A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. Source code in zamba/pytorch_lightning/utils.py def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ): r \"\"\" Allows users to call ``self.all_gather()`` from the LightningModule, thus making the ``all_gather`` operation accelerator agnostic. ``all_gather`` is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. \"\"\" group = group if group is not None else torch . distributed . group . WORLD all_gather = self . trainer . accelerator . all_gather data = convert_to_tensors ( data , device = self . device ) return apply_to_collection ( data , torch . Tensor , all_gather , group = group , sync_grads = sync_grads ) apply ( self : ~ T , fn : Callable [[ Module ], NoneType ]) -> ~ T inherited \u00b6 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn class: Module -> None): function to be applied to each submodule required Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Source code in zamba/pytorch_lightning/utils.py def apply ( self : T , fn : Callable [[ 'Module' ], None ]) -> T : r \"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\"\" for module in self . children (): module . apply ( fn ) fn ( self ) return self backward ( self , loss : Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None inherited \u00b6 Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss Tensor The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). required optimizer Optional[torch.optim.optimizer.Optimizer] Current optimizer being used. None if using manual optimization. required optimizer_idx Optional[int] Index of the current optimizer being used. None if using manual optimization. required Example:: def backward(self, loss, optimizer, optimizer_idx): loss.backward() Source code in zamba/pytorch_lightning/utils.py def backward ( self , loss : Tensor , optimizer : Optional [ Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None : \"\"\" Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own implementation if you need to. Args: loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). optimizer: Current optimizer being used. ``None`` if using manual optimization. optimizer_idx: Index of the current optimizer being used. ``None`` if using manual optimization. Example:: def backward(self, loss, optimizer, optimizer_idx): loss.backward() \"\"\" loss . backward ( * args , ** kwargs ) bfloat16 ( self : ~ T ) -> ~ T inherited \u00b6 Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def bfloat16 ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] inherited \u00b6 Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch_lightning/utils.py def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ]: r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for _ , buf in self . named_buffers ( recurse = recurse ): yield buf children ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over immediate children modules. !!! yields Module: a child module Source code in zamba/pytorch_lightning/utils.py def children ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module compute_and_log_metrics ( self , y_true : ndarray , y_pred : ndarray , y_proba : ndarray , subset : str ) \u00b6 Source code in zamba/pytorch_lightning/utils.py def compute_and_log_metrics ( self , y_true : np . ndarray , y_pred : np . ndarray , y_proba : np . ndarray , subset : str ): self . log ( f \" { subset } _macro_f1\" , f1_score ( y_true , y_pred , average = \"macro\" , zero_division = 0 )) # if only two classes, skip top_k accuracy since not enough classes if self . num_classes > 2 : for k in DEFAULT_TOP_K : if k < self . num_classes : self . log ( f \" { subset } _top_ { k } _accuracy\" , top_k_accuracy_score ( y_true . argmax ( axis = 1 ), # top k accuracy only supports single label case y_proba , labels = np . arange ( y_proba . shape [ 1 ]), k = k , ), ) else : self . log ( f \" { subset } _accuracy\" , accuracy_score ( y_true , y_pred )) for metric_name , label , metric in compute_species_specific_metrics ( y_true , y_pred , self . species ): self . log ( f \"species/ { subset } _ { metric_name } / { label } \" , metric ) configure_callbacks ( self ) inherited \u00b6 Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Returns: Type Description A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] !!! note Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here. Source code in zamba/pytorch_lightning/utils.py def configure_callbacks ( self ): \"\"\" Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth:`~pytorch_lightning.callbacks.base.Callback.on_init_start` will never be invoked on the new callbacks returned here. \"\"\" return [] configure_optimizers ( self ) \u00b6 Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. Source code in zamba/pytorch_lightning/utils.py def configure_optimizers ( self ): \"\"\" Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. \"\"\" optim = self . _get_optimizer () if self . scheduler is None : return optim else : return { \"optimizer\" : optim , \"lr_scheduler\" : self . scheduler ( optim , ** ({} if self . scheduler_params is None else self . scheduler_params ) ), } configure_sharded_model ( self ) -> None inherited \u00b6 Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. The accelerator manages whether to call this hook at every given stage. For sharded plugins where model parallelism is required, the hook is usually on called once to initialize the sharded parameters, and not called again in the same process. By default for accelerators/plugins that do not use model sharding techniques, this hook is called during each fit/val/test/predict stages. Source code in zamba/pytorch_lightning/utils.py def configure_sharded_model ( self ) -> None : \"\"\" Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. The accelerator manages whether to call this hook at every given stage. For sharded plugins where model parallelism is required, the hook is usually on called once to initialize the sharded parameters, and not called again in the same process. By default for accelerators/plugins that do not use model sharding techniques, this hook is called during each fit/val/test/predict stages. \"\"\" cpu ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves all model parameters and buffers to the CPU. Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def cpu ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" self . __update_properties ( device = torch . device ( \"cpu\" )) return super () . cpu () cuda ( self , device : Union [ torch . device , int ] = None ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device Union[torch.device, int] if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def cuda ( self , device : Optional [ Union [ torch . device , int ]] = None ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self \"\"\" if device is None or isinstance ( device , int ): device = torch . device ( \"cuda\" , index = device ) self . __update_properties ( device = device ) return super () . cuda ( device = device ) double ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def double ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . double ) return super () . double () eval ( self : ~ T ) -> ~ T inherited \u00b6 Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def eval ( self : T ) -> T : r \"\"\"Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self \"\"\" return self . train ( False ) extra_repr ( self ) -> str inherited \u00b6 Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. Source code in zamba/pytorch_lightning/utils.py def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def float ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``float`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . float ) return super () . float () forward ( self , x ) \u00b6 Same as :meth: torch.nn.Module.forward() . Parameters: Name Type Description Default *args Whatever you decide to pass into the forward method. required **kwargs Keyword arguments are also possible. required Returns: Type Description Your model's output Source code in zamba/pytorch_lightning/utils.py def forward ( self , x ): return self . model ( x ) freeze ( self ) -> None inherited \u00b6 Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() Source code in zamba/pytorch_lightning/utils.py def freeze ( self ) -> None : r \"\"\" Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() \"\"\" for param in self . parameters (): param . requires_grad = False self . eval () from_disk ( path : PathLike ) classmethod \u00b6 Source code in zamba/pytorch_lightning/utils.py @classmethod def from_disk ( cls , path : os . PathLike ): return cls . load_from_checkpoint ( path ) get_buffer ( self , target : str ) -> Tensor inherited \u00b6 Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.Tensor The buffer referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer Source code in zamba/pytorch_lightning/utils.py def get_buffer ( self , target : str ) -> \"Tensor\" : \"\"\" Returns the buffer given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer \"\"\" module_path , _ , buffer_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , buffer_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + buffer_name + \"`\" ) buffer : torch . Tensor = getattr ( mod , buffer_name ) if buffer_name not in mod . _buffers : raise AttributeError ( \"`\" + buffer_name + \"` is not a buffer\" ) return buffer get_extra_state ( self ) -> Any inherited \u00b6 Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict Source code in zamba/pytorch_lightning/utils.py def get_extra_state ( self ) -> Any : \"\"\" Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict \"\"\" raise RuntimeError ( \"Reached a code path in Module.get_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7f45559664f0 >> ) -> None inherited \u00b6 Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Parameters: Name Type Description Default queue <bound method BaseContext.SimpleQueue of <multiprocessing.context.DefaultContext object at 0x7f45559664f0>> the instance of the queue from where to get the data. required Source code in zamba/pytorch_lightning/utils.py def get_from_queue ( self , queue : torch . multiprocessing . SimpleQueue ) -> None : \"\"\" Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency, we cast back the data to ``torch.Tensor``. Args: queue: the instance of the queue from where to get the data. \"\"\" # NOTE: `add_to_queue` needs to be called before callback_metrics : dict = queue . get () self . trainer . callback_metrics . update ( apply_to_collection ( callback_metrics , np . ndarray , lambda x : torch . tensor ( x )) ) get_parameter ( self , target : str ) -> Parameter inherited \u00b6 Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target str The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Parameter The Parameter referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter Source code in zamba/pytorch_lightning/utils.py def get_parameter ( self , target : str ) -> \"Parameter\" : \"\"\" Returns the parameter given by ``target`` if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` \"\"\" module_path , _ , param_name = target . rpartition ( \".\" ) mod : torch . nn . Module = self . get_submodule ( module_path ) if not hasattr ( mod , param_name ): raise AttributeError ( mod . _get_name () + \" has no attribute `\" + param_name + \"`\" ) param : torch . nn . Parameter = getattr ( mod , param_name ) if not isinstance ( param , torch . nn . Parameter ): raise AttributeError ( \"`\" + param_name + \"` is not an \" \"nn.Parameter\" ) return param get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] inherited \u00b6 Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. .. code-block:: Epoch 1: 4%|\u258e | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10] Here is an example how to override the defaults: .. code-block:: python def get_progress_bar_dict(self): # don't show the version number items = super().get_progress_bar_dict() items.pop(\"v_num\", None) return items Returns: Type Description Dict[str, Union[int, str]] Dictionary with the items to be displayed in the progress bar. Source code in zamba/pytorch_lightning/utils.py def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]: r \"\"\" Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. .. code-block:: Epoch 1: 4%|\u258e | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10] Here is an example how to override the defaults: .. code-block:: python def get_progress_bar_dict(self): # don't show the version number items = super().get_progress_bar_dict() items.pop(\"v_num\", None) return items Return: Dictionary with the items to be displayed in the progress bar. \"\"\" # call .item() only once but store elements without graphs running_train_loss = self . trainer . fit_loop . running_loss . mean () avg_training_loss = None if running_train_loss is not None : avg_training_loss = running_train_loss . cpu () . item () elif self . automatic_optimization : avg_training_loss = float ( \"NaN\" ) tqdm_dict = {} if avg_training_loss is not None : tqdm_dict [ \"loss\" ] = f \" { avg_training_loss : .3g } \" module_tbptt_enabled = self . truncated_bptt_steps > 0 trainer_tbptt_enabled = self . trainer . truncated_bptt_steps is not None and self . trainer . truncated_bptt_steps > 0 if module_tbptt_enabled or trainer_tbptt_enabled : tqdm_dict [ \"split_idx\" ] = self . trainer . fit_loop . split_idx if self . trainer . logger is not None and self . trainer . logger . version is not None : version = self . trainer . logger . version # show last 4 places of long version strings version = version [ - 4 :] if isinstance ( version , str ) else version tqdm_dict [ \"v_num\" ] = version return tqdm_dict get_submodule ( self , target : str ) -> Module inherited \u00b6 Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target str The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) required Returns: Type Description torch.nn.Module The submodule referenced by target Exceptions: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module Source code in zamba/pytorch_lightning/utils.py def get_submodule ( self , target : str ) -> \"Module\" : \"\"\" Returns the submodule given by ``target`` if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` \"\"\" if target == \"\" : return self atoms : List [ str ] = target . split ( \".\" ) mod : torch . nn . Module = self for item in atoms : if not hasattr ( mod , item ): raise AttributeError ( mod . _get_name () + \" has no \" \"attribute `\" + item + \"`\" ) mod = getattr ( mod , item ) if not isinstance ( mod , torch . nn . Module ): raise AttributeError ( \"`\" + item + \"` is not \" \"an nn.Module\" ) return mod grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ] inherited \u00b6 Compute each parameter's gradient's norm and their overall norm. .. deprecated:: v1.3 Will be removed in v1.5.0. Use :func: pytorch_lightning.utilities.grads.grad_norm instead. Source code in zamba/pytorch_lightning/utils.py def grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ]: \"\"\"Compute each parameter's gradient's norm and their overall norm. .. deprecated:: v1.3 Will be removed in v1.5.0. Use :func:`pytorch_lightning.utilities.grads.grad_norm` instead. \"\"\" rank_zero_deprecation ( \"LightningModule.grad_norm is deprecated in v1.3 and will be removed in v1.5.\" \" Use grad_norm from pytorch_lightning.utilities.grads instead.\" ) return new_grad_norm ( self , norm_type ) half ( self ) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def half ( self ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\"\" self . __update_properties ( dtype = torch . half ) return super () . half () load_state_dict ( self , state_dict : OrderedDict [ str , Tensor ], strict : bool = True ) inherited \u00b6 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys !!! note If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . Source code in zamba/pytorch_lightning/utils.py def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ): r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as ``None`` and its corresponding key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a ``RuntimeError``. \"\"\" missing_keys : List [ str ] = [] unexpected_keys : List [ str ] = [] error_msgs : List [ str ] = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : # mypy isn't aware that \"_metadata\" exists in state_dict state_dict . _metadata = metadata # type: ignore[attr-defined] def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) del load if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {} . ' . format ( ', ' . join ( '\" {} \"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {} : \\n\\t {} ' . format ( self . __class__ . __name__ , \" \\n\\t \" . join ( error_msgs ))) return _IncompatibleKeys ( missing_keys , unexpected_keys ) log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None inherited \u00b6 Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name str key to log required value Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number, Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number]]] value to log. Can be a float , Tensor , Metric , or a dictionary of the former. required prog_bar bool if True logs to the progress bar False logger bool if True logs to the logger True on_step Optional[bool] if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch Optional[bool] if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx Union[str, Callable] reduction function over step values for end of epoch. :meth: torch.mean by default. 'default' enable_graph bool if True, will not auto detach the graph False sync_dist bool if True, reduces the metric across GPUs/TPUs False sync_dist_group Optional[Any] the ddp group to sync across None add_dataloader_idx bool if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values True batch_size Optional[int] Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute Optional[str] To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only Optional[bool] Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None Source code in zamba/pytorch_lightning/utils.py def log ( self , name : str , value : _METRIC_COLLECTION , prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = \"default\" , # TODO: change to 'mean' when `sync_dist_op` is removed in 1.6 tbptt_reduce_fx : Optional = None , # noqa: Remove in 1.6 tbptt_pad_token : Optional = None , # noqa: Remove in 1.6 enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , # noqa: Remove in 1.6 sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None , ) -> None : \"\"\" Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: ``*`` also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step*\", \"F\", \"T\", \"F\", \"T\" \"validation_step_end*\", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Args: name: key to log value: value to log. Can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former. prog_bar: if True logs to the progress bar logger: if True logs to the logger on_step: if True logs at this step. None auto-logs at the training_step but not validation/test_step on_epoch: if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default. enable_graph: if True, will not auto detach the graph sync_dist: if True, reduces the metric across GPUs/TPUs sync_dist_group: the ddp group to sync across add_dataloader_idx: if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values batch_size: Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. metric_attribute: To restore the metric state, Lightning requires the reference of the :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute. rank_zero_only: Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. \"\"\" if tbptt_reduce_fx is not None : rank_zero_deprecation ( \"`self.log(tbptt_reduce_fx=...)` is no longer supported. The flag will be removed in v1.6.\" \" Please, open a discussion explaining your use-case in\" \" `https://github.com/PyTorchLightning/pytorch-lightning/discussions`\" ) if tbptt_pad_token is not None : rank_zero_deprecation ( \"`self.log(tbptt_pad_token=...)` is no longer supported. The flag will be removed in v1.6.\" \" Please, open a discussion explaining your use-case in\" \" `https://github.com/PyTorchLightning/pytorch-lightning/discussions`\" ) if sync_dist_op is not None : rank_zero_deprecation ( f \"`self.log(sync_dist_op=' { sync_dist_op } ')` is deprecated and will be removed in v.1.6.\" f \" Use `self.log(reduce_fx= { sync_dist_op } )` instead.\" ) if reduce_fx == \"default\" : reduce_fx = sync_dist_op elif reduce_fx == \"default\" : reduce_fx = \"mean\" # check for invalid values apply_to_collection ( value , dict , self . __check_not_nested , name ) apply_to_collection ( value , object , self . __check_allowed , name , value , wrong_dtype = ( numbers . Number , Metric , Tensor , dict ) ) # set the default depending on the fx_name on_step = self . __auto_choose_log_on_step ( on_step ) on_epoch = self . __auto_choose_log_on_epoch ( on_epoch ) results = self . trainer . _results assert results is not None assert self . _current_fx_name is not None FxValidator . check_logging ( self . _current_fx_name , on_step = on_step , on_epoch = on_epoch ) # make sure user doesn't introduce logic for multi-dataloaders if \"/dataloader_idx_\" in name : raise MisconfigurationException ( f \"You called `self.log` with the key ` { name } `\" \" but it should not contain information about `dataloader_idx`\" ) value = apply_to_collection ( value , numbers . Number , self . __to_tensor ) if self . trainer . logger_connector . should_reset_tensors ( self . _current_fx_name ): # if we started a new epoch (running it's first batch) the hook name has changed # reset any tensors for the new hook name results . reset ( metrics = False , fx = self . _current_fx_name ) if metric_attribute is None and isinstance ( value , Metric ): if self . _metric_attributes is None : # compute once self . _metric_attributes = { id ( module ): name for name , module in self . named_modules () if isinstance ( module , Metric ) } if not self . _metric_attributes : raise MisconfigurationException ( \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\" \" You can fix this by setting an attribute for the metric in your `LightningModule`.\" ) # try to find the passed metric in the LightningModule metric_attribute = self . _metric_attributes . get ( id ( value ), None ) if metric_attribute is None : raise MisconfigurationException ( \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\" f \" You can fix this by calling `self.log( { name } , ..., metric_attribute=name)` where `name` is one\" f \" of { list ( self . _metric_attributes . values ()) } \" ) results . log ( self . _current_fx_name , name , value , prog_bar = prog_bar , logger = logger , on_step = on_step , on_epoch = on_epoch , reduce_fx = reduce_fx , enable_graph = enable_graph , dataloader_idx = ( self . _current_dataloader_idx if add_dataloader_idx else None ), batch_size = batch_size , sync_dist = sync_dist and distributed_available (), sync_dist_fn = self . trainer . training_type_plugin . reduce or sync_ddp , sync_dist_group = sync_dist_group , metric_attribute = metric_attribute , rank_zero_only = rank_zero_only , ) self . trainer . logger_connector . _current_fx = self . _current_fx_name log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , numbers . Number ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True ) -> None inherited \u00b6 Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number, Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number]]]] key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. required prog_bar bool if True logs to the progress base False logger bool if True logs to the logger True on_step Optional[bool] if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch Optional[bool] if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx Union[str, Callable] reduction function over step values for end of epoch. :meth: torch.mean by default. 'default' enable_graph bool if True, will not auto detach the graph False sync_dist bool if True, reduces the metric across GPUs/TPUs False sync_dist_group Optional[Any] the ddp group sync across None add_dataloader_idx bool if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values True Source code in zamba/pytorch_lightning/utils.py def log_dict ( self , dictionary : Mapping [ str , _METRIC_COLLECTION ], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = \"default\" , # TODO: change to 'mean' when `sync_dist_op` is removed in 1.6 tbptt_reduce_fx : Optional [ Any ] = None , # noqa: Remove in 1.6 tbptt_pad_token : Optional [ Any ] = None , # noqa: Remove in 1.6 enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , # noqa: Remove in 1.6 sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , ) -> None : \"\"\" Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Args: dictionary: key value pairs. The values can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former. prog_bar: if True logs to the progress base logger: if True logs to the logger on_step: if True logs at this step. None auto-logs for training_step but not validation/test_step on_epoch: if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default. enable_graph: if True, will not auto detach the graph sync_dist: if True, reduces the metric across GPUs/TPUs sync_dist_group: the ddp group sync across add_dataloader_idx: if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values \"\"\" for k , v in dictionary . items (): self . log ( name = k , value = v , prog_bar = prog_bar , logger = logger , on_step = on_step , on_epoch = on_epoch , reduce_fx = reduce_fx , enable_graph = enable_graph , sync_dist = sync_dist , sync_dist_group = sync_dist_group , sync_dist_op = sync_dist_op , tbptt_pad_token = tbptt_pad_token , tbptt_reduce_fx = tbptt_reduce_fx , add_dataloader_idx = add_dataloader_idx , ) log_grad_norm ( self , grad_norm_dict : Dict [ str , torch . Tensor ]) -> None inherited \u00b6 Override this method to change the default behaviour of log_grad_norm . Parameters: Name Type Description Default grad_norm_dict Dict[str, torch.Tensor] Dictionary containing current grad norm metrics required Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) Source code in zamba/pytorch_lightning/utils.py def log_grad_norm ( self , grad_norm_dict : Dict [ str , torch . Tensor ]) -> None : \"\"\"Override this method to change the default behaviour of ``log_grad_norm``. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) \"\"\" self . log_dict ( grad_norm_dict , on_step = True , on_epoch = True , prog_bar = True , logger = True ) lr_schedulers ( self ) -> Union [ Any , List [ Any ]] inherited \u00b6 Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no schedulers were returned in meth: configure_optimizers . Source code in zamba/pytorch_lightning/utils.py def lr_schedulers ( self ) -> Optional [ Union [ Any , List [ Any ]]]: \"\"\" Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no schedulers were returned in :meth:`configure_optimizers`. \"\"\" if not self . trainer . lr_schedulers : return None # ignore other keys \"interval\", \"frequency\", etc. lr_schedulers = [ s [ \"scheduler\" ] for s in self . trainer . lr_schedulers ] # single scheduler if len ( lr_schedulers ) == 1 : return lr_schedulers [ 0 ] # multiple schedulers return lr_schedulers manual_backward ( self , loss : Tensor , * args , ** kwargs ) -> None inherited \u00b6 Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss Tensor The tensor on which to compute gradients. Must have a graph attached. required *args Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward () **kwargs Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward {} Source code in zamba/pytorch_lightning/utils.py def manual_backward ( self , loss : Tensor , * args , ** kwargs ) -> None : \"\"\" Call this directly from your :meth:`training_step` when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref:`manual optimization<common/optimizers:Manual optimization>` for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Args: loss: The tensor on which to compute gradients. Must have a graph attached. *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward` **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward` \"\"\" # make sure we're using manual opt self . _verify_is_manual_optimization ( \"manual_backward\" ) # backward self . trainer . fit_loop . epoch_loop . batch_loop . backward ( loss , None , None , * args , ** kwargs ) modules ( self ) -> Iterator [ Module ] inherited \u00b6 Returns an iterator over all modules in the network. !!! yields Module: a module in the network !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) Source code in zamba/pytorch_lightning/utils.py def modules ( self ) -> Iterator [ 'Module' ]: r \"\"\"Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\"\" for _ , module in self . named_modules (): yield module named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] inherited \u00b6 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. '' recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. True !!! yields (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Source code in zamba/pytorch_lightning/utils.py def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children ( self ) -> Iterator [ Tuple [ str , Module ]] inherited \u00b6 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. !!! yields (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) Source code in zamba/pytorch_lightning/utils.py def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules ( self , memo : Optional [ Set [ Module ]] = None , prefix : str = '' , remove_duplicate : bool = True ) inherited \u00b6 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters: Name Type Description Default memo Optional[Set[Module]] a memo to store the set of modules already added to the result None prefix str a prefix that will be added to the name of the module '' remove_duplicate bool whether to remove the duplicated module instances in the result True !!! yields (string, Module): Tuple of name and module !!! note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Source code in zamba/pytorch_lightning/utils.py def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' , remove_duplicate : bool = True ): r \"\"\"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\"\" if memo is None : memo = set () if self not in memo : if remove_duplicate : memo . add ( self ) yield prefix , self for name , module in self . _modules . items (): if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix , remove_duplicate ): yield m named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] inherited \u00b6 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. '' recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Source code in zamba/pytorch_lightning/utils.py def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Parameter ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem on_after_backward ( self ) -> None inherited \u00b6 Called after loss.backward() and before optimizers are stepped. !!! note If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients. Source code in zamba/pytorch_lightning/utils.py def on_after_backward ( self ) -> None : \"\"\" Called after ``loss.backward()`` and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the ``on_before_optimizer_step`` if you need the unscaled gradients. \"\"\" on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any inherited \u00b6 Override to alter or apply batch augmentations to your batch after it is transferred to the device. !!! note To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be altered or augmented. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch See Also: - :meth: on_before_batch_transfer - :meth: transfer_batch_to_device Source code in zamba/pytorch_lightning/utils.py def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any : \"\"\" Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be altered or augmented. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`on_before_batch_transfer` - :meth:`transfer_batch_to_device` \"\"\" return batch on_before_backward ( self , loss : Tensor ) -> None inherited \u00b6 Called before loss.backward() . Parameters: Name Type Description Default loss Tensor Loss divided by number of batches for gradient accumulation and scaled if using native AMP. required Source code in zamba/pytorch_lightning/utils.py def on_before_backward ( self , loss : torch . Tensor ) -> None : \"\"\" Called before ``loss.backward()``. Args: loss: Loss divided by number of batches for gradient accumulation and scaled if using native AMP. \"\"\" pass on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any inherited \u00b6 Override to alter or apply batch augmentations to your batch before it is transferred to the device. !!! note To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be altered or augmented. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch See Also: - :meth: on_after_batch_transfer - :meth: transfer_batch_to_device Source code in zamba/pytorch_lightning/utils.py def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any : \"\"\" Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be altered or augmented. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`on_after_batch_transfer` - :meth:`transfer_batch_to_device` \"\"\" return batch on_before_optimizer_step ( self , optimizer : Optimizer , optimizer_idx : int ) -> None inherited \u00b6 Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. Parameters: Name Type Description Default optimizer Optimizer Current optimizer being used. required optimizer_idx int Index of the current optimizer being used. required Example:: def on_before_optimizer_step(self, optimizer, optimizer_idx): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) Source code in zamba/pytorch_lightning/utils.py def on_before_optimizer_step ( self , optimizer : Optimizer , optimizer_idx : int ) -> None : \"\"\" Called before ``optimizer.step()``. The hook is only called if gradients do not need to be accumulated. See: :paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`. If using native AMP, the loss will be unscaled before calling this hook. See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__ for more information on the scaling of gradients. Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. Example:: def on_before_optimizer_step(self, optimizer, optimizer_idx): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) \"\"\" on_before_zero_grad ( self , optimizer : Optimizer ) -> None inherited \u00b6 Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer Optimizer The optimizer for which grads should be zeroed. required Source code in zamba/pytorch_lightning/utils.py def on_before_zero_grad ( self , optimizer : Optimizer ) -> None : \"\"\" Called after ``training_step()`` and before ``optimizer.zero_grad()``. Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Args: optimizer: The optimizer for which grads should be zeroed. \"\"\" on_epoch_end ( self ) -> None inherited \u00b6 Called when either of train/val/test epoch ends. Source code in zamba/pytorch_lightning/utils.py def on_epoch_end ( self ) -> None : \"\"\" Called when either of train/val/test epoch ends. \"\"\" on_epoch_start ( self ) -> None inherited \u00b6 Called when either of train/val/test epoch begins. Source code in zamba/pytorch_lightning/utils.py def on_epoch_start ( self ) -> None : \"\"\" Called when either of train/val/test epoch begins. \"\"\" on_fit_end ( self ) -> None inherited \u00b6 Called at the very end of fit. If on DDP it is called on every process Source code in zamba/pytorch_lightning/utils.py def on_fit_end ( self ) -> None : \"\"\" Called at the very end of fit. If on DDP it is called on every process \"\"\" on_fit_start ( self ) -> None inherited \u00b6 Called at the very beginning of fit. If on DDP it is called on every process Source code in zamba/pytorch_lightning/utils.py def on_fit_start ( self ) -> None : \"\"\" Called at the very beginning of fit. If on DDP it is called on every process \"\"\" on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary with variables from the checkpoint. required Source code in zamba/pytorch_lightning/utils.py def on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\" on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. required Source code in zamba/pytorch_lightning/utils.py def on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\" on_load_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Do something with the checkpoint. Gives model a chance to load something before state_dict is restored. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary with variables from the checkpoint. required Source code in zamba/pytorch_lightning/utils.py def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Do something with the checkpoint. Gives model a chance to load something before ``state_dict`` is restored. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\" on_post_move_to_device ( self ) -> None inherited \u00b6 Called in the parameter_validation decorator after :meth: ~pytorch_lightning.core.LightningModule.to is called. This is a good place to tie weights between modules after moving them to a device. Can be used when training models with weight sharing properties on TPU. Addresses the handling of shared weights on TPU: https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks Example:: def on_post_move_to_device(self): self.decoder.weight = self.encoder.weight Source code in zamba/pytorch_lightning/utils.py def on_post_move_to_device ( self ) -> None : \"\"\" Called in the ``parameter_validation`` decorator after :meth:`~pytorch_lightning.core.LightningModule.to` is called. This is a good place to tie weights between modules after moving them to a device. Can be used when training models with weight sharing properties on TPU. Addresses the handling of shared weights on TPU: https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks Example:: def on_post_move_to_device(self): self.decoder.weight = self.encoder.weight \"\"\" on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the predict loop after the batch. Parameters: Name Type Description Default outputs Optional[Any] The outputs of predict_step_end(test_step(x)) required batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/pytorch_lightning/utils.py def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the predict loop after the batch. Args: outputs: The outputs of predict_step_end(test_step(x)) batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/pytorch_lightning/utils.py def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the predict loop before anything happens for that batch. Args: batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_predict_dataloader ( self ) -> None inherited \u00b6 Called before requesting the predict dataloader. .. deprecated:: v1.5 :meth: on_predict_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: predict_dataloader() directly. Source code in zamba/pytorch_lightning/utils.py def on_predict_dataloader ( self ) -> None : \"\"\"Called before requesting the predict dataloader. .. deprecated:: v1.5 :meth:`on_predict_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`predict_dataloader()` directly. \"\"\" on_predict_end ( self ) -> None inherited \u00b6 Called at the end of predicting. Source code in zamba/pytorch_lightning/utils.py def on_predict_end ( self ) -> None : \"\"\" Called at the end of predicting. \"\"\" on_predict_epoch_end ( self , results : List [ Any ]) -> None inherited \u00b6 Called at the end of predicting. Source code in zamba/pytorch_lightning/utils.py def on_predict_epoch_end ( self , results : List [ Any ]) -> None : \"\"\" Called at the end of predicting. \"\"\" on_predict_epoch_start ( self ) -> None inherited \u00b6 Called at the beginning of predicting. Source code in zamba/pytorch_lightning/utils.py def on_predict_epoch_start ( self ) -> None : \"\"\" Called at the beginning of predicting. \"\"\" on_predict_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the predict loop Source code in zamba/pytorch_lightning/utils.py def on_predict_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the predict loop \"\"\" self . trainer . model . eval () on_predict_start ( self ) -> None inherited \u00b6 Called at the beginning of predicting. Source code in zamba/pytorch_lightning/utils.py def on_predict_start ( self ) -> None : \"\"\" Called at the beginning of predicting. \"\"\" on_pretrain_routine_end ( self ) -> None inherited \u00b6 Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start Source code in zamba/pytorch_lightning/utils.py def on_pretrain_routine_end ( self ) -> None : \"\"\" Called at the end of the pretrain routine (between fit and train start). - fit - pretrain_routine start - pretrain_routine end - training_start \"\"\" on_pretrain_routine_start ( self ) -> None inherited \u00b6 Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start Source code in zamba/pytorch_lightning/utils.py def on_pretrain_routine_start ( self ) -> None : \"\"\" Called at the beginning of the pretrain routine (between fit and train start). - fit - pretrain_routine start - pretrain_routine end - training_start \"\"\" on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None inherited \u00b6 Give the model a chance to add something to the checkpoint. state_dict is already there. Parameters: Name Type Description Default checkpoint Dict[str, Any] A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. required Source code in zamba/pytorch_lightning/utils.py def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Give the model a chance to add something to the checkpoint. ``state_dict`` is already there. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\" on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the test loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of test_step_end(test_step(x)) required batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/pytorch_lightning/utils.py def on_test_batch_end ( self , outputs : Optional [ STEP_OUTPUT ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the test loop after the batch. Args: outputs: The outputs of test_step_end(test_step(x)) batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the test DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/pytorch_lightning/utils.py def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the test loop before anything happens for that batch. Args: batch: The batched data as it is returned by the test DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_test_dataloader ( self ) -> None inherited \u00b6 Called before requesting the test dataloader. .. deprecated:: v1.5 :meth: on_test_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: test_dataloader() directly. Source code in zamba/pytorch_lightning/utils.py def on_test_dataloader ( self ) -> None : \"\"\"Called before requesting the test dataloader. .. deprecated:: v1.5 :meth:`on_test_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`test_dataloader()` directly. \"\"\" on_test_end ( self ) -> None inherited \u00b6 Called at the end of testing. Source code in zamba/pytorch_lightning/utils.py def on_test_end ( self ) -> None : \"\"\" Called at the end of testing. \"\"\" on_test_epoch_end ( self ) -> None inherited \u00b6 Called in the test loop at the very end of the epoch. Source code in zamba/pytorch_lightning/utils.py def on_test_epoch_end ( self ) -> None : \"\"\" Called in the test loop at the very end of the epoch. \"\"\" on_test_epoch_start ( self ) -> None inherited \u00b6 Called in the test loop at the very beginning of the epoch. Source code in zamba/pytorch_lightning/utils.py def on_test_epoch_start ( self ) -> None : \"\"\" Called in the test loop at the very beginning of the epoch. \"\"\" on_test_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the test loop Source code in zamba/pytorch_lightning/utils.py def on_test_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the test loop \"\"\" self . trainer . model . eval () on_test_model_train ( self ) -> None inherited \u00b6 Sets the model to train during the test loop Source code in zamba/pytorch_lightning/utils.py def on_test_model_train ( self ) -> None : \"\"\" Sets the model to train during the test loop \"\"\" self . trainer . model . train () on_test_start ( self ) -> None inherited \u00b6 Called at the beginning of testing. Source code in zamba/pytorch_lightning/utils.py def on_test_start ( self ) -> None : \"\"\" Called at the beginning of testing. \"\"\" on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the training loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of training_step_end(training_step(x)) required batch Any The batched data as it is returned by the training DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/pytorch_lightning/utils.py def on_train_batch_end ( self , outputs : STEP_OUTPUT , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the training loop after the batch. Args: outputs: The outputs of training_step_end(training_step(x)) batch: The batched data as it is returned by the training DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_train_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the training DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/pytorch_lightning/utils.py def on_train_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_train_dataloader ( self ) -> None inherited \u00b6 Called before requesting the train dataloader. .. deprecated:: v1.5 :meth: on_train_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: train_dataloader() directly. Source code in zamba/pytorch_lightning/utils.py def on_train_dataloader ( self ) -> None : \"\"\"Called before requesting the train dataloader. .. deprecated:: v1.5 :meth:`on_train_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`train_dataloader()` directly. \"\"\" on_train_end ( self ) -> None inherited \u00b6 Called at the end of training before logger experiment is closed. Source code in zamba/pytorch_lightning/utils.py def on_train_end ( self ) -> None : \"\"\" Called at the end of training before logger experiment is closed. \"\"\" on_train_epoch_end ( self , unused : Optional = None ) -> None inherited \u00b6 Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook Source code in zamba/pytorch_lightning/utils.py def on_train_epoch_end ( self , unused : Optional = None ) -> None : \"\"\" Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: 1. Implement `training_epoch_end` in the LightningModule OR 2. Cache data across steps on the attribute(s) of the `LightningModule` and access them in this hook \"\"\" on_train_epoch_start ( self ) -> None inherited \u00b6 Called in the training loop at the very beginning of the epoch. Source code in zamba/pytorch_lightning/utils.py def on_train_epoch_start ( self ) -> None : \"\"\" Called in the training loop at the very beginning of the epoch. \"\"\" on_train_start ( self ) \u00b6 Called at the beginning of training after sanity check. Source code in zamba/pytorch_lightning/utils.py def on_train_start ( self ): metrics = { \"val_macro_f1\" : {}} if self . num_classes > 2 : metrics . update ( { f \"val_top_ { k } _accuracy\" : {} for k in DEFAULT_TOP_K if k < self . num_classes } ) else : metrics . update ({ \"val_accuracy\" : {}}) # write hparams to hparams.yaml file, log metrics to tb hparams tab self . logger . log_hyperparams ( self . hparams , metrics ) on_val_dataloader ( self ) -> None inherited \u00b6 Called before requesting the val dataloader. .. deprecated:: v1.5 :meth: on_val_dataloader is deprecated and will be removed in v1.7.0. Please use :meth: val_dataloader() directly. Source code in zamba/pytorch_lightning/utils.py def on_val_dataloader ( self ) -> None : \"\"\"Called before requesting the val dataloader. .. deprecated:: v1.5 :meth:`on_val_dataloader` is deprecated and will be removed in v1.7.0. Please use :meth:`val_dataloader()` directly. \"\"\" on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the validation loop after the batch. Parameters: Name Type Description Default outputs Union[torch.Tensor, Dict[str, Any]] The outputs of validation_step_end(validation_step(x)) required batch Any The batched data as it is returned by the validation DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/pytorch_lightning/utils.py def on_validation_batch_end ( self , outputs : Optional [ STEP_OUTPUT ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the validation loop after the batch. Args: outputs: The outputs of validation_step_end(validation_step(x)) batch: The batched data as it is returned by the validation DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None inherited \u00b6 Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch Any The batched data as it is returned by the validation DataLoader. required batch_idx int the index of the batch required dataloader_idx int the index of the dataloader required Source code in zamba/pytorch_lightning/utils.py def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None : \"\"\" Called in the validation loop before anything happens for that batch. Args: batch: The batched data as it is returned by the validation DataLoader. batch_idx: the index of the batch dataloader_idx: the index of the dataloader \"\"\" on_validation_end ( self ) -> None inherited \u00b6 Called at the end of validation. Source code in zamba/pytorch_lightning/utils.py def on_validation_end ( self ) -> None : \"\"\" Called at the end of validation. \"\"\" on_validation_epoch_end ( self ) -> None inherited \u00b6 Called in the validation loop at the very end of the epoch. Source code in zamba/pytorch_lightning/utils.py def on_validation_epoch_end ( self ) -> None : \"\"\" Called in the validation loop at the very end of the epoch. \"\"\" on_validation_epoch_start ( self ) -> None inherited \u00b6 Called in the validation loop at the very beginning of the epoch. Source code in zamba/pytorch_lightning/utils.py def on_validation_epoch_start ( self ) -> None : \"\"\" Called in the validation loop at the very beginning of the epoch. \"\"\" on_validation_model_eval ( self ) -> None inherited \u00b6 Sets the model to eval during the val loop Source code in zamba/pytorch_lightning/utils.py def on_validation_model_eval ( self ) -> None : \"\"\" Sets the model to eval during the val loop \"\"\" self . trainer . model . eval () on_validation_model_train ( self ) -> None inherited \u00b6 Sets the model to train during the val loop Source code in zamba/pytorch_lightning/utils.py def on_validation_model_train ( self ) -> None : \"\"\" Sets the model to train during the val loop \"\"\" self . trainer . model . train () on_validation_start ( self ) -> None inherited \u00b6 Called at the beginning of validation. Source code in zamba/pytorch_lightning/utils.py def on_validation_start ( self ) -> None : \"\"\" Called at the beginning of validation. \"\"\" optimizer_step ( self , epoch : int = None , batch_idx : int = None , optimizer : Optimizer = None , optimizer_idx : int = None , optimizer_closure : Optional [ Callable ] = None , on_tpu : bool = None , using_native_amp : bool = None , using_lbfgs : bool = None ) -> None inherited \u00b6 Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . !!! warning If you are overriding this method, make sure that you pass the optimizer_closure parameter to optimizer.step() function as shown in the examples. This ensures that training_step() , optimizer.zero_grad() , backward() are called within the training loop. Parameters: Name Type Description Default epoch int Current epoch None batch_idx int Index of current batch None optimizer Optimizer A PyTorch optimizer None optimizer_idx int If you used multiple optimizers, this indexes into that list. None optimizer_closure Optional[Callable] Closure for all optimizers None on_tpu bool True if TPU backward is required None using_native_amp bool True if using native amp None using_lbfgs bool True if the matching optimizer is :class: torch.optim.LBFGS None Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) Source code in zamba/pytorch_lightning/utils.py def optimizer_step ( self , epoch : int = None , batch_idx : int = None , optimizer : Optimizer = None , optimizer_idx : int = None , optimizer_closure : Optional [ Callable ] = None , on_tpu : bool = None , using_native_amp : bool = None , using_lbfgs : bool = None , ) -> None : r \"\"\" Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer. By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer. This method (and ``zero_grad()``) won't be called during the accumulation phase when ``Trainer(accumulate_grad_batches != 1)``. Warning: If you are overriding this method, make sure that you pass the ``optimizer_closure`` parameter to ``optimizer.step()`` function as shown in the examples. This ensures that ``training_step()``, ``optimizer.zero_grad()``, ``backward()`` are called within the training loop. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers on_tpu: ``True`` if TPU backward is required using_native_amp: ``True`` if using native amp using_lbfgs: True if the matching optimizer is :class:`torch.optim.LBFGS` Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) \"\"\" optimizer . step ( closure = optimizer_closure ) optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ) inherited \u00b6 Override this method to change the default behaviour of optimizer.zero_grad() . Parameters: Name Type Description Default epoch int Current epoch required batch_idx int Index of current batch required optimizer Optimizer A PyTorch optimizer required optimizer_idx int If you used multiple optimizers this indexes into that list. required Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example. Source code in zamba/pytorch_lightning/utils.py def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ): \"\"\"Override this method to change the default behaviour of ``optimizer.zero_grad()``. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example. \"\"\" optimizer . zero_grad () optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] inherited \u00b6 Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer bool If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. True Returns: Type Description Union[torch.optim.optimizer.Optimizer, pytorch_lightning.core.optimizer.LightningOptimizer, List[torch.optim.optimizer.Optimizer], List[pytorch_lightning.core.optimizer.LightningOptimizer]] A single optimizer, or a list of optimizers in case multiple ones are present. Source code in zamba/pytorch_lightning/utils.py def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ Optimizer , LightningOptimizer , List [ Optimizer ], List [ LightningOptimizer ]]: \"\"\" Returns the optimizer(s) that are being used during training. Useful for manual optimization. Args: use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a :class:`~pytorch_lightning.core.optimizer.LightningOptimizer` for automatic handling of precision and profiling. Returns: A single optimizer, or a list of optimizers in case multiple ones are present. \"\"\" if use_pl_optimizer : opts = list ( self . trainer . lightning_optimizers . values ()) else : opts = self . trainer . optimizers # single optimizer if isinstance ( opts , list ) and len ( opts ) == 1 and isinstance ( opts [ 0 ], ( Optimizer , LightningOptimizer )): return opts [ 0 ] # multiple opts return opts parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] inherited \u00b6 Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. True !!! yields Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) Source code in zamba/pytorch_lightning/utils.py def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ]: r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ): yield param predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. !!! note In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. Source code in zamba/pytorch_lightning/utils.py def predict_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth:`prepare_data`. - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth:`predict` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" predict_step ( self , batch , batch_idx , dataloader_idx : Optional [ int ] = None ) \u00b6 Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(accelerator=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Parameters: Name Type Description Default batch Current batch required batch_idx Index of current batch required dataloader_idx Optional[int] Index of the current dataloader None Returns: Type Description Predicted output Source code in zamba/pytorch_lightning/utils.py def predict_step ( self , batch , batch_idx , dataloader_idx : Optional [ int ] = None ): x , y = batch y_hat = self ( x ) pred = torch . sigmoid ( y_hat ) . cpu () . numpy () return pred prepare_data ( self ) -> None inherited \u00b6 Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() Source code in zamba/pytorch_lightning/utils.py def prepare_data ( self ) -> None : \"\"\" Use this to download and prepare data. .. warning:: DO NOT set state to the model (use `setup` instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): 1. Once per node. This is the default and is only called on LOCAL_RANK=0. 2. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() \"\"\" print ( self , * args , ** kwargs ) -> None inherited \u00b6 Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args The thing to print. The same as for Python's built-in print function. () **kwargs The same as for Python's built-in print function. {} Example:: def forward(self, x): self.print(x, 'in forward') Source code in zamba/pytorch_lightning/utils.py def print ( self , * args , ** kwargs ) -> None : r \"\"\" Prints only from process 0. Use this in any distributed mode to log only once. Args: *args: The thing to print. The same as for Python's built-in print function. **kwargs: The same as for Python's built-in print function. Example:: def forward(self, x): self.print(x, 'in forward') \"\"\" if self . trainer . is_global_zero : progress_bar = self . trainer . progress_bar_callback if progress_bar is not None and progress_bar . is_enabled : progress_bar . print ( * args , ** kwargs ) else : print ( * args , ** kwargs ) register_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch_lightning/utils.py def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is True : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = False handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None inherited \u00b6 Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters: Name Type Description Default name string name of the buffer. The buffer can be accessed from this module using the given name required tensor Tensor or None buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . required persistent bool whether the buffer is part of this module's :attr: state_dict . True Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Source code in zamba/pytorch_lightning/utils.py def register_buffer ( self , name : str , tensor : Optional [ Tensor ], persistent : bool = True ) -> None : r \"\"\"Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\"\" if persistent is False and isinstance ( self , torch . jit . ScriptModule ): raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"buffer name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ): raise TypeError ( \"cannot assign ' {} ' object to buffer ' {} ' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch_lightning/utils.py def register_forward_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ]) -> RemovableHandle inherited \u00b6 Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch_lightning/utils.py def register_forward_pre_hook ( self , hook : Callable [ ... , None ]) -> RemovableHandle : r \"\"\"Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_full_backward_hook ( self , hook : Callable [[ Module , Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]]) -> RemovableHandle inherited \u00b6 Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Source code in zamba/pytorch_lightning/utils.py def register_full_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ], Union [ None , Tensor ]] ) -> RemovableHandle : r \"\"\"Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\"\" if self . _is_full_backward_hook is False : raise RuntimeError ( \"Cannot use both regular backward hooks and full backward hooks on a \" \"single Module. Please use only one of them.\" ) self . _is_full_backward_hook = True handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ]) -> None inherited \u00b6 Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name required param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . required Source code in zamba/pytorch_lightning/utils.py def register_parameter ( self , name : str , param : Optional [ Parameter ]) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ): raise TypeError ( \"parameter name should be a string. \" \"Got {} \" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute ' {} ' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ): raise TypeError ( \"cannot assign ' {} ' object to parameter ' {} ' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter ' {0} '. Model \" \"parameters must be created explicitly. To express ' {0} ' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T inherited \u00b6 Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . True Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \"\"\"Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\"\" for p in self . parameters (): p . requires_grad_ ( requires_grad ) return self save_hyperparameters ( self , * args , * , ignore : Union [ Sequence [ str ], str ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None inherited \u00b6 Save arguments to hparams attribute. Parameters: Name Type Description Default args single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ () ignore Union[Sequence[str], str] an argument name or a list of argument names from class __init__ to be ignored None frame Optional[frame] a frame object. Default is None None logger bool Whether to send the hyperparameters to the logger. Default: True True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 Source code in zamba/pytorch_lightning/utils.py def save_hyperparameters ( self , * args , ignore : Optional [ Union [ Sequence [ str ], str ]] = None , frame : Optional [ types . FrameType ] = None , logger : bool = True , ) -> None : \"\"\"Save arguments to ``hparams`` attribute. Args: args: single object of `dict`, `NameSpace` or `OmegaConf` or string names or arguments from class ``__init__`` ignore: an argument name or a list of argument names from class ``__init__`` to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 \"\"\" self . _log_hyperparams = logger # the frame needs to be created in this file. if not frame : frame = inspect . currentframe () . f_back save_hyperparameters ( self , * args , ignore = ignore , frame = frame ) set_extra_state ( self , state : Any ) inherited \u00b6 This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Parameters: Name Type Description Default state dict Extra state from the state_dict required Source code in zamba/pytorch_lightning/utils.py def set_extra_state ( self , state : Any ): \"\"\" This function is called from :func:`load_state_dict` to handle any extra state found within the `state_dict`. Implement this function and a corresponding :func:`get_extra_state` for your module if you need to store extra state within its `state_dict`. Args: state (dict): Extra state from the `state_dict` \"\"\" raise RuntimeError ( \"Reached a code path in Module.set_extra_state() that should never be called. \" \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md \" \"to report this bug.\" ) setup ( self , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) Source code in zamba/pytorch_lightning/utils.py def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Args: stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'`` Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) \"\"\" share_memory ( self : ~ T ) -> ~ T inherited \u00b6 See :meth: torch.Tensor.share_memory_ Source code in zamba/pytorch_lightning/utils.py def share_memory ( self : T ) -> T : r \"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\" return self . _apply ( lambda t : t . share_memory_ ()) state_dict ( self , destination = None , prefix = '' , keep_vars = False ) inherited \u00b6 Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Source code in zamba/pytorch_lightning/utils.py def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) self . _save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . core . memory . ModelSummary ] inherited \u00b6 Summarize this LightningModule. Parameters: Name Type Description Default mode Optional[str] Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of max_depth and will be removed in v1.6. 'top' max_depth Optional[int] The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. None Returns: Type Description Optional[pytorch_lightning.core.memory.ModelSummary] The model summary object Source code in zamba/pytorch_lightning/utils.py def summarize ( self , mode : Optional [ str ] = \"top\" , max_depth : Optional [ int ] = None ) -> Optional [ ModelSummary ]: \"\"\" Summarize this LightningModule. Args: mode: Can be either ``'top'`` (summarize only direct submodules) or ``'full'`` (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object \"\"\" model_summary = None # temporary mapping from mode to max_depth if max_depth is None : if mode in ModelSummary . MODES : max_depth = ModelSummary . MODES [ mode ] rank_zero_deprecation ( f \"Argument `mode` in `LightningModule.summarize` is deprecated in v1.4\" f \" and will be removed in v1.6. Use `max_depth= { max_depth } ` to replicate `mode= { mode } ` behavior.\" ) model_summary = ModelSummary ( self , max_depth = max_depth ) elif mode is not None : raise MisconfigurationException ( f \"`mode` can be None, { ', ' . join ( ModelSummary . MODES ) } , got { mode } \" ) else : model_summary = ModelSummary ( self , max_depth = max_depth ) log . info ( \" \\n \" + str ( model_summary )) return model_summary tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list inherited \u00b6 When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Parameters: Name Type Description Default batch Tensor Current batch required split_size int The size of the split required Returns: Type Description List of batch splits. Each split will be passed to meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits !!! note Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . Source code in zamba/pytorch_lightning/utils.py def tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list : r \"\"\" When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start` if :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` > 0. Each returned batch split is passed separately to :meth:`training_step`. \"\"\" time_dims = [ len ( x [ 0 ]) for x in batch if isinstance ( x , ( torch . Tensor , collections . Sequence ))] assert len ( time_dims ) >= 1 , \"Unable to determine batch time dimension\" assert all ( x == time_dims [ 0 ] for x in time_dims ), \"Batch time dimension length is ambiguous\" splits = [] for t in range ( 0 , time_dims [ 0 ], split_size ): batch_split = [] for i , x in enumerate ( batch ): if isinstance ( x , torch . Tensor ): split_x = x [:, t : t + split_size ] elif isinstance ( x , collections . Sequence ): split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )): split_x [ batch_idx ] = x [ batch_idx ][ t : t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits teardown ( self , stage : Optional [ str ] = None ) -> None inherited \u00b6 Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Source code in zamba/pytorch_lightning/utils.py def teardown ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Called at the end of fit (train + validate), validate, test, predict, or tune. Args: stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'`` \"\"\" test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a postive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] !!! note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. !!! note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in zamba/pytorch_lightning/utils.py def test_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a postive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`setup` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying testing samples. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note: If you don't need a test dataset and a :meth:`test_step`, you don't need to implement this method. Note: In the case where you return multiple test dataloaders, the :meth:`test_step` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" test_epoch_end ( self , outputs : List [ Dict [ str , numpy . ndarray ]]) \u00b6 Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs List[Dict[str, numpy.ndarray]] List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader required Returns: Type Description None !!! note If you didn't define a :meth: test_step , this won't be called. Examples: With a single dataloader: .. code-block:: python def test_epoch_end(self, outputs): # do something with the outputs of all test batches all_test_preds = test_step_outputs.predictions some_result = calc_all_results(all_test_preds) self.log(some_result) With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each test step for that dataloader. .. code-block:: python def test_epoch_end(self, outputs): final_value = 0 for dataloader_outputs in outputs: for test_step_out in dataloader_outputs: # do something final_value += test_step_out self.log(\"final_metric\", final_value) Source code in zamba/pytorch_lightning/utils.py def test_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"test\" ) test_step ( self , batch , batch_idx ) \u00b6 Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch. required dataloader_idx int The index of the dataloader that produced this batch (only if multiple test dataloaders used). required Returns: Type Description Any of. Any object or value None - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... !!! note If you don't need to test you don't need to implement this method. !!! note When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. Source code in zamba/pytorch_lightning/utils.py def test_step ( self , batch , batch_idx ): return self . validation_step ( batch , batch_idx ) test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in :meth: test_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/pytorch_lightning/utils.py def test_step_end ( self , * args , ** kwargs ) -> Optional [ STEP_OUTPUT ]: \"\"\" Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`test_step` for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" to ( self , * args : Any , ** kwargs : Any ) -> DeviceDtypeModuleMixin inherited \u00b6 Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. !!! note This method modifies the module in-place. Parameters: Name Type Description Default device the desired device of the parameters and buffers in this module required dtype the desired floating point type of the floating point parameters and buffers in this module required tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 Source code in zamba/pytorch_lightning/utils.py def to ( self , * args : Any , ** kwargs : Any ) -> \"DeviceDtypeModuleMixin\" : \"\"\"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def __init__(self, weight: torch.Tensor): ... super().__init__() ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 \"\"\" # there is diff nb vars in PT 1.5 out = torch . _C . _nn . _parse_to ( * args , ** kwargs ) self . __update_properties ( device = out [ 0 ], dtype = out [ 1 ]) return super () . to ( * args , ** kwargs ) to_disk ( self , path : PathLike ) \u00b6 Source code in zamba/pytorch_lightning/utils.py def to_disk ( self , path : os . PathLike ): checkpoint = { \"state_dict\" : self . state_dict (), \"hyper_parameters\" : self . hparams , } torch . save ( checkpoint , path ) to_empty ( self : ~ T , * , device : Union [ str , torch . device ]) -> ~ T inherited \u00b6 Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device class: torch.device ): The desired device of the parameters and buffers in this module. required Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def to_empty ( self : T , * , device : Union [ str , device ]) -> T : r \"\"\"Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self \"\"\" return self . _apply ( lambda t : torch . empty_like ( t , device = device )) to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) inherited \u00b6 Saves the model in ONNX format. Parameters: Name Type Description Default file_path Union[str, pathlib.Path] The path of the file the onnx model should be saved to. required input_sample Optional[Any] An input for tracing. Default: None (Use self.example_input_array) None **kwargs Will be passed to torch.onnx.export function. {} Examples: >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) >>> with tempfile . NamedTemporaryFile ( suffix = '.onnx' , delete = False ) as tmpfile : ... model = SimpleModel () ... input_sample = torch . randn (( 1 , 64 )) ... model . to_onnx ( tmpfile . name , input_sample , export_params = True ) ... os . path . isfile ( tmpfile . name ) True Source code in zamba/pytorch_lightning/utils.py @torch . no_grad () def to_onnx ( self , file_path : Union [ str , Path ], input_sample : Optional [ Any ] = None , ** kwargs ): \"\"\" Saves the model in ONNX format. Args: file_path: The path of the file the onnx model should be saved to. input_sample: An input for tracing. Default: None (Use self.example_input_array) **kwargs: Will be passed to torch.onnx.export function. Example: >>> class SimpleModel(LightningModule): ... def __init__(self): ... super().__init__() ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) >>> with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmpfile: ... model = SimpleModel() ... input_sample = torch.randn((1, 64)) ... model.to_onnx(tmpfile.name, input_sample, export_params=True) ... os.path.isfile(tmpfile.name) True \"\"\" mode = self . training if input_sample is None : if self . example_input_array is None : raise ValueError ( \"Could not export to ONNX since neither `input_sample` nor\" \" `model.example_input_array` attribute is set.\" ) input_sample = self . example_input_array input_sample = self . _apply_batch_transfer_handler ( input_sample ) if \"example_outputs\" not in kwargs : self . eval () if isinstance ( input_sample , Tuple ): kwargs [ \"example_outputs\" ] = self ( * input_sample ) else : kwargs [ \"example_outputs\" ] = self ( input_sample ) torch . onnx . export ( self , input_sample , file_path , ** kwargs ) self . train ( mode ) to_torchscript ( self , file_path : Union [ str , pathlib . Path ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] inherited \u00b6 By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Parameters: Name Type Description Default file_path Union[str, pathlib.Path] Path where to save the torchscript. Default: None (no file saved). None method Optional[str] Whether to use TorchScript's script or trace method. Default: 'script' 'script' example_inputs Optional[Any] An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) None **kwargs Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. {} !!! note - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Examples: >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) ... >>> model = SimpleModel () >>> torch . jit . save ( model . to_torchscript (), \"model.pt\" ) # doctest: +SKIP >>> os . path . isfile ( \"model.pt\" ) # doctest: +SKIP >>> torch . jit . save ( model . to_torchscript ( file_path = \"model_trace.pt\" , method = 'trace' , # doctest: +SKIP ... example_inputs = torch . randn ( 1 , 64 ))) # doctest: +SKIP >>> os . path . isfile ( \"model_trace.pt\" ) # doctest: +SKIP True Returns: Type Description Union[torch._C.ScriptModule, Dict[str, torch._C.ScriptModule]] This LightningModule as a torchscript, regardless of whether file_path is defined or not. Source code in zamba/pytorch_lightning/utils.py @torch . no_grad () def to_torchscript ( self , file_path : Optional [ Union [ str , Path ]] = None , method : Optional [ str ] = \"script\" , example_inputs : Optional [ Any ] = None , ** kwargs , ) -> Union [ ScriptModule , Dict [ str , ScriptModule ]]: \"\"\" By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing, please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr:`example_input_array`) **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or :func:`torch.jit.trace` function. Note: - Requires the implementation of the :meth:`~pytorch_lightning.core.lightning.LightningModule.forward` method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod:`torch.jit` documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def __init__(self): ... super().__init__() ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether `file_path` is defined or not. \"\"\" mode = self . training if method == \"script\" : torchscript_module = torch . jit . script ( self . eval (), ** kwargs ) elif method == \"trace\" : # if no example inputs are provided, try to see if model has example_input_array set if example_inputs is None : if self . example_input_array is None : raise ValueError ( \"Choosing method=`trace` requires either `example_inputs`\" \" or `model.example_input_array` to be defined.\" ) example_inputs = self . example_input_array # automatically send example inputs to the right device and use trace example_inputs = self . _apply_batch_transfer_handler ( example_inputs ) torchscript_module = torch . jit . trace ( func = self . eval (), example_inputs = example_inputs , ** kwargs ) else : raise ValueError ( f \"The 'method' parameter only supports 'script' or 'trace', but value given was: { method } \" ) self . train ( mode ) if file_path is not None : fs = get_filesystem ( file_path ) with fs . open ( file_path , \"wb\" ) as f : torch . jit . save ( torchscript_module , f ) return torchscript_module toggle_optimizer ( self , optimizer : Optimizer , optimizer_idx : int ) inherited \u00b6 Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Override for your own behavior. Parameters: Name Type Description Default optimizer Optimizer Current optimizer used in the training loop required optimizer_idx int Current optimizer idx in the training loop required !!! note Only called when using multiple optimizers Source code in zamba/pytorch_lightning/utils.py def toggle_optimizer ( self , optimizer : Optimizer , optimizer_idx : int ): \"\"\" Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset. Override for your own behavior. Args: optimizer: Current optimizer used in the training loop optimizer_idx: Current optimizer idx in the training loop Note: Only called when using multiple optimizers \"\"\" # Iterate over all optimizer parameters to preserve their `requires_grad` information # in case these are pre-defined during `configure_optimizers` param_requires_grad_state = {} for opt in self . optimizers ( use_pl_optimizer = False ): for group in opt . param_groups : for param in group [ \"params\" ]: # If a param already appear in param_requires_grad_state, continue if param in param_requires_grad_state : continue param_requires_grad_state [ param ] = param . requires_grad param . requires_grad = False # Then iterate over the current optimizer's parameters and set its `requires_grad` # properties accordingly for group in optimizer . param_groups : for param in group [ \"params\" ]: param . requires_grad = param_requires_grad_state [ param ] self . _param_requires_grad_state = param_requires_grad_state train ( self : ~ T , mode : bool = True ) -> ~ T inherited \u00b6 Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . True Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def train ( self : T , mode : bool = True ) -> T : r \"\"\"Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\"\" if not isinstance ( mode , bool ): raise ValueError ( \"training mode is expected to be boolean\" ) self . training = mode for module in self . children (): module . train ( mode ) return self train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] inherited \u00b6 Implement one or more PyTorch DataLoaders for training. Returns: Type Description A collection of class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in zamba/pytorch_lightning/utils.py def train_dataloader ( self ) -> TRAIN_DATALOADERS : \"\"\" Implement one or more PyTorch DataLoaders for training. Return: A collection of :class:`torch.utils.data.DataLoader` specifying training samples. In the case of multiple dataloaders, please see this :ref:`page <multiple-training-dataloaders>`. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a positive integer. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`setup` - :meth:`train_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} \"\"\" rank_zero_warn ( \"`train_dataloader` must be implemented to be used with the Lightning Trainer\" ) training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]]) -> None inherited \u00b6 Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs List[Union[torch.Tensor, Dict[str, Any]]] List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. required Returns: Type Description None None !!! note If this method is not overridden, this won't be called. Example:: def training_epoch_end(self, training_step_outputs): # do something with all training_step outputs return result With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, training_step_outputs): for out in training_step_outputs: ... Source code in zamba/pytorch_lightning/utils.py def training_epoch_end ( self , outputs : EPOCH_OUTPUT ) -> None : \"\"\" Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth:`training_step`. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth:`training_step`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If this method is not overridden, this won't be called. Example:: def training_epoch_end(self, training_step_outputs): # do something with all training_step outputs return result With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, training_step_outputs): for out in training_step_outputs: ... \"\"\" training_step ( self , batch , batch_idx ) \u00b6 Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int Integer displaying index of this batch required optimizer_idx int When using multiple optimizers, this argument will also be present. required hiddens( class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. required Returns: Type Description Any of. - class: ~torch.Tensor - The loss tensor - dict - A dictionary. Can include any keys, but must include the key 'loss' - None - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU or TPU, or using DeepSpeed . In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step ... out, hiddens = self.lstm(data, hiddens) ... return {\"loss\": loss, \"hiddens\": hiddens} !!! note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. Source code in zamba/pytorch_lightning/utils.py def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"train_loss\" , loss . detach ()) return loss training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in training_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} def training_step_end(self, training_step_outputs): gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/pytorch_lightning/utils.py def training_step_end ( self , * args , ** kwargs ) -> STEP_OUTPUT : \"\"\" Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in `training_step` for each batch part. Return: Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} def training_step_end(self, training_step_outputs): gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" transfer_batch_to_device ( self , batch : Any , device : device , dataloader_idx : int ) -> Any inherited \u00b6 Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). !!! note This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. !!! note This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch Any A batch of data that needs to be transferred to a new device. required device device The target device as defined in PyTorch. required dataloader_idx int The index of the dataloader to which the batch belongs. required Returns: Type Description Any A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) !!! else batch = super().transfer_batch_to_device(data, device) return batch See Also: - :meth: move_data_to_device - :meth: apply_to_collection Source code in zamba/pytorch_lightning/utils.py def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any : \"\"\" Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: - :class:`torch.Tensor` or anything that implements `.to(...)` - :class:`list` - :class:`dict` - :class:`tuple` - :class:`torchtext.data.batch.Batch` For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use ``self.trainer.training/testing/validating/predicting`` so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. dataloader_idx: The index of the dataloader to which the batch belongs. Returns: A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Raises: MisconfigurationException: If using data-parallel, ``Trainer(accelerator='dp')``. See Also: - :meth:`move_data_to_device` - :meth:`apply_to_collection` \"\"\" return move_data_to_device ( batch , device ) type ( self , dst_type : Union [ str , torch . dtype ]) -> DeviceDtypeModuleMixin inherited \u00b6 Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type required Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def type ( self , dst_type : Union [ str , torch . dtype ]) -> \"DeviceDtypeModuleMixin\" : \"\"\"Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\"\" self . __update_properties ( dtype = dst_type ) return super () . type ( dst_type = dst_type ) unfreeze ( self ) -> None inherited \u00b6 Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() Source code in zamba/pytorch_lightning/utils.py def unfreeze ( self ) -> None : \"\"\" Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() \"\"\" for param in self . parameters (): param . requires_grad = True self . train () untoggle_optimizer ( self , optimizer_idx : int ) inherited \u00b6 Resets the state of required gradients that were toggled with :meth: toggle_optimizer . Override for your own behavior. Parameters: Name Type Description Default optimizer_idx int Current optimizer idx in the training loop required !!! note Only called when using multiple optimizers Source code in zamba/pytorch_lightning/utils.py def untoggle_optimizer ( self , optimizer_idx : int ): \"\"\" Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`. Override for your own behavior. Args: optimizer_idx: Current optimizer idx in the training loop Note: Only called when using multiple optimizers \"\"\" for opt_idx , opt in enumerate ( self . optimizers ( use_pl_optimizer = False )): if optimizer_idx != opt_idx : for group in opt . param_groups : for param in group [ \"params\" ]: if param in self . _param_requires_grad_state : param . requires_grad = self . _param_requires_grad_state [ param ] # save memory self . _param_requires_grad_state = {} val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] inherited \u00b6 Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs to a positive integer. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader !!! note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description A class: torch.utils.data.DataLoader or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] !!! note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. !!! note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in zamba/pytorch_lightning/utils.py def val_dataloader ( self ) -> EVAL_DATALOADERS : r \"\"\" Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to a positive integer. It's recommended that all data downloads and preparation happen in :meth:`prepare_data`. - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note: If you don't need a validation dataset and a :meth:`validation_step`, you don't need to implement this method. Note: In the case where you return multiple validation dataloaders, the :meth:`validation_step` will have an argument ``dataloader_idx`` which matches the order here. \"\"\" validation_epoch_end ( self , outputs : List [ Dict [ str , numpy . ndarray ]]) \u00b6 Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Parameters: Name Type Description Default outputs List[dict] list of output dictionaries from each validation step containing y_pred and y_true. required Source code in zamba/pytorch_lightning/utils.py def validation_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Args: outputs (List[dict]): list of output dictionaries from each validation step containing y_pred and y_true. \"\"\" y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"val\" ) validation_step ( self , batch , batch_idx ) \u00b6 Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch required dataloader_idx int The index of the dataloader that produced this batch (only if multiple val dataloaders used) required Returns: Type Description Any object or value None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... !!! note If you don't need to validate you don't need to implement this method. !!! note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. Source code in zamba/pytorch_lightning/utils.py def validation_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"val_loss\" , loss . detach ()) y_proba = torch . sigmoid ( y_hat . cpu ()) . numpy () return { \"y_true\" : y . cpu () . numpy () . astype ( int ), \"y_pred\" : y_proba . round () . astype ( int ), \"y_proba\" : y_proba , } validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] inherited \u00b6 Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. !!! note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs What you return in :meth: validation_step for each batch part. required Returns: Type Description Union[torch.Tensor, Dict[str, Any]] None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. Source code in zamba/pytorch_lightning/utils.py def validation_step_end ( self , * args , ** kwargs ) -> Optional [ STEP_OUTPUT ]: \"\"\" Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`validation_step` for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details. \"\"\" write_prediction ( self , name : str , value : Union [ torch . Tensor , List [ torch . Tensor ]], filename : str = 'predictions.pt' ) inherited \u00b6 Write predictions to disk using torch.save Example:: self.write_prediction('pred', torch.tensor(...), filename='my_predictions.pt') Parameters: Name Type Description Default name str a string indicating the name to save the predictions under required value Union[torch.Tensor, List[torch.Tensor]] the predictions, either a single :class: ~torch.Tensor or a list of them required filename str name of the file to save the predictions to 'predictions.pt' !!! note when running in distributed mode, calling write_prediction will create a file for each device with respective names: filename_rank_0.pt , filename_rank_1.pt , ... .. deprecated::v1.3 Will be removed in v1.5.0. Source code in zamba/pytorch_lightning/utils.py def write_prediction ( self , name : str , value : Union [ torch . Tensor , List [ torch . Tensor ]], filename : str = \"predictions.pt\" ): \"\"\" Write predictions to disk using ``torch.save`` Example:: self.write_prediction('pred', torch.tensor(...), filename='my_predictions.pt') Args: name: a string indicating the name to save the predictions under value: the predictions, either a single :class:`~torch.Tensor` or a list of them filename: name of the file to save the predictions to Note: when running in distributed mode, calling ``write_prediction`` will create a file for each device with respective names: ``filename_rank_0.pt``, ``filename_rank_1.pt``, ... .. deprecated::v1.3 Will be removed in v1.5.0. \"\"\" rank_zero_deprecation ( \"LightningModule method `write_prediction` was deprecated in v1.3 and will be removed in v1.5.\" ) self . trainer . _evaluation_loop . predictions . _add_prediction ( name , value , filename ) write_prediction_dict ( self , predictions_dict : Dict [ str , Any ], filename : str = 'predictions.pt' ) inherited \u00b6 Write a dictonary of predictions to disk at once using torch.save Example:: pred_dict = {'pred1': torch.tensor(...), 'pred2': torch.tensor(...)} self.write_prediction_dict(pred_dict) Parameters: Name Type Description Default predictions_dict Dict[str, Any] dict containing predictions, where each prediction should either be single :class: ~torch.Tensor or a list of them required !!! note when running in distributed mode, calling write_prediction_dict will create a file for each device with respective names: filename_rank_0.pt , filename_rank_1.pt , ... .. deprecated::v1.3 Will be removed in v1.5.0. Source code in zamba/pytorch_lightning/utils.py def write_prediction_dict ( self , predictions_dict : Dict [ str , Any ], filename : str = \"predictions.pt\" ): \"\"\" Write a dictonary of predictions to disk at once using ``torch.save`` Example:: pred_dict = {'pred1': torch.tensor(...), 'pred2': torch.tensor(...)} self.write_prediction_dict(pred_dict) Args: predictions_dict: dict containing predictions, where each prediction should either be single :class:`~torch.Tensor` or a list of them Note: when running in distributed mode, calling ``write_prediction_dict`` will create a file for each device with respective names: ``filename_rank_0.pt``, ``filename_rank_1.pt``, ... .. deprecated::v1.3 Will be removed in v1.5.0. \"\"\" rank_zero_deprecation ( \"LightningModule method `write_prediction_dict` was deprecated in v1.3 and will be removed in v1.5.\" ) for k , v in predictions_dict . items (): self . write_prediction ( k , v , filename ) xpu ( self : ~ T , device : Union [ int , torch . device ] = None ) -> ~ T inherited \u00b6 Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self Source code in zamba/pytorch_lightning/utils.py def xpu ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . xpu ( device )) zero_grad ( self , set_to_none : bool = False ) -> None inherited \u00b6 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. False Source code in zamba/pytorch_lightning/utils.py def zero_grad ( self , set_to_none : bool = False ) -> None : r \"\"\"Sets gradients of all model parameters to zero. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. \"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : if set_to_none : p . grad = None else : if p . grad . grad_fn is not None : p . grad . detach_ () else : p . grad . requires_grad_ ( False ) p . grad . zero_ () Functions \u00b6 register_model ( cls ) \u00b6 Used to decorate subclasses of ZambaVideoClassificationLightningModule so that they are included in available_models. Source code in zamba/pytorch_lightning/utils.py def register_model ( cls ): \"\"\"Used to decorate subclasses of ZambaVideoClassificationLightningModule so that they are included in available_models.\"\"\" if not issubclass ( cls , ZambaVideoClassificationLightningModule ): raise TypeError ( \"Cannot register object that is not a subclass of \" \"ZambaVideoClassificationLightningModule.\" ) available_models [ cls . __name__ ] = cls return cls","title":"zamba.pytorch_lightning.utils"},{"location":"api-reference/pytorch_lightning-utils/#zambapytorch_lightningutils","text":"","title":"zamba.pytorch_lightning.utils"},{"location":"api-reference/pytorch_lightning-utils/#zamba.pytorch_lightning.utils-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch_lightning-utils/#zamba.pytorch_lightning.utils-functions","text":"","title":"Functions"},{"location":"api-reference/settings/","text":"zamba.settings \u00b6 SPLIT_SEED \u00b6 VIDEO_SUFFIXES \u00b6","title":"zamba.settings"},{"location":"api-reference/settings/#zambasettings","text":"","title":"zamba.settings"},{"location":"contribute/","text":"Help make zamba better \u00b6 zamba is an open source project, which means you can help make it better! Develop the github repository \u00b6 To get involved, check out the Github code repository . There you can find open issues with comments and links to help you along. zamba uses continuous integration and test-driven-development to ensure that we always have a working project. So what are you waiting for? git going! Installation for development \u00b6 To install zamba for development, you need to clone the git repository and then install the cloned version of the library for local development. To install for development: $ git clone https://github.com/drivendataorg/zamba.git $ cd zamba $ pip install -r requirements-dev.txt Running the zamba test suite \u00b6 The included Makefile contains code that uses pytest to run all tests in zamba/tests . The command is (from the project root), $ make tests Submit additional training videos \u00b6 If you have additional labeled videos that may be useful for improving the basic models that ship with zamba , we'd love to hear from you! You can get in touch at info@drivendata.org","title":"Help make `zamba` better"},{"location":"contribute/#help-make-zamba-better","text":"zamba is an open source project, which means you can help make it better!","title":"Help make zamba better"},{"location":"contribute/#develop-the-github-repository","text":"To get involved, check out the Github code repository . There you can find open issues with comments and links to help you along. zamba uses continuous integration and test-driven-development to ensure that we always have a working project. So what are you waiting for? git going!","title":"Develop the github repository"},{"location":"contribute/#installation-for-development","text":"To install zamba for development, you need to clone the git repository and then install the cloned version of the library for local development. To install for development: $ git clone https://github.com/drivendataorg/zamba.git $ cd zamba $ pip install -r requirements-dev.txt","title":"Installation for development"},{"location":"contribute/#running-the-zamba-test-suite","text":"The included Makefile contains code that uses pytest to run all tests in zamba/tests . The command is (from the project root), $ make tests","title":"Running the zamba test suite"},{"location":"contribute/#submit-additional-training-videos","text":"If you have additional labeled videos that may be useful for improving the basic models that ship with zamba , we'd love to hear from you! You can get in touch at info@drivendata.org","title":"Submit additional training videos"},{"location":"models/denspose/","text":"Densepose \u00b6 Background \u00b6 Facebook AI Research has published a model, DensePose ( Neverova et al, 2021 ), which can be used to get segmentations for animals that appear in videos. This was trained on the following animals, but often works for other species as well: sheep, zebra, horse, giraffe, elephant, cow, ear, cat, dog. Here's an example of the segmentation output for a frame: Additionally, the model provides mapping of the segmentation output to specific anatomy for chimpanzees. This can be helpful for determining the orientation of chimpanzees in videos and for their behaviors. Here is an example of what that output looks like: For more information on the algorithms and outputs of the DensePose model, see the Facebook DensePose Github Repository . Outputs \u00b6 The Zamba package supports running Densepose on videos to generate three types of outputs: A .json file with details of segmentations per video frame. A .mp4 file where the original video has the segmentation rendered on top of animal so that the output can be vsiually inspected. A .csv (when --output-type chimp_anatomy ) that contains the height and width of the bounding box around each chimpanzee, the frame number and timestamp of the observation, and the percentage of pixels in the bounding box that correspond with each anatomical part. Generally, running the densepose model is computationally intensive. It is recommended to run the model at a relatively low framerate (e.g., 1 frame per second) to generate outputs for a video. Another caveat is that because the output JSON output contains the full embedding, these files can be quite large. These are not written out by default. In order to use the densepose model, you must have PyTorch already installed on your system, and then you must install the densepose extra: pip install torch # see https://pytorch.org/get-started/locally/ pip install \"zamba[densepose]\" Running DensePose \u00b6 Once that is done, here's how to run the DensePose model: CLI # create a segmentation output video for each input video in PATH_TO_VIDEOS zamba densepose --data-dir PATH_TO_VIDEOS --render-output Python from zamba.models.densepose import DensePoseConfig densepose_conf = DensePoseConfig ( data_dir = \"PATH_TO_VIDEOS\" , render_output = True ) densepose_conf . run_model () ## Getting help To see all of the available options, run `zamba densepose --help`. $ zamba densepose --help Usage: zamba densepose [OPTIONS] Run densepose algorithm on videos. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to video or image file or folder containing images/videos. --filepaths PATH Path to csv containing `filepath` column with videos. --save-dir PATH An optional directory for saving the output. Defaults to the current working directory. --config PATH Specify options using yaml configuration file instead of through command line options. --fps FLOAT Number of frames per second to process. Defaults to 1.0 (1 frame per second). [default: 1.0] --output-type [segmentation|chimp_anatomy] If 'chimp_anatomy' will apply anatomy model from densepose to the rendering and create a CSV with the anatomy visible in each frame. If 'segmentation', will just output the segmented area where an animal is identified, which works for more species than chimpanzees. [default: chimp_anatomy] --render-output / --no-render-output If True, generate an output image or video with either the segmentation or anatomy rendered depending on the `output_type` that is chosen. [default: no-render-output] --weight-download-region [us|eu|asia] Server region for downloading weights. --cache-dir PATH Path to directory for model weights. Alternatively, specify with environment variable `ZAMBA_CACHE_DIR`. If not specified, user's cache directory is used. -y, --yes Skip confirmation of configuration and proceed right to prediction. --help Show this message and exit.","title":"DensePose"},{"location":"models/denspose/#densepose","text":"","title":"Densepose"},{"location":"models/denspose/#background","text":"Facebook AI Research has published a model, DensePose ( Neverova et al, 2021 ), which can be used to get segmentations for animals that appear in videos. This was trained on the following animals, but often works for other species as well: sheep, zebra, horse, giraffe, elephant, cow, ear, cat, dog. Here's an example of the segmentation output for a frame: Additionally, the model provides mapping of the segmentation output to specific anatomy for chimpanzees. This can be helpful for determining the orientation of chimpanzees in videos and for their behaviors. Here is an example of what that output looks like: For more information on the algorithms and outputs of the DensePose model, see the Facebook DensePose Github Repository .","title":"Background"},{"location":"models/denspose/#outputs","text":"The Zamba package supports running Densepose on videos to generate three types of outputs: A .json file with details of segmentations per video frame. A .mp4 file where the original video has the segmentation rendered on top of animal so that the output can be vsiually inspected. A .csv (when --output-type chimp_anatomy ) that contains the height and width of the bounding box around each chimpanzee, the frame number and timestamp of the observation, and the percentage of pixels in the bounding box that correspond with each anatomical part. Generally, running the densepose model is computationally intensive. It is recommended to run the model at a relatively low framerate (e.g., 1 frame per second) to generate outputs for a video. Another caveat is that because the output JSON output contains the full embedding, these files can be quite large. These are not written out by default. In order to use the densepose model, you must have PyTorch already installed on your system, and then you must install the densepose extra: pip install torch # see https://pytorch.org/get-started/locally/ pip install \"zamba[densepose]\"","title":"Outputs"},{"location":"models/denspose/#running-densepose","text":"Once that is done, here's how to run the DensePose model: CLI # create a segmentation output video for each input video in PATH_TO_VIDEOS zamba densepose --data-dir PATH_TO_VIDEOS --render-output Python from zamba.models.densepose import DensePoseConfig densepose_conf = DensePoseConfig ( data_dir = \"PATH_TO_VIDEOS\" , render_output = True ) densepose_conf . run_model () ## Getting help To see all of the available options, run `zamba densepose --help`. $ zamba densepose --help Usage: zamba densepose [OPTIONS] Run densepose algorithm on videos. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to video or image file or folder containing images/videos. --filepaths PATH Path to csv containing `filepath` column with videos. --save-dir PATH An optional directory for saving the output. Defaults to the current working directory. --config PATH Specify options using yaml configuration file instead of through command line options. --fps FLOAT Number of frames per second to process. Defaults to 1.0 (1 frame per second). [default: 1.0] --output-type [segmentation|chimp_anatomy] If 'chimp_anatomy' will apply anatomy model from densepose to the rendering and create a CSV with the anatomy visible in each frame. If 'segmentation', will just output the segmented area where an animal is identified, which works for more species than chimpanzees. [default: chimp_anatomy] --render-output / --no-render-output If True, generate an output image or video with either the segmentation or anatomy rendered depending on the `output_type` that is chosen. [default: no-render-output] --weight-download-region [us|eu|asia] Server region for downloading weights. --cache-dir PATH Path to directory for model weights. Alternatively, specify with environment variable `ZAMBA_CACHE_DIR`. If not specified, user's cache directory is used. -y, --yes Skip confirmation of configuration and proceed right to prediction. --help Show this message and exit.","title":"Running DensePose"},{"location":"models/species-detection/","text":"Available models \u00b6 The algorithms in zamba are designed to identify species of animals that appear in camera trap videos. There are three models that ship with the zamba package: time_distributed , slowfast , and european . For more details of each, read on! Model summary \u00b6 Model Geography Relative strengths Architecture time_distributed Central and West Africa Better than slowfast at duikers, chimps, and gorillas and other larger species Image-based TimeDistributedEfficientNet slowfast Central and West Africa Better than time_distributed at blank detection and small species detection Video-native SlowFast european Western Europe Trained on non-jungle ecologies Finetuned time_distributed model All models support training, fine-tuning, and inference. For fine-tuning, we recommend using the time_distributed model as the starting point. What species can zamba detect? \u00b6 time_distributed and slowfast are both trained to identify 32 common species from Central and West Africa. The output labels in these models are: aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal european is trained to identify 11 common species in western Europe. The possible class labels are: bird blank domestic_cat european_badger european_beaver european_hare european_roe_deer north_american_raccoon red_fox unidentified weasel wild_boar time_distributed model \u00b6 Architecture \u00b6 The time_distributed model was built by re-training a well-known image classification architecture called EfficientNetV2 (Tan, M., & Le, Q., 2019) to identify the species in our camera trap videos. EfficientNetV2 models are convolutional neural networks designed to jointly optimize model size and training speed. EfficientNetV2 is image native, meaning it classifies each frame separately when generating predictions. The model is wrapped in a TimeDistributed layer which enables a single prediction per video. Training data \u00b6 time_distributed was trained using data collected and annotated by partners at The Max Planck Institute for Evolutionary Anthropology and Chimp & See . The data included camera trap videos from: Country Location Cameroon Campo Ma'an National Park Korup National Park Central African Republic Dzanga-Sangha Protected Area C\u00f4te d'Ivoire Como\u00e9 National Park Guiroutou Ta\u00ef National Park Democratic Republic of the Congo Bili-Uele Protect Area Salonga National Park Gabon Loango National Park Lop\u00e9 National Park Guinea Bakoun Classified Forest Moyen-Bafing National Park Liberia East Nimba Nature Reserve Grebo-Krahn National Park Sapo National Park Mozambique Gorongosa National Park Nigeria Gashaka-Gumti National Park Republic of the Congo Conkouati-Douli National Park Nouabale-Ndoki National Park Senegal Kayan Tanzania Grumeti Game Reserve Ugalla River National Park Uganda Budongo Forest Reserve Bwindi Forest National Park Ngogo and Kibale National Park Default configuration \u00b6 The full default configuration is available on Github . By default, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then time_distributed is run on only the 16 frames with the highest predicted probability of detection. By default, videos are resized to 240x426 pixels following frame selection. The default video loading configuration for time_distributed is: video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 4 total_frames : 16 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 16 You can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file . The only requirement for the time_distributed model is that the video loader must return 16 frames. slowfast model \u00b6 Architecture \u00b6 The slowfast model was built by re-training a video classification backbone called SlowFast (Feichtenhofer, C., Fan, H., Malik, J., & He, K., 2019). SlowFast refers to the two model pathways involved: one that operates at a low frame rate to capture spatial semantics, and one that operates at a high frame rate to capture motion over time. Source: Feichtenhofer, C., Fan, H., Malik, J., & He, K. (2019). Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 6202-6211). Unlike time_distributed , slowfast is video native. This means it takes into account the relationship between frames in a video, rather than running independently on each frame. Training data \u00b6 The slowfast model was trained using the same data as the time_distributed model . Default configuration \u00b6 The full default configuration is available on Github . By default, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then slowfast is run on only the 32 frames with the highest predicted probability of detection. By default, videos are resized to 240x426 pixels. The full default video loading configuration is: video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 8 total_frames : 32 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 32 You can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file . The two requirements for the slowfast model are that: - the video loader must return 32 frames - videos inputted into the model must be at least 200 x 200 pixels european model \u00b6 Architecture \u00b6 The european model starts from the trained time_distributed model, and then replaces and trains the final output layer to predict European species. Training data \u00b6 The european model is finetuned with data collected and annotated by partners at The Max Planck Institute for Evolutionary Anthropology . The finetuning data included camera trap videos from Hintenteiche bei Biesenbrow, Germany. Default configuration \u00b6 The full default configuration is available on Github . The european model uses the same frame selection as the time_distributed model. By default, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then european is run on only the 16 frames with the highest predicted probability of detection. By default, videos are resized to 240x426 pixels following frame selection. The full default video loading configuration is: video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 4 total_frames : 16 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 16 As with all models, you can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file . The only requirement for the european model is that the video loader must return 16 frames. MegadetectorLite \u00b6 Frame selection for video models is critical as it would be infeasible to train neural networks on all the frames in a video. For all the species detection models that ship with zamba , the default frame selection method is an efficient object detection model called MegadetectorLite that determines the likelihood that each frame contains an animal. Then, only the frames with the highest probability of detection are passed to the model. MegadetectorLite combines two open-source models: Megadetector is a pretrained image model designed to detect animals, people, and vehicles in camera trap videos. YOLOX is a high-performance, lightweight object detection model that is much less computationally intensive than Megadetector. While highly accurate, Megadetector is too computationally intensive to run on every frame. MegadetectorLite was created by training a YOLOX model using the predictions of the Megadetector as ground truth - this method is called student-teacher training .","title":"Species detection"},{"location":"models/species-detection/#available-models","text":"The algorithms in zamba are designed to identify species of animals that appear in camera trap videos. There are three models that ship with the zamba package: time_distributed , slowfast , and european . For more details of each, read on!","title":"Available models"},{"location":"models/species-detection/#model-summary","text":"Model Geography Relative strengths Architecture time_distributed Central and West Africa Better than slowfast at duikers, chimps, and gorillas and other larger species Image-based TimeDistributedEfficientNet slowfast Central and West Africa Better than time_distributed at blank detection and small species detection Video-native SlowFast european Western Europe Trained on non-jungle ecologies Finetuned time_distributed model All models support training, fine-tuning, and inference. For fine-tuning, we recommend using the time_distributed model as the starting point.","title":"Model summary"},{"location":"models/species-detection/#what-species-can-zamba-detect","text":"time_distributed and slowfast are both trained to identify 32 common species from Central and West Africa. The output labels in these models are: aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal european is trained to identify 11 common species in western Europe. The possible class labels are: bird blank domestic_cat european_badger european_beaver european_hare european_roe_deer north_american_raccoon red_fox unidentified weasel wild_boar","title":"What species can zamba detect?"},{"location":"models/species-detection/#time_distributed-model","text":"","title":"time_distributed model"},{"location":"models/species-detection/#slowfast-model","text":"","title":"slowfast model"},{"location":"models/species-detection/#european-model","text":"","title":"european model"},{"location":"models/species-detection/#megadetectorlite","text":"Frame selection for video models is critical as it would be infeasible to train neural networks on all the frames in a video. For all the species detection models that ship with zamba , the default frame selection method is an efficient object detection model called MegadetectorLite that determines the likelihood that each frame contains an animal. Then, only the frames with the highest probability of detection are passed to the model. MegadetectorLite combines two open-source models: Megadetector is a pretrained image model designed to detect animals, people, and vehicles in camera trap videos. YOLOX is a high-performance, lightweight object detection model that is much less computationally intensive than Megadetector. While highly accurate, Megadetector is too computationally intensive to run on every frame. MegadetectorLite was created by training a YOLOX model using the predictions of the Megadetector as ground truth - this method is called student-teacher training .","title":"MegadetectorLite"}]}